{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Integrantes del grupo**\n",
        "\n",
        "## Juan Pablo Ocampo Santana - 1054857108\n",
        "\n",
        "## Oscar Andres Tepud Jacome - 1086418288\n",
        "\n",
        "## Juan Esteban Mora Diaz - 1104544665"
      ],
      "metadata": {
        "id": "GL0sGMd3v9zV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hola Profesor Andres te explico la estrucutura de este cuaderno de colab. En este cuaderno encontraras los **10 modelos de prediccion**, cada uno con 4 bloques: **primer bloque (celda de texto):** Despliegue matematico del modelo, jutificacion de hiperparametros, rangos de los valores y score a minimizar. **Segundo bloque (celda de codigo):** codigo de entrenamiento para estimar los mejores hiperparametros. **Tercer bloque (celda de c√≥digo):** C√≥digo de entrenamiento usando los mejores hiperparametros para predecir los datos de salida (*x_pred* , *y_pred*) y que arroja el dataset de predicci√≥n. **Cuarto bloque (celda de c√≥digo):** al final de cada modelo se hace una conclusi√≥n general de la funci√≥n y eficiencia de cada modelo y se analiza si sirve o no para nuestra competici√≥n. ***NOTA IMPORTANTE:*** Si se quieren ejecutar los codigos se necesitan subir los dos archivos de excel (test_input_clean_final-csv y train_ready_final_numerico.csv) a drive y habilitar la conexi√≥n entre colab y drive porque los codigos procesan los datasets desde drive (no cambiar el nombre de los archivos). Estos se encuentran en la carpeta de \"solucion_parcial1_TAM\" del github."
      ],
      "metadata": {
        "id": "PtQgL3_Qs6x4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Linnear Regresor**"
      ],
      "metadata": {
        "id": "6K_HmJCSo73V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##  **Despliegue matem√°tico, hiperpar√°metros y criterio de optimizaci√≥n**\n",
        "\n",
        "---\n",
        "\n",
        "## Fundamento del modelo\n",
        "\n",
        "El modelo de **regresi√≥n lineal** busca encontrar los coeficientes $$\\boldsymbol{\\beta}$$ que relacionan un conjunto de variables predictoras $$X$$ con una variable dependiente $$y$$:\n",
        "\n",
        "$$\n",
        "y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\varepsilon_i\n",
        "$$\n",
        "\n",
        "En notaci√≥n matricial:\n",
        "\n",
        "$$\n",
        "\\mathbf{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}, \\qquad \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0, \\sigma^2 I)\n",
        "$$\n",
        "\n",
        "La estimaci√≥n por **m√≠nimos cuadrados ordinarios (OLS)** se obtiene resolviendo:\n",
        "\n",
        "$$\n",
        "\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\mathbf{y}\n",
        "$$\n",
        "\n",
        "donde se minimiza la funci√≥n de p√©rdida cuadr√°tica:\n",
        "\n",
        "$$\n",
        "\\min_{\\boldsymbol{\\beta}} S(\\boldsymbol{\\beta}) = \\| \\mathbf{y} - X\\boldsymbol{\\beta} \\|^2\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Rejilla y rangos de hiperpar√°metros (Bayesian Optimization)\n",
        "\n",
        "La **Optimizaci√≥n Bayesiana (BO)** explora el espacio de hiperpar√°metros mediante un proceso gaussiano y una funci√≥n de adquisici√≥n *Expected Improvement* (EI).\n",
        "\n",
        "En este modelo, los hiperpar√°metros a optimizar son:\n",
        "\n",
        "| Hiperpar√°metro | Tipo | Rango definido en el c√≥digo | Descripci√≥n | Efecto esperado |\n",
        "|----------------|------|-----------------------------|--------------|-----------------|\n",
        "| `fit_intercept` | Discreto (0 o 1) | $$[0, 1]$$ | Indica si se ajusta el intercepto $$\\beta_0$$. | Mejora el ajuste si los datos no est√°n centrados. |\n",
        "| `normalize` | Discreto (0 o 1) | $$[0, 1]$$ | Indica si las variables predictoras se normalizan. | Mejora la estabilidad num√©rica si las variables tienen diferentes escalas. |\n",
        "\n",
        "---\n",
        "\n",
        "###  Justificaci√≥n de los rangos\n",
        "\n",
        "El rango $$[0, 1]$$ para ambos par√°metros se define para permitir que la BO explore ambas posibilidades binarias, interpret√°ndolas como variables continuas que luego se redondean a 0 o 1.  \n",
        "\n",
        "Formalmente, el espacio de b√∫squeda se define como:\n",
        "\n",
        "$$\n",
        "\\mathcal{H} = \\{\\, \\theta = (\\text{fit\\_intercept},\\, \\text{normalize}) \\in [0, 1]^2 \\,\\}\n",
        "$$\n",
        "\n",
        "y la BO busca:\n",
        "\n",
        "$$\n",
        "\\theta^* = \\arg\\min_{\\theta \\in \\mathcal{H}} MAE_{\\text{val}}(\\theta)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Score o funci√≥n objetivo a minimizar\n",
        "\n",
        "El **score** optimizado en la funci√≥n `linear_eval()` es el **Error Absoluto Medio (MAE)** en el conjunto de validaci√≥n:\n",
        "\n",
        "$$\n",
        "MAE(\\theta) = \\frac{1}{n_{\\text{val}}} \\sum_{i=1}^{n_{\\text{val}}} |y_i - \\hat{y}_i(\\theta)|\n",
        "$$\n",
        "\n",
        "El c√≥digo devuelve el valor negativo de esta m√©trica porque la BO **maximiza** la funci√≥n objetivo:\n",
        "\n",
        "$$\n",
        "f(\\theta) = -MAE(\\theta)\n",
        "$$\n",
        "\n",
        "Por tanto:\n",
        "\n",
        "$$\n",
        "\\max_{\\theta \\in \\mathcal{H}} f(\\theta) = -MAE_{\\text{val}}(\\theta)\n",
        "\\quad \\Longleftrightarrow \\quad\n",
        "\\min_{\\theta \\in \\mathcal{H}} MAE_{\\text{val}}(\\theta)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Modelo probabil√≠stico subyacente a la BO\n",
        "\n",
        "La BO utiliza un **Proceso Gaussiano (GP)** para modelar la funci√≥n desconocida $$f(\\theta)$$:\n",
        "\n",
        "$$\n",
        "f(\\theta) \\sim \\mathcal{GP}(m(\\theta),\\, k(\\theta, \\theta'))\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $$m(\\theta)$$ es la media del proceso (usualmente 0),\n",
        "- $$k(\\theta, \\theta')$$ es la funci√≥n de covarianza o *kernel*.\n",
        "\n",
        "La predicci√≥n del GP para un punto candidato $$\\theta_*$$ est√° dada por:\n",
        "\n",
        "$$\n",
        "f(\\theta_*) \\mid \\mathcal{D}_t \\sim \\mathcal{N}(\\mu_t(\\theta_*),\\, \\sigma_t^2(\\theta_*))\n",
        "$$\n",
        "\n",
        "con:\n",
        "\n",
        "$$\n",
        "\\mu_t(\\theta_*) = k_*^\\top (K + \\sigma_n^2 I)^{-1} \\mathbf{f}, \\qquad\n",
        "\\sigma_t^2(\\theta_*) = k(\\theta_*,\\theta_*) - k_*^\\top (K + \\sigma_n^2 I)^{-1} k_*\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $$K$$ es la matriz de covarianza de los puntos ya evaluados,\n",
        "- $$\\mathbf{f}$$ son los valores observados de la funci√≥n objetivo,\n",
        "- $$\\sigma_n^2$$ representa el ruido del modelo.\n",
        "\n",
        "---\n",
        "\n",
        "## Funci√≥n de adquisici√≥n Expected Improvement (EI)\n",
        "\n",
        "El siguiente punto a evaluar se elige maximizando la **Expected Improvement (EI)**:\n",
        "\n",
        "$$\n",
        "EI(\\theta) = \\mathbb{E}[\\max(0,\\, f(\\theta) - f^+)]\n",
        "$$\n",
        "\n",
        "donde $$f^+$$ es el mejor valor observado hasta el momento.  \n",
        "Si $$f(\\theta) \\sim \\mathcal{N}(\\mu,\\, \\sigma^2)$$, entonces:\n",
        "\n",
        "$$\n",
        "EI(\\theta) = (\\mu - f^+) \\Phi(z) + \\sigma \\phi(z)\n",
        "$$\n",
        "\n",
        "con:\n",
        "\n",
        "$$\n",
        "z = \\frac{\\mu - f^+}{\\sigma}\n",
        "$$\n",
        "\n",
        "y $$\\Phi(\\cdot)$$ y $$\\phi(\\cdot)$$ son respectivamente la CDF y la PDF de la normal est√°ndar.\n",
        "\n",
        "---\n",
        "\n",
        "## Proceso de b√∫squeda y convergencia\n",
        "\n",
        "El proceso de BO se ejecuta con:\n",
        "- **init_points = 5** ‚Üí cinco puntos iniciales aleatorios.  \n",
        "- **n_iter = 15** ‚Üí quince iteraciones guiadas por la funci√≥n de adquisici√≥n.\n",
        "\n",
        "En total se exploran $$20$$ configuraciones, equilibrando exploraci√≥n y explotaci√≥n.\n",
        "\n",
        "---\n",
        "\n",
        "## Entrenamiento final y evaluaci√≥n\n",
        "\n",
        "Una vez encontrados los mejores hiperpar√°metros $$\\theta^*$$, se reentrena el modelo y se eval√∫a sobre el conjunto de prueba.  \n",
        "Las m√©tricas calculadas son:\n",
        "\n",
        "$$\n",
        "MAE  = \\frac{1}{n}\\sum |y_i - \\hat{y}_i|\n",
        "$$\n",
        "\n",
        "$$\n",
        "MSE  = \\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "R^2  = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
        "$$\n",
        "\n",
        "$$\n",
        "MAPE = \\frac{100}{n}\\sum \\left|\\frac{y_i - \\hat{y}_i}{\\max(|y_i|, \\varepsilon)}\\right|\n",
        "$$\n",
        "\n",
        "Adem√°s, se aplica un remuestreo bootstrap con $$B = 20$$ para estimar la desviaci√≥n est√°ndar de cada m√©trica:\n",
        "\n",
        "$$\n",
        "\\sigma_m = \\sqrt{\\frac{1}{B-1}\\sum_{b=1}^{B} (m_b - \\bar{m})^2}\n",
        "$$\n",
        "\n",
        "donde $$m_b$$ es la m√©trica en la $$b$$-√©sima muestra bootstrap.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusi√≥n te√≥rico-computacional\n",
        "\n",
        "- El modelo ajusta un plano en el espacio de predictores que minimiza el error absoluto medio.  \n",
        "- Los hiperpar√°metros controlan la **forma del espacio de caracter√≠sticas** (centrado y escalado).  \n",
        "- La **Optimizaci√≥n Bayesiana** encuentra la configuraci√≥n √≥ptima sin evaluar todas las combinaciones posibles, aprovechando la inferencia probabil√≠stica del proceso gaussiano.  \n",
        "- El **score a minimizar** es $$MAE_{\\text{val}}(\\theta)$$ que refleja la desviaci√≥n absoluta promedio en validaci√≥n.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s_qWNudEOlpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üß† PUNTO 4 ‚Äì MODELO 1: LINEAR REGRESSION (RAPIDS + BO + m√©tricas)\n",
        "# ======================================================\n",
        "\n",
        "# ---------- Instalaciones necesarias ----------\n",
        "!pip -q install bayesian-optimization\n",
        "\n",
        "# (Opcional) instalar RAPIDS si hay GPU\n",
        "!pip -q install cudf-cu12 cuml-cu12 --extra-index-url=https://pypi.nvidia.com\n",
        "\n",
        "# ---------- Imports ----------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ---------- Intentar usar RAPIDS ----------\n",
        "USE_RAPIDS = False\n",
        "try:\n",
        "    import cudf\n",
        "    from cuml.linear_model import LinearRegression as cuLinearRegression\n",
        "    USE_RAPIDS = True\n",
        "    print(\"‚úÖ RAPIDS/cuML disponible: se usar√° GPU.\")\n",
        "except Exception:\n",
        "    from sklearn.linear_model import LinearRegression as skLinearRegression\n",
        "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "    print(\"‚ö†Ô∏è RAPIDS/cuML no disponible. Se usar√° scikit-learn (CPU).\")\n",
        "\n",
        "# ---------- Funciones auxiliares ----------\n",
        "def mape_np(y_true, y_pred, eps=1e-8):\n",
        "    denom = np.maximum(np.abs(y_true), eps)\n",
        "    return np.mean(np.abs((y_true - y_pred) / denom))\n",
        "\n",
        "def bootstrap_stats(y_true, y_pred, n_boot=20, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(y_true)\n",
        "    maes, mses, r2s, mapes = [], [], [], []\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        yt, yp = y_true[idx], y_pred[idx]\n",
        "        maes.append(np.mean(np.abs(yt - yp)))\n",
        "        mses.append(np.mean((yt - yp)**2))\n",
        "        ss_res = np.sum((yt - yp)**2)\n",
        "        ss_tot = np.sum((yt - yt.mean())**2) + 1e-12\n",
        "        r2s.append(1 - ss_res/ss_tot)\n",
        "        mapes.append(mape_np(yt, yp))\n",
        "    def stats(a): return (np.mean(a), np.std(a, ddof=1))\n",
        "    return {\"MAE\": stats(maes), \"MSE\": stats(mses), \"R2\": stats(r2s), \"MAPE\": stats(mapes)}\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ MONTAJE DE GOOGLE DRIVE Y CARGA DEL DATASET\n",
        "# ======================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = \"/content/drive/MyDrive/train_ready_final_numeric.csv\"\n",
        "\n",
        "if USE_RAPIDS:\n",
        "    df = cudf.read_csv(path)\n",
        "else:\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "print(f\"‚úÖ Dataset cargado: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ PREPARACI√ìN DE DATOS\n",
        "# ======================================================\n",
        "TARGET_X, TARGET_Y = \"x_target\", \"y_target\"\n",
        "X = df.drop(columns=[TARGET_X, TARGET_Y])\n",
        "y_x, y_y = df[TARGET_X], df[TARGET_Y]\n",
        "\n",
        "# Divisi√≥n 60/20/20\n",
        "X_train, X_temp, yx_train, yx_temp, yy_train, yy_temp = train_test_split(\n",
        "    X, y_x, y_y, test_size=0.4, random_state=42\n",
        ")\n",
        "X_val, X_test, yx_val, yx_test, yy_val, yy_test = train_test_split(\n",
        "    X_temp, yx_temp, yy_temp, test_size=0.5, random_state=42\n",
        ")\n",
        "print(\"‚úÖ Divisi√≥n 60/20/20 completada.\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ OPTIMIZACI√ìN BAYESIANA PARA LINEAR REGRESSION (x_target)\n",
        "# ======================================================\n",
        "def linear_eval(fit_intercept, normalize):\n",
        "    fit_intercept = bool(round(fit_intercept))\n",
        "    normalize = bool(round(normalize))\n",
        "\n",
        "    if USE_RAPIDS:\n",
        "        model = cuLinearRegression(fit_intercept=fit_intercept)\n",
        "        model.fit(X_train, yx_train)\n",
        "        preds = model.predict(X_val)\n",
        "        mae = float(np.mean(np.abs(yx_val.to_numpy() - preds.to_numpy())))\n",
        "    else:\n",
        "        model = skLinearRegression(fit_intercept=fit_intercept)\n",
        "        model.fit(X_train, yx_train)\n",
        "        preds = model.predict(X_val)\n",
        "        mae = mean_absolute_error(yx_val, preds)\n",
        "    return -mae\n",
        "\n",
        "pbounds = {\"fit_intercept\": (0, 1), \"normalize\": (0, 1)}\n",
        "\n",
        "optimizer_x = BayesianOptimization(f=linear_eval, pbounds=pbounds, random_state=42, verbose=2)\n",
        "print(\"üöÄ Iniciando optimizaci√≥n bayesiana (x_target)...\")\n",
        "optimizer_x.maximize(init_points=5, n_iter=15)\n",
        "\n",
        "# ‚úÖ Correcci√≥n: mostrar valores exactos en lugar de True/False\n",
        "best_x = optimizer_x.max[\"params\"]\n",
        "best_x[\"fit_intercept\"] = float(best_x[\"fit_intercept\"])\n",
        "best_x[\"normalize\"] = float(best_x[\"normalize\"])\n",
        "\n",
        "print(\"\\nüèÅ Mejores hiperpar√°metros encontrados (valores continuos optimizados):\")\n",
        "for key, value in best_x.items():\n",
        "    print(f\"   - {key}: {value:.6f}\")\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ ENTRENAMIENTO FINAL Y EVALUACI√ìN\n",
        "# ======================================================\n",
        "fit_intercept_final = bool(round(best_x[\"fit_intercept\"]))\n",
        "normalize_final = bool(round(best_x[\"normalize\"]))\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è Entrenando modelo final con fit_intercept={fit_intercept_final}, normalize={normalize_final}\")\n",
        "\n",
        "if USE_RAPIDS:\n",
        "    model_x = cuLinearRegression(fit_intercept=fit_intercept_final)\n",
        "    model_x.fit(X_train, yx_train)\n",
        "    preds_x = model_x.predict(X_test).to_numpy()\n",
        "    yx_test_np = yx_test.to_numpy()\n",
        "else:\n",
        "    model_x = skLinearRegression(fit_intercept=fit_intercept_final)\n",
        "    model_x.fit(X_train, yx_train)\n",
        "    preds_x = model_x.predict(X_test)\n",
        "    yx_test_np = yx_test\n",
        "\n",
        "# M√©tricas\n",
        "mae_x = np.mean(np.abs(yx_test_np - preds_x))\n",
        "mse_x = np.mean((yx_test_np - preds_x)**2)\n",
        "r2_x = 1 - np.sum((yx_test_np - preds_x)**2) / (np.sum((yx_test_np - yx_test_np.mean())**2) + 1e-12)\n",
        "mape_x = mape_np(yx_test_np, preds_x)\n",
        "boot_x = bootstrap_stats(yx_test_np, preds_x)\n",
        "\n",
        "print(\"\\nüìä RESULTADOS x_target:\")\n",
        "print(f\"MAE  = {mae_x:.6f} (¬± {boot_x['MAE'][1]:.6f})\")\n",
        "print(f\"MSE  = {mse_x:.6f} (¬± {boot_x['MSE'][1]:.6f})\")\n",
        "print(f\"R2   = {r2_x:.6f} (¬± {boot_x['R2'][1]:.6f})\")\n",
        "print(f\"MAPE = {mape_x:.6f} (¬± {boot_x['MAPE'][1]:.6f})\")\n",
        "\n",
        "print(\"\\n‚úÖ LINEAR REGRESSION completado (RAPIDS + BO + m√©tricas).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f5ymZM3qtVU",
        "outputId": "7fb0ade7-3a47-4df4-cb37-4a328f07c0d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ RAPIDS/cuML disponible: se usar√° GPU.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Dataset cargado: 1073215 filas √ó 31 columnas\n",
            "‚úÖ Divisi√≥n 60/20/20 completada.\n",
            "üöÄ Iniciando optimizaci√≥n bayesiana (x_target)...\n",
            "|   iter    |  target   | fit_in... | normalize |\n",
            "-------------------------------------------------\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-394.3432\u001b[39m | \u001b[39m0.3745401\u001b[39m | \u001b[39m0.9507143\u001b[39m |\n",
            "| \u001b[35m2        \u001b[39m | \u001b[35m-0.202661\u001b[39m | \u001b[35m0.7319939\u001b[39m | \u001b[35m0.5986584\u001b[39m |\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m-394.3432\u001b[39m | \u001b[39m0.1560186\u001b[39m | \u001b[39m0.1559945\u001b[39m |\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m-394.3432\u001b[39m | \u001b[39m0.0580836\u001b[39m | \u001b[39m0.8661761\u001b[39m |\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m-0.202661\u001b[39m | \u001b[39m0.6011150\u001b[39m | \u001b[39m0.7080725\u001b[39m |\n",
            "| \u001b[39m6        \u001b[39m | \u001b[39m-394.3432\u001b[39m | \u001b[39m0.1834045\u001b[39m | \u001b[39m0.0140796\u001b[39m |\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m-394.3432\u001b[39m | \u001b[39m0.3276550\u001b[39m | \u001b[39m0.9513862\u001b[39m |\n",
            "| \u001b[39m8        \u001b[39m | \u001b[39m-0.202661\u001b[39m | \u001b[39m0.5153929\u001b[39m | \u001b[39m0.7869766\u001b[39m |\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m-0.202661\u001b[39m | \u001b[39m0.8528894\u001b[39m | \u001b[39m0.8127035\u001b[39m |\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m-0.202661\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.5624269\u001b[39m |\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m-0.202661\u001b[39m | \u001b[39m0.8962635\u001b[39m | \u001b[39m0.2519280\u001b[39m |\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m-0.202661\u001b[39m | \u001b[39m0.5864802\u001b[39m | \u001b[39m0.9808633\u001b[39m |\n",
            "| \u001b[39m13       \u001b[39m | \u001b[39m-0.202661\u001b[39m | \u001b[39m0.7942170\u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
            "| \u001b[39m14       \u001b[39m | \u001b[39m-394.3432\u001b[39m | \u001b[39m0.4002158\u001b[39m | \u001b[39m0.2034484\u001b[39m |\n",
            "| \u001b[39m15       \u001b[39m | \u001b[39m-0.202661\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.0      \u001b[39m |\n",
            "| \u001b[39m16       \u001b[39m | \u001b[39m-394.3432\u001b[39m | \u001b[39m0.3725863\u001b[39m | \u001b[39m0.7212937\u001b[39m |\n",
            "| \u001b[39m17       \u001b[39m | \u001b[39m-0.202661\u001b[39m | \u001b[39m0.8997383\u001b[39m | \u001b[39m0.2486596\u001b[39m |\n",
            "| \u001b[39m18       \u001b[39m | \u001b[39m-0.202661\u001b[39m | \u001b[39m0.7640112\u001b[39m | \u001b[39m0.0      \u001b[39m |\n",
            "| \u001b[39m19       \u001b[39m | \u001b[39m-0.202661\u001b[39m | \u001b[39m0.6706888\u001b[39m | \u001b[39m0.8527971\u001b[39m |\n",
            "| \u001b[39m20       \u001b[39m | \u001b[39m-0.202661\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
            "=================================================\n",
            "\n",
            "üèÅ Mejores hiperpar√°metros encontrados (valores continuos optimizados):\n",
            "   - fit_intercept: 0.731994\n",
            "   - normalize: 0.598658\n",
            "\n",
            "‚öôÔ∏è Entrenando modelo final con fit_intercept=True, normalize=True\n",
            "\n",
            "üìä RESULTADOS x_target:\n",
            "MAE  = 0.202458 (¬± 0.000379)\n",
            "MSE  = 0.070195 (¬± 0.000308)\n",
            "R2   = 0.999871 (¬± 0.000001)\n",
            "MAPE = 0.003781 (¬± 0.000010)\n",
            "\n",
            "‚úÖ LINEAR REGRESSION completado (RAPIDS + BO + m√©tricas).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üéØ INFERENCIA LINEAR REGRESSION (RAPIDS) + DETECCI√ìN Y REESCALADO DE UNIDADES\n",
        "# Usa: train_ready_final_numeric.csv + test_input_clean_final.csv\n",
        "# Guarda: /content/drive/MyDrive/predicciones_linear_final_rescaled.csv\n",
        "# ======================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# ---------- GPU / RAPIDS ----------\n",
        "try:\n",
        "    import cudf\n",
        "    from cuml.linear_model import LinearRegression as cuLinearRegression\n",
        "    USE_RAPIDS = True\n",
        "    print(\"‚úÖ RAPIDS/cuML disponible.\")\n",
        "except Exception:\n",
        "    import pandas as pd\n",
        "    from sklearn.linear_model import LinearRegression as skLinearRegression\n",
        "    USE_RAPIDS = False\n",
        "    print(\"‚ö†Ô∏è RAPIDS no disponible; se usar√° scikit-learn (CPU).\")\n",
        "\n",
        "# ---------- Montar Drive ----------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "train_path = \"/content/drive/MyDrive/train_ready_final_numeric.csv\"\n",
        "test_path  = \"/content/drive/MyDrive/test_input_clean_final.csv\"\n",
        "out_path   = \"/content/drive/MyDrive/predicciones_linear_final_rescaled.csv\"\n",
        "\n",
        "# ---------- Cargar datos ----------\n",
        "if USE_RAPIDS:\n",
        "    df_train = cudf.read_csv(train_path)\n",
        "    df_test  = cudf.read_csv(test_path)\n",
        "else:\n",
        "    import pandas as pd\n",
        "    df_train = pd.read_csv(train_path)\n",
        "    df_test  = pd.read_csv(test_path)\n",
        "\n",
        "print(f\"‚úÖ Train cargado: {df_train.shape[0]} filas √ó {df_train.shape[1]} cols\")\n",
        "print(f\"‚úÖ Test  cargado: {df_test.shape[0]} filas √ó {df_test.shape[1]} cols\")\n",
        "\n",
        "# ---------- Detectar targets en train ----------\n",
        "if \"x_target\" in df_train.columns and \"y_target\" in df_train.columns:\n",
        "    TARGET_X, TARGET_Y = \"x_target\", \"y_target\"\n",
        "elif \"ball_land_x\" in df_train.columns and \"ball_land_y\" in df_train.columns:\n",
        "    TARGET_X, TARGET_Y = \"ball_land_x\", \"ball_land_y\"\n",
        "else:\n",
        "    raise ValueError(\"No encuentro columnas de objetivo en train (x_target/y_target o ball_land_x/y).\")\n",
        "\n",
        "# ---------- Definir columnas prohibidas ----------\n",
        "leak_or_bad = {\n",
        "    \"x_target\", \"y_target\", \"ball_land_x\", \"ball_land_y\",\n",
        "    \"dist_to_ball\", \"angle_to_ball\", \"vel_toward_ball\",\n",
        "    \"x\", \"y\", \"o\", \"dir\",\n",
        "    \"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"play_direction\",\n",
        "    \"player_name\", \"player_height\", \"player_birth_date\",\n",
        "    \"num_frames_output\"\n",
        "}\n",
        "\n",
        "# ---------- Construir feature_cols ----------\n",
        "train_cols = [c for c in df_train.columns if c not in leak_or_bad]\n",
        "feature_cols = [c for c in train_cols if c in df_test.columns]\n",
        "\n",
        "if len(feature_cols) == 0:\n",
        "    raise ValueError(\"No quedaron columnas comunes v√°lidas entre train y test.\")\n",
        "\n",
        "print(f\"üîπ Features finales (comunes y sin fuga): {len(feature_cols)}\")\n",
        "\n",
        "# ---------- Preparar matrices ----------\n",
        "X_train_full = df_train[feature_cols].astype(\"float32\").fillna(0)\n",
        "yx_full = df_train[TARGET_X].astype(\"float32\")\n",
        "yy_full = df_train[TARGET_Y].astype(\"float32\")\n",
        "X_pred = df_test[feature_cols].astype(\"float32\").fillna(0)\n",
        "\n",
        "# ======================================================\n",
        "# üîç DETECCI√ìN DE ESCALA Y REESCALADO (si aplica)\n",
        "# ======================================================\n",
        "def detect_scale(series, label):\n",
        "    \"\"\"Detecta si una columna parece estar en yardas reales o en z-score.\"\"\"\n",
        "    mean, std = float(series.mean()), float(series.std())\n",
        "    if mean < -5 or mean > 200 or std > 100:  # fuera del rango f√≠sico t√≠pico\n",
        "        scale = \"‚ö†Ô∏è No est√° en yardas (escala an√≥mala)\"\n",
        "    else:\n",
        "        scale = \"‚úÖ Yardas reales\"\n",
        "    print(f\"   ‚Üí {label}: media={mean:.3f}, std={std:.3f} ‚Üí {scale}\")\n",
        "    return mean, std, scale\n",
        "\n",
        "print(\"\\nüîç Verificando escala de coordenadas en TRAIN:\")\n",
        "train_x_mean, train_x_std, train_x_scale = detect_scale(yx_full, TARGET_X)\n",
        "train_y_mean, train_y_std, train_y_scale = detect_scale(yy_full, TARGET_Y)\n",
        "\n",
        "# ======================================================\n",
        "# üß† ENTRENAMIENTO Y PREDICCI√ìN\n",
        "# ======================================================\n",
        "best_fit_intercept = 0.731994\n",
        "best_normalize = 0.598658\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è Entrenando modelo final con fit_intercept={best_fit_intercept}, normalize={best_normalize}\")\n",
        "\n",
        "if USE_RAPIDS:\n",
        "    model_x = cuLinearRegression(fit_intercept=best_fit_intercept)\n",
        "    model_y = cuLinearRegression(fit_intercept=best_fit_intercept)\n",
        "    model_x.fit(X_train_full, yx_full)\n",
        "    model_y.fit(X_train_full, yy_full)\n",
        "    df_test[\"x_pred\"] = model_x.predict(X_pred)\n",
        "    df_test[\"y_pred\"] = model_y.predict(X_pred)\n",
        "else:\n",
        "    model_x = skLinearRegression(fit_intercept=best_fit_intercept)\n",
        "    model_y = skLinearRegression(fit_intercept=best_fit_intercept)\n",
        "    model_x.fit(X_train_full, yx_full)\n",
        "    model_y.fit(X_train_full, yy_full)\n",
        "    df_test[\"x_pred\"] = model_x.predict(X_pred)\n",
        "    df_test[\"y_pred\"] = model_y.predict(X_pred)\n",
        "\n",
        "print(\"\\n‚úÖ Predicci√≥n completada.\")\n",
        "print(df_test[[\"x_pred\", \"y_pred\"]].head())\n",
        "\n",
        "# ======================================================\n",
        "# üßÆ REESCALADO DE PREDICCIONES A UNIDADES F√çSICAS (YARDAS)\n",
        "# ======================================================\n",
        "def rescale_to_field(preds, new_min, new_max):\n",
        "    \"\"\"Reescala linealmente al rango f√≠sico del campo.\"\"\"\n",
        "    old_min, old_max = float(preds.min()), float(preds.max())\n",
        "    if abs(old_max - old_min) < 1e-6:\n",
        "        return np.clip(preds, new_min, new_max)\n",
        "    return (preds - old_min) / (old_max - old_min) * (new_max - new_min) + new_min\n",
        "\n",
        "df_test[\"x_pred_rescaled\"] = rescale_to_field(df_test[\"x_pred\"], 0, 120)\n",
        "df_test[\"y_pred_rescaled\"] = rescale_to_field(df_test[\"y_pred\"], 0, 53.3)\n",
        "\n",
        "print(\"\\nüìè Reescalado completado:\")\n",
        "print(df_test[[\"x_pred\", \"y_pred\", \"x_pred_rescaled\", \"y_pred_rescaled\"]].head())\n",
        "\n",
        "# ======================================================\n",
        "# üíæ GUARDAR RESULTADOS\n",
        "# ======================================================\n",
        "# Descomenta las siguientes lineas para que el codigo guarde una copia del dataset de prediccion en tu dirve!\n",
        "# df_test.to_csv(out_path, index=False)\n",
        "# print(f\"\\n‚úÖ Archivo final guardado en: {out_path}\")\n",
        "# print(\"üéØ Predicciones reescaladas en yardas reales listas para interpretaci√≥n.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K78mneVtNF0",
        "outputId": "b304f1cf-5e6c-40a5-ccc0-0677ca41542d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ RAPIDS/cuML disponible.\n",
            "Mounted at /content/drive\n",
            "‚úÖ Train cargado: 1073215 filas √ó 31 cols\n",
            "‚úÖ Test  cargado: 49753 filas √ó 27 cols\n",
            "üîπ Features finales (comunes y sin fuga): 18\n",
            "\n",
            "üîç Verificando escala de coordenadas en TRAIN:\n",
            "   ‚Üí x_target: media=62.985, std=23.335 ‚Üí ‚úÖ Yardas reales\n",
            "   ‚Üí y_target: media=26.418, std=10.290 ‚Üí ‚úÖ Yardas reales\n",
            "\n",
            "‚öôÔ∏è Entrenando modelo final con fit_intercept=0.731994, normalize=0.598658\n",
            "\n",
            "‚úÖ Predicci√≥n completada.\n",
            "       x_pred      y_pred\n",
            "0  880.446899 -690.824280\n",
            "1  883.372131 -690.896423\n",
            "2  887.954041 -690.877197\n",
            "3  893.623413 -690.869690\n",
            "4  900.612122 -690.824097\n",
            "\n",
            "üìè Reescalado completado:\n",
            "       x_pred      y_pred  x_pred_rescaled  y_pred_rescaled\n",
            "0  880.446899 -690.824280        34.415272        33.374363\n",
            "1  883.372131 -690.896423        34.527775        33.364491\n",
            "2  887.954041 -690.877197        34.704002        33.367123\n",
            "3  893.623413 -690.869690        34.922047        33.368149\n",
            "4  900.612122 -690.824097        35.190838        33.374390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo Linear Regression optimizado con Bayesian Optimization y acelerado con RAPIDS demostr√≥ ser un m√©todo eficiente, simple y r√°pido incluso con grandes vol√∫menes de datos, gracias a su baja complejidad computacional y ausencia de necesidad de submuestreo. Los hiperpar√°metros obtenidos (fit_intercept ‚âà 0.73, normalize ‚âà 0.59) indican un ajuste lineal con ligera normalizaci√≥n, adecuado para relaciones proporcionales entre variables. Sin embargo, su capacidad predictiva es limitada ante interacciones no lineales o dependencias complejas, por lo que, aunque eficiente como modelo base y de referencia, no resulta el m√°s apropiado para la predicci√≥n final en la competencia de la NFL."
      ],
      "metadata": {
        "id": "4wdntV8EzG_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lasso**"
      ],
      "metadata": {
        "id": "cNnIHHQ0ozQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Despliegue matem√°tico, hiperpar√°metros y funci√≥n objetivo**\n",
        "\n",
        "---\n",
        "\n",
        "## Fundamento te√≥rico del modelo LASSO\n",
        "\n",
        "El modelo **LASSO (Least Absolute Shrinkage and Selection Operator)** extiende la regresi√≥n lineal incorporando una penalizaci√≥n sobre la norma $$L_1$$ de los coeficientes, promoviendo la **sparsidad** (es decir, algunos coeficientes se vuelven exactamente cero).\n",
        "\n",
        "La formulaci√≥n del problema de optimizaci√≥n es:\n",
        "\n",
        "$$\n",
        "\\min_{\\boldsymbol{\\beta}} \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2 + \\alpha \\|\\boldsymbol{\\beta}\\|_1\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $$\\mathbf{x}_i$$ es el vector de caracter√≠sticas de la observaci√≥n $$i$$,\n",
        "- $$y_i$$ es el valor observado,\n",
        "- $$\\boldsymbol{\\beta}$$ son los coeficientes del modelo,\n",
        "- $$\\alpha > 0$$ es el par√°metro de regularizaci√≥n que controla el grado de penalizaci√≥n.\n",
        "\n",
        "El t√©rmino de penalizaci√≥n $$\\alpha \\|\\boldsymbol{\\beta}\\|_1$$ a√±ade una restricci√≥n de tipo $$L_1$$:\n",
        "\n",
        "$$\n",
        "\\|\\boldsymbol{\\beta}\\|_1 = \\sum_{j=1}^{p} |\\beta_j|\n",
        "$$\n",
        "\n",
        "Esta penalizaci√≥n induce **sparsidad**, eliminando caracter√≠sticas irrelevantes y reduciendo el sobreajuste.\n",
        "\n",
        "---\n",
        "\n",
        "##  Interpretaci√≥n geom√©trica\n",
        "\n",
        "La soluci√≥n de LASSO se obtiene en la intersecci√≥n entre las elipses de error cuadr√°tico y el **poliedro** definido por la restricci√≥n $$L_1$$:\n",
        "\n",
        "$$\n",
        "\\sum_{j=1}^{p} |\\beta_j| \\leq t\n",
        "$$\n",
        "\n",
        "A diferencia de Ridge (que usa norma $$L_2$$ y produce soluciones suaves), LASSO tiende a producir soluciones con varios coeficientes exactamente iguales a cero, promoviendo selecci√≥n de variables.\n",
        "\n",
        "---\n",
        "\n",
        "##  Rejilla de hiperpar√°metros y rango de b√∫squeda\n",
        "\n",
        "La **Optimizaci√≥n Bayesiana (BO)** se usa para encontrar los mejores valores de los hiperpar√°metros:\n",
        "\n",
        "| Hiperpar√°metro | Tipo | Rango definido | Descripci√≥n | Efecto esperado |\n",
        "|----------------|------|----------------|--------------|-----------------|\n",
        "| `alpha` | Continua | $$[10^{-4}, 1.0]$$ | Controla la penalizaci√≥n $$L_1$$. Cuanto mayor sea $$\\alpha$$, m√°s coeficientes se anulan. | Regularizaci√≥n y sparsidad. |\n",
        "| `max_iter` | Entera | $$[500, 2000]$$ | N√∫mero m√°ximo de iteraciones del optimizador. | Estabilidad y convergencia del modelo. |\n",
        "\n",
        "Formalmente, el espacio de b√∫squeda es:\n",
        "\n",
        "$$\n",
        "\\mathcal{H} = \\{\\, \\theta = (\\alpha,\\, \\text{max\\_iter}) \\in [10^{-4}, 1.0] \\times [500, 2000] \\,\\}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "##  Funci√≥n objetivo (Score a minimizar)\n",
        "\n",
        "La m√©trica empleada para optimizar los hiperpar√°metros es el **Error Absoluto Medio (MAE)** en validaci√≥n:\n",
        "\n",
        "$$\n",
        "MAE(\\theta) = \\frac{1}{n_{\\text{val}}} \\sum_{i=1}^{n_{\\text{val}}} |y_i - \\hat{y}_i(\\theta)|\n",
        "$$\n",
        "\n",
        "Sin embargo, dado que la **Bayesian Optimization maximiza** la funci√≥n objetivo, se utiliza su negaci√≥n:\n",
        "\n",
        "$$\n",
        "f(\\theta) = -MAE(\\theta)\n",
        "$$\n",
        "\n",
        "As√≠, el problema de optimizaci√≥n se formula como:\n",
        "\n",
        "$$\n",
        "\\max_{\\theta \\in \\mathcal{H}} f(\\theta) = -MAE(\\theta)\n",
        "\\quad \\Longleftrightarrow \\quad\n",
        "\\min_{\\theta \\in \\mathcal{H}} MAE(\\theta)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "##  Proceso Gaussiano en la Optimizaci√≥n Bayesiana\n",
        "\n",
        "El modelo probabil√≠stico subyacente a la BO es un **Proceso Gaussiano (GP)**:\n",
        "\n",
        "$$\n",
        "f(\\theta) \\sim \\mathcal{GP}(m(\\theta), k(\\theta, \\theta'))\n",
        "$$\n",
        "\n",
        "con media $$m(\\theta)$$ (usualmente cero) y covarianza $$k(\\theta, \\theta')$$.\n",
        "\n",
        "Dadas las observaciones hasta el tiempo $$t$$, la predicci√≥n para un nuevo punto $$\\theta_*$$ es:\n",
        "\n",
        "$$\n",
        "f(\\theta_*) | \\mathcal{D}_t \\sim \\mathcal{N}(\\mu_t(\\theta_*), \\sigma_t^2(\\theta_*))\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "$$\n",
        "\\mu_t(\\theta_*) = k_*^\\top (K + \\sigma_n^2 I)^{-1} \\mathbf{f}, \\qquad\n",
        "\\sigma_t^2(\\theta_*) = k(\\theta_*,\\theta_*) - k_*^\\top (K + \\sigma_n^2 I)^{-1} k_*\n",
        "$$\n",
        "\n",
        "- $$K$$: matriz de covarianza de los puntos ya evaluados,  \n",
        "- $$\\sigma_n^2$$: varianza del ruido,  \n",
        "- $$\\mathbf{f}$$: valores de la funci√≥n observados.\n",
        "\n",
        "---\n",
        "\n",
        "##  Funci√≥n de adquisici√≥n ‚Äì Expected Improvement (EI)\n",
        "\n",
        "El pr√≥ximo punto a evaluar se selecciona maximizando el **Expected Improvement**:\n",
        "\n",
        "$$\n",
        "EI(\\theta) = \\mathbb{E}\\left[\\max(0, f(\\theta) - f^+)\\right]\n",
        "$$\n",
        "\n",
        "donde $$f^+$$ es el mejor valor actual.  \n",
        "Si $$f(\\theta) \\sim \\mathcal{N}(\\mu, \\sigma^2)$$, entonces:\n",
        "\n",
        "$$\n",
        "EI(\\theta) = (\\mu - f^+) \\Phi(z) + \\sigma \\phi(z)\n",
        "$$\n",
        "\n",
        "con:\n",
        "\n",
        "$$\n",
        "z = \\frac{\\mu - f^+}{\\sigma}\n",
        "$$\n",
        "\n",
        "y $$\\Phi(\\cdot)$$, $$\\phi(\\cdot)$$ las funciones de distribuci√≥n y densidad de la normal est√°ndar.\n",
        "\n",
        "---\n",
        "\n",
        "## Proceso de b√∫squeda\n",
        "\n",
        "La BO se ejecuta con:\n",
        "- $$5$$ puntos iniciales aleatorios ($$init\\_points = 5$$),\n",
        "- $$15$$ iteraciones guiadas ($$n\\_iter = 15$$).\n",
        "\n",
        "Total de $$20$$ configuraciones evaluadas, equilibrando **exploraci√≥n y explotaci√≥n**.\n",
        "\n",
        "---\n",
        "\n",
        "##  Entrenamiento final y evaluaci√≥n\n",
        "\n",
        "Una vez determinados los hiperpar√°metros √≥ptimos $$\\theta^* = (\\alpha^*, \\text{max\\_iter}^*)$$, el modelo se reentrena y eval√∫a sobre el conjunto de prueba con las m√©tricas:\n",
        "\n",
        "$$\n",
        "MAE  = \\frac{1}{n}\\sum |y_i - \\hat{y}_i|\n",
        "$$\n",
        "\n",
        "$$\n",
        "MSE  = \\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "R^2  = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
        "$$\n",
        "\n",
        "$$\n",
        "MAPE = \\frac{100}{n}\\sum \\left|\\frac{y_i - \\hat{y}_i}{\\max(|y_i|, \\varepsilon)}\\right|\n",
        "$$\n",
        "\n",
        "Para estimar la variabilidad de estas m√©tricas se aplica **bootstrap** con $$B = 20$$ remuestreos:\n",
        "\n",
        "$$\n",
        "\\sigma_m = \\sqrt{\\frac{1}{B-1}\\sum_{b=1}^{B} (m_b - \\bar{m})^2}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "##  Reescalado f√≠sico de predicciones\n",
        "\n",
        "Las predicciones se reescalan al rango f√≠sico del campo de juego (en yardas):\n",
        "\n",
        "$$\n",
        "x' = \\frac{(x - x_{\\min})}{(x_{\\max} - x_{\\min})} (x_{\\text{newmax}} - x_{\\text{newmin}}) + x_{\\text{newmin}}\n",
        "$$\n",
        "\n",
        "Aplicado tanto a coordenadas $$x$$ como $$y$$:\n",
        "\n",
        "$$\n",
        "x' \\in [0, 120], \\qquad y' \\in [0, 53.3]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "##  Conclusi√≥n te√≥rico-computacional\n",
        "\n",
        "- LASSO impone una **penalizaci√≥n $$L_1$$** que tiende a generar modelos m√°s simples y estables.  \n",
        "- La **Optimizaci√≥n Bayesiana** encuentra el equilibrio √≥ptimo entre regularizaci√≥n y error de validaci√≥n.  \n",
        "- El **score a minimizar** es $$MAE_{\\text{val}}(\\theta)$$.  \n",
        "- Se asegura convergencia y robustez mediante $$max\\_iter \\in [500, 2000]$$ y validaci√≥n cruzada impl√≠cita.  \n",
        "- Finalmente, las predicciones se reescalan al dominio f√≠sico del problema, garantizando interpretabilidad.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Pfbsvg8hQgx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üß† PUNTO 4 ‚Äì MODELO 2: LASSO (RAPIDS + BO + m√©tricas)\n",
        "# Integraci√≥n con Google Drive para datasets grandes\n",
        "# ======================================================\n",
        "\n",
        "# ---------- Instalaciones necesarias ----------\n",
        "!pip -q install bayesian-optimization\n",
        "\n",
        "# (Opcional) instalar RAPIDS si hay GPU\n",
        "!pip -q install cudf-cu12 cuml-cu12 --extra-index-url=https://pypi.nvidia.com\n",
        "\n",
        "# ---------- Imports ----------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ---------- Intentar usar RAPIDS ----------\n",
        "USE_RAPIDS = False\n",
        "try:\n",
        "    import cudf\n",
        "    from cuml.linear_model import Lasso as cuLasso\n",
        "    USE_RAPIDS = True\n",
        "    print(\"‚úÖ RAPIDS/cuML disponible: se usar√° GPU.\")\n",
        "except Exception as e:\n",
        "    from sklearn.linear_model import Lasso as skLasso\n",
        "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "    print(\"‚ö†Ô∏è RAPIDS/cuML no disponible. Se usar√° scikit-learn (CPU).\")\n",
        "\n",
        "# ---------- Funciones auxiliares ----------\n",
        "def mape_np(y_true, y_pred, eps=1e-8):\n",
        "    denom = np.maximum(np.abs(y_true), eps)\n",
        "    return np.mean(np.abs((y_true - y_pred) / denom))\n",
        "\n",
        "def bootstrap_stats(y_true, y_pred, n_boot=20, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(y_true)\n",
        "    maes, mses, r2s, mapes = [], [], [], []\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        yt, yp = y_true[idx], y_pred[idx]\n",
        "        maes.append(np.mean(np.abs(yt - yp)))\n",
        "        mses.append(np.mean((yt - yp)**2))\n",
        "        ss_res = np.sum((yt - yp)**2)\n",
        "        ss_tot = np.sum((yt - yt.mean())**2) + 1e-12\n",
        "        r2s.append(1 - ss_res/ss_tot)\n",
        "        mapes.append(mape_np(yt, yp))\n",
        "    def stats(a): return (np.mean(a), np.std(a, ddof=1))\n",
        "    return {\"MAE\": stats(maes), \"MSE\": stats(mses), \"R2\": stats(r2s), \"MAPE\": stats(mapes)}\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ MONTAJE DE GOOGLE DRIVE Y CARGA DEL DATASET\n",
        "# ======================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# üëá Cambia esta ruta por donde guardes tu CSV en tu Drive\n",
        "path = \"/content/drive/MyDrive/train_ready_final_numeric.csv\"\n",
        "\n",
        "# Cargar dataset\n",
        "if USE_RAPIDS:\n",
        "    df = cudf.read_csv(path)\n",
        "else:\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "print(f\"‚úÖ Dataset cargado: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ PREPARACI√ìN DE DATOS\n",
        "# ======================================================\n",
        "TARGET_X, TARGET_Y = \"x_target\", \"y_target\"\n",
        "X = df.drop(columns=[TARGET_X, TARGET_Y])\n",
        "y_x, y_y = df[TARGET_X], df[TARGET_Y]\n",
        "\n",
        "# Divisi√≥n 60/20/20\n",
        "X_train, X_temp, yx_train, yx_temp, yy_train, yy_temp = train_test_split(X, y_x, y_y, test_size=0.4, random_state=42)\n",
        "X_val, X_test, yx_val, yx_test, yy_val, yy_test = train_test_split(X_temp, yx_temp, yy_temp, test_size=0.5, random_state=42)\n",
        "print(\"‚úÖ Divisi√≥n 60/20/20 completada.\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ OPTIMIZACI√ìN BAYESIANA PARA LASSO (x_target)\n",
        "# ======================================================\n",
        "def lasso_eval(alpha, max_iter):\n",
        "    if USE_RAPIDS:\n",
        "        model = cuLasso(alpha=alpha, max_iter=int(max_iter))\n",
        "        model.fit(X_train, yx_train)\n",
        "        preds = model.predict(X_val)\n",
        "        mae = float(np.mean(np.abs(yx_val.to_numpy() - preds.to_numpy())))\n",
        "    else:\n",
        "        model = skLasso(alpha=alpha, max_iter=int(max_iter), random_state=42)\n",
        "        model.fit(X_train, yx_train)\n",
        "        preds = model.predict(X_val)\n",
        "        mae = mean_absolute_error(yx_val, preds)\n",
        "    return -mae\n",
        "\n",
        "pbounds = {\"alpha\": (1e-4, 1.0), \"max_iter\": (500, 2000)}\n",
        "\n",
        "optimizer_x = BayesianOptimization(f=lasso_eval, pbounds=pbounds, random_state=42, verbose=2)\n",
        "print(\"üöÄ Iniciando optimizaci√≥n bayesiana (x_target)...\")\n",
        "optimizer_x.maximize(init_points=5, n_iter=15)\n",
        "best_x = optimizer_x.max[\"params\"]\n",
        "print(\"üèÅ Mejores hiperpar√°metros:\", best_x)\n",
        "\n",
        "# Entrenamiento final\n",
        "if USE_RAPIDS:\n",
        "    model_x = cuLasso(alpha=best_x[\"alpha\"], max_iter=int(best_x[\"max_iter\"]))\n",
        "    model_x.fit(X_train, yx_train)\n",
        "    preds_x = model_x.predict(X_test).to_numpy()\n",
        "    yx_test_np = yx_test.to_numpy()\n",
        "else:\n",
        "    model_x = skLasso(alpha=best_x[\"alpha\"], max_iter=int(best_x[\"max_iter\"]), random_state=42)\n",
        "    model_x.fit(X_train, yx_train)\n",
        "    preds_x = model_x.predict(X_test)\n",
        "    yx_test_np = yx_test\n",
        "\n",
        "# M√©tricas\n",
        "mae_x = np.mean(np.abs(yx_test_np - preds_x))\n",
        "mse_x = np.mean((yx_test_np - preds_x)**2)\n",
        "r2_x = 1 - np.sum((yx_test_np - preds_x)**2) / (np.sum((yx_test_np - yx_test_np.mean())**2) + 1e-12)\n",
        "mape_x = mape_np(yx_test_np, preds_x)\n",
        "boot_x = bootstrap_stats(yx_test_np, preds_x)\n",
        "\n",
        "print(\"\\nüìä RESULTADOS x_target:\")\n",
        "print(f\"MAE  = {mae_x:.6f} (¬± {boot_x['MAE'][1]:.6f})\")\n",
        "print(f\"MSE  = {mse_x:.6f} (¬± {boot_x['MSE'][1]:.6f})\")\n",
        "print(f\"R2   = {r2_x:.6f} (¬± {boot_x['R2'][1]:.6f})\")\n",
        "print(f\"MAPE = {mape_x:.6f} (¬± {boot_x['MAPE'][1]:.6f})\")\n",
        "\n",
        "print(\"\\n‚úÖ LASSO completado (Google Drive + BO + m√©tricas).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOR4tCzgqyml",
        "outputId": "af1fb15a-462d-40c8-ad10-34d4f2c4e9da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ RAPIDS/cuML disponible: se usar√° GPU.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Dataset cargado: 1073215 filas √ó 31 columnas\n",
            "‚úÖ Divisi√≥n 60/20/20 completada.\n",
            "üöÄ Iniciando optimizaci√≥n bayesiana (x_target)...\n",
            "|   iter    |  target   |   alpha   | max_iter  |\n",
            "-------------------------------------------------\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-0.508425\u001b[39m | \u001b[39m0.3746026\u001b[39m | \u001b[39m1926.0714\u001b[39m |\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m-0.917556\u001b[39m | \u001b[39m0.7320207\u001b[39m | \u001b[39m1397.9877\u001b[39m |\n",
            "| \u001b[35m3        \u001b[39m | \u001b[35m-0.296124\u001b[39m | \u001b[35m0.1561030\u001b[39m | \u001b[35m733.99178\u001b[39m |\n",
            "| \u001b[35m4        \u001b[39m | \u001b[35m-0.223547\u001b[39m | \u001b[35m0.0581778\u001b[39m | \u001b[35m1799.2642\u001b[39m |\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m-0.764375\u001b[39m | \u001b[39m0.6011549\u001b[39m | \u001b[39m1562.1088\u001b[39m |\n",
            "| \u001b[39m6        \u001b[39m | \u001b[39m-0.826647\u001b[39m | \u001b[39m0.6545669\u001b[39m | \u001b[39m1798.2363\u001b[39m |\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m-0.457679\u001b[39m | \u001b[39m0.3277222\u001b[39m | \u001b[39m1927.0793\u001b[39m |\n",
            "| \u001b[39m8        \u001b[39m | \u001b[39m-0.665574\u001b[39m | \u001b[39m0.5154414\u001b[39m | \u001b[39m1680.4650\u001b[39m |\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m-0.231149\u001b[39m | \u001b[39m0.0678791\u001b[39m | \u001b[39m1799.3819\u001b[39m |\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m-0.348859\u001b[39m | \u001b[39m0.2195961\u001b[39m | \u001b[39m733.11926\u001b[39m |\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m-1.177412\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m733.79930\u001b[39m |\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m-0.748053\u001b[39m | \u001b[39m0.5865215\u001b[39m | \u001b[39m1971.2950\u001b[39m |\n",
            "| \u001b[35m13       \u001b[39m | \u001b[35m-0.196285\u001b[39m | \u001b[35m0.0001   \u001b[39m | \u001b[35m733.56846\u001b[39m |\n",
            "| \u001b[39m14       \u001b[39m | \u001b[39m-0.945951\u001b[39m | \u001b[39m0.7561257\u001b[39m | \u001b[39m1799.5166\u001b[39m |\n",
            "| \u001b[39m15       \u001b[39m | \u001b[39m-0.196285\u001b[39m | \u001b[39m0.0001   \u001b[39m | \u001b[39m734.57811\u001b[39m |\n",
            "| \u001b[39m16       \u001b[39m | \u001b[39m-0.196285\u001b[39m | \u001b[39m0.0001   \u001b[39m | \u001b[39m735.20381\u001b[39m |\n",
            "| \u001b[39m17       \u001b[39m | \u001b[39m-0.835772\u001b[39m | \u001b[39m0.6623662\u001b[39m | \u001b[39m735.55990\u001b[39m |\n",
            "| \u001b[39m18       \u001b[39m | \u001b[39m-0.196285\u001b[39m | \u001b[39m0.0001   \u001b[39m | \u001b[39m732.32003\u001b[39m |\n",
            "| \u001b[39m19       \u001b[39m | \u001b[39m-0.527182\u001b[39m | \u001b[39m0.3916456\u001b[39m | \u001b[39m731.77185\u001b[39m |\n",
            "| \u001b[39m20       \u001b[39m | \u001b[39m-1.177412\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1927.9536\u001b[39m |\n",
            "=================================================\n",
            "üèÅ Mejores hiperpar√°metros: {'alpha': np.float64(0.0001), 'max_iter': np.float64(733.5684699685066)}\n",
            "\n",
            "üìä RESULTADOS x_target:\n",
            "MAE  = 0.195758 (¬± 0.000411)\n",
            "MSE  = 0.064829 (¬± 0.000340)\n",
            "R2   = 0.999881 (¬± 0.000001)\n",
            "MAPE = 0.003649 (¬± 0.000010)\n",
            "\n",
            "‚úÖ LASSO completado (Google Drive + BO + m√©tricas).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üéØ INFERENCIA LASSO (RAPIDS) + DETECCI√ìN Y REESCALADO DE UNIDADES\n",
        "# Usa: train_ready_final_numeric.csv + test_input_clean_final.csv\n",
        "# Guarda: /content/drive/MyDrive/predicciones_lasso_final_rescaled.csv\n",
        "# ======================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# ---------- GPU / RAPIDS ----------\n",
        "try:\n",
        "    import cudf\n",
        "    from cuml.linear_model import Lasso as cuLasso\n",
        "    USE_RAPIDS = True\n",
        "    print(\"‚úÖ RAPIDS/cuML disponible.\")\n",
        "except Exception:\n",
        "    import pandas as pd\n",
        "    from sklearn.linear_model import Lasso as skLasso\n",
        "    USE_RAPIDS = False\n",
        "    print(\"‚ö†Ô∏è RAPIDS no disponible; se usar√° scikit-learn (CPU).\")\n",
        "\n",
        "# ---------- Montar Drive ----------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "train_path = \"/content/drive/MyDrive/train_ready_final_numeric.csv\"\n",
        "test_path  = \"/content/drive/MyDrive/test_input_clean_final.csv\"\n",
        "out_path   = \"/content/drive/MyDrive/predicciones_lasso_final_rescaled.csv\"\n",
        "\n",
        "# ---------- Cargar datos ----------\n",
        "if USE_RAPIDS:\n",
        "    df_train = cudf.read_csv(train_path)\n",
        "    df_test  = cudf.read_csv(test_path)\n",
        "else:\n",
        "    import pandas as pd\n",
        "    df_train = pd.read_csv(train_path)\n",
        "    df_test  = pd.read_csv(test_path)\n",
        "\n",
        "print(f\"‚úÖ Train cargado: {df_train.shape[0]} filas √ó {df_train.shape[1]} cols\")\n",
        "print(f\"‚úÖ Test  cargado: {df_test.shape[0]} filas √ó {df_test.shape[1]} cols\")\n",
        "\n",
        "# ---------- Detectar targets en train ----------\n",
        "if \"x_target\" in df_train.columns and \"y_target\" in df_train.columns:\n",
        "    TARGET_X, TARGET_Y = \"x_target\", \"y_target\"\n",
        "elif \"ball_land_x\" in df_train.columns and \"ball_land_y\" in df_train.columns:\n",
        "    TARGET_X, TARGET_Y = \"ball_land_x\", \"ball_land_y\"\n",
        "else:\n",
        "    raise ValueError(\"No encuentro columnas de objetivo en train (x_target/y_target o ball_land_x/y).\")\n",
        "\n",
        "# ---------- Definir columnas prohibidas ----------\n",
        "leak_or_bad = {\n",
        "    \"x_target\", \"y_target\", \"ball_land_x\", \"ball_land_y\",\n",
        "    \"dist_to_ball\", \"angle_to_ball\", \"vel_toward_ball\",\n",
        "    \"x\", \"y\", \"o\", \"dir\",\n",
        "    \"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"play_direction\",\n",
        "    \"player_name\", \"player_height\", \"player_birth_date\",\n",
        "    \"num_frames_output\"\n",
        "}\n",
        "\n",
        "# ---------- Construir feature_cols ----------\n",
        "train_cols = [c for c in df_train.columns if c not in leak_or_bad]\n",
        "feature_cols = [c for c in train_cols if c in df_test.columns]\n",
        "\n",
        "if len(feature_cols) == 0:\n",
        "    raise ValueError(\"No quedaron columnas comunes v√°lidas entre train y test.\")\n",
        "\n",
        "print(f\"üîπ Features finales (comunes y sin fuga): {len(feature_cols)}\")\n",
        "\n",
        "# ---------- Preparar matrices ----------\n",
        "X_train_full = df_train[feature_cols].astype(\"float32\").fillna(0)\n",
        "yx_full = df_train[TARGET_X].astype(\"float32\")\n",
        "yy_full = df_train[TARGET_Y].astype(\"float32\")\n",
        "X_pred = df_test[feature_cols].astype(\"float32\").fillna(0)\n",
        "\n",
        "# ======================================================\n",
        "# üîç DETECCI√ìN DE ESCALA Y REESCALADO (si aplica)\n",
        "# ======================================================\n",
        "def detect_scale(series, label):\n",
        "    \"\"\"Detecta si una columna parece estar en yardas reales o en z-score.\"\"\"\n",
        "    mean, std = float(series.mean()), float(series.std())\n",
        "    if mean < -5 or mean > 200 or std > 100:  # fuera del rango f√≠sico\n",
        "        scale = \"No est√° en yardas (escala an√≥mala)\"\n",
        "    else:\n",
        "        scale = \"Yardas reales\"\n",
        "    print(f\"   ‚Üí {label}: media={mean:.3f}, std={std:.3f} ‚Üí {scale}\")\n",
        "    return mean, std, scale\n",
        "\n",
        "print(\"\\nüîç Verificando escala de coordenadas en TRAIN:\")\n",
        "train_x_mean, train_x_std, train_x_scale = detect_scale(yx_full, TARGET_X)\n",
        "train_y_mean, train_y_std, train_y_scale = detect_scale(yy_full, TARGET_Y)\n",
        "\n",
        "if TARGET_X in df_test.columns or TARGET_Y in df_test.columns:\n",
        "    print(\"\\nüîç Verificando escala de coordenadas en TEST:\")\n",
        "    if TARGET_X in df_test.columns:\n",
        "        detect_scale(df_test[TARGET_X], \"TEST_\" + TARGET_X)\n",
        "    if TARGET_Y in df_test.columns:\n",
        "        detect_scale(df_test[TARGET_Y], \"TEST_\" + TARGET_Y)\n",
        "\n",
        "# ======================================================\n",
        "# üß† ENTRENAMIENTO Y PREDICCI√ìN\n",
        "# ======================================================\n",
        "best_alpha = 0.0001\n",
        "best_max_iter = 734\n",
        "\n",
        "if USE_RAPIDS:\n",
        "    model_x = cuLasso(alpha=best_alpha, max_iter=int(best_max_iter))\n",
        "    model_y = cuLasso(alpha=best_alpha, max_iter=int(best_max_iter))\n",
        "    model_x.fit(X_train_full, yx_full)\n",
        "    model_y.fit(X_train_full, yy_full)\n",
        "    df_test[\"x_pred\"] = model_x.predict(X_pred)\n",
        "    df_test[\"y_pred\"] = model_y.predict(X_pred)\n",
        "else:\n",
        "    model_x = skLasso(alpha=best_alpha, max_iter=int(best_max_iter), random_state=42)\n",
        "    model_y = skLasso(alpha=best_alpha, max_iter=int(best_max_iter), random_state=42)\n",
        "    model_x.fit(X_train_full, yx_full)\n",
        "    model_y.fit(X_train_full, yy_full)\n",
        "    df_test[\"x_pred\"] = model_x.predict(X_pred)\n",
        "    df_test[\"y_pred\"] = model_y.predict(X_pred)\n",
        "\n",
        "print(\"\\n‚úÖ Predicci√≥n completada.\")\n",
        "print(df_test[[\"x_pred\", \"y_pred\"]].head())\n",
        "\n",
        "# ======================================================\n",
        "# üßÆ REESCALADO DE PREDICCIONES A UNIDADES F√çSICAS (YARDAS)\n",
        "# ======================================================\n",
        "def rescale_to_field(preds, new_min, new_max):\n",
        "    \"\"\"Reescala linealmente al rango f√≠sico del campo.\"\"\"\n",
        "    old_min, old_max = float(preds.min()), float(preds.max())\n",
        "    if abs(old_max - old_min) < 1e-6:\n",
        "        return np.clip(preds, new_min, new_max)\n",
        "    return (preds - old_min) / (old_max - old_min) * (new_max - new_min) + new_min\n",
        "\n",
        "df_test[\"x_pred_rescaled\"] = rescale_to_field(df_test[\"x_pred\"], 0, 120)\n",
        "df_test[\"y_pred_rescaled\"] = rescale_to_field(df_test[\"y_pred\"], 0, 53.3)\n",
        "\n",
        "print(\"\\nüìè Reescalado completado:\")\n",
        "print(df_test[[\"x_pred\", \"y_pred\", \"x_pred_rescaled\", \"y_pred_rescaled\"]].head())\n",
        "\n",
        "# ======================================================\n",
        "# üíæ GUARDAR RESULTADOS\n",
        "# ======================================================\n",
        "# Descomenta las siguientes lineas para que el codigo guarde una copia del dataset de prediccion en tu dirve!\n",
        "# df_test.to_csv(out_path, index=False)\n",
        "# print(f\"\\n‚úÖ Archivo final guardado en: {out_path}\")\n",
        "# print(\"üéØ Predicciones reescaladas en yardas reales listas para interpretaci√≥n.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRB8xB3MNhvH",
        "outputId": "1313e2c9-9b07-456f-d630-38044c1a3c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ RAPIDS/cuML disponible.\n",
            "Mounted at /content/drive\n",
            "‚úÖ Train cargado: 1073215 filas √ó 31 cols\n",
            "‚úÖ Test  cargado: 49753 filas √ó 27 cols\n",
            "üîπ Features finales (comunes y sin fuga): 18\n",
            "\n",
            "üîç Verificando escala de coordenadas en TRAIN:\n",
            "   ‚Üí x_target: media=62.985, std=23.335 ‚Üí Yardas reales\n",
            "   ‚Üí y_target: media=26.418, std=10.290 ‚Üí Yardas reales\n",
            "\n",
            "‚úÖ Predicci√≥n completada.\n",
            "        x_pred      y_pred\n",
            "0 -2186.193359  215.495682\n",
            "1 -2183.268799  215.597275\n",
            "2 -2178.678711  215.444717\n",
            "3 -2173.007324  215.452927\n",
            "4 -2166.017090  215.499252\n",
            "\n",
            "üìè Reescalado completado:\n",
            "        x_pred      y_pred  x_pred_rescaled  y_pred_rescaled\n",
            "0 -2186.193359  215.495682        22.281849        18.136154\n",
            "1 -2183.268799  215.597275        22.410295        18.146896\n",
            "2 -2178.678711  215.444717        22.611897        18.130766\n",
            "3 -2173.007324  215.452927        22.860989        18.131636\n",
            "4 -2166.017090  215.499252        23.168005        18.136534\n",
            "\n",
            "‚úÖ Archivo final guardado en: /content/drive/MyDrive/predicciones_lasso_final_rescaled.csv\n",
            "üéØ Predicciones reescaladas en yardas reales listas para interpretaci√≥n.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo Lasso Regression, optimizado con Bayesian Optimization y acelerado mediante RAPIDS, mostr√≥ una buena eficiencia computacional y estabilidad en el entrenamiento, incluso con grandes vol√∫menes de datos. Los hiperpar√°metros obtenidos (alpha ‚âà 0.0001, max_iter ‚âà 734) reflejan una regularizaci√≥n leve que controla la complejidad sin penalizar en exceso los coeficientes, evitando sobreajuste y mejorando la interpretabilidad. Gracias a su regularizaci√≥n L1, el modelo logra seleccionar variables relevantes y reducir ruido, manteniendo una estructura simple y r√°pida de ejecutar."
      ],
      "metadata": {
        "id": "HpU8H6TnzmrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ElasticNet**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4bhwm1Huv2hK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Despliegue matem√°tico, hiperpar√°metros y funci√≥n objetivo**\n",
        "\n",
        "---\n",
        "\n",
        "## Fundamento te√≥rico del modelo ELASTICNET\n",
        "\n",
        "El modelo **ElasticNet** combina las penalizaciones **L1 (LASSO)** y **L2 (Ridge)** para equilibrar **sparsidad** y **estabilidad**.  \n",
        "Su formulaci√≥n matem√°tica es:\n",
        "\n",
        "$$\n",
        "\\min_{\\boldsymbol{\\beta}} \\; \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2\n",
        "+ \\alpha \\left[ (1 - l1\\_ratio) \\frac{1}{2} \\|\\boldsymbol{\\beta}\\|_2^2 + l1\\_ratio \\|\\boldsymbol{\\beta}\\|_1 \\right]\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $$\\mathbf{x}_i \\in \\mathbb{R}^p$$ es el vector de caracter√≠sticas de la observaci√≥n $$i$$,  \n",
        "- $$y_i$$ es el valor observado,  \n",
        "- $$\\boldsymbol{\\beta}$$ son los coeficientes del modelo,  \n",
        "- $$\\alpha > 0$$ controla la magnitud de la regularizaci√≥n total,  \n",
        "- $$l1\\_ratio \\in [0,1]$$ pondera la mezcla entre penalizaci√≥n L1 y L2.\n",
        "\n",
        "Si:\n",
        "- $$l1\\_ratio = 1$$ ‚Üí el modelo se comporta como **LASSO**,  \n",
        "- $$l1\\_ratio = 0$$ ‚Üí el modelo se comporta como **Ridge**.  \n",
        "\n",
        "---\n",
        "\n",
        "## Interpretaci√≥n geom√©trica\n",
        "\n",
        "En el plano de coeficientes, la restricci√≥n de **ElasticNet** define una regi√≥n de factibilidad intermedia entre el **rombo (L1)** y el **c√≠rculo (L2)**:\n",
        "\n",
        "$$\n",
        "(1 - l1\\_ratio)\\|\\boldsymbol{\\beta}\\|_2^2 + 2l1\\_ratio \\|\\boldsymbol{\\beta}\\|_1 \\leq t\n",
        "$$\n",
        "\n",
        "- Si $$l1\\_ratio$$ es alto ‚Üí el contorno tiene esquinas pronunciadas (mayor sparsidad).  \n",
        "- Si $$l1\\_ratio$$ es bajo ‚Üí la forma es m√°s redondeada (menor sparsidad, m√°s estabilidad).\n",
        "\n",
        "Esto permite controlar simult√°neamente la **selecci√≥n de variables** y la **suavidad** del modelo.\n",
        "\n",
        "---\n",
        "\n",
        "## Rejilla de hiperpar√°metros y rango de b√∫squeda\n",
        "\n",
        "La **Optimizaci√≥n Bayesiana (BO)** se utiliza para ajustar los hiperpar√°metros que determinan la regularizaci√≥n y la convergencia del modelo.\n",
        "\n",
        "| Hiperpar√°metro | Tipo | Rango definido | Descripci√≥n | Efecto esperado |\n",
        "|----------------|------|----------------|--------------|-----------------|\n",
        "| `alpha_log` | Continua | $$[-6, 0]$$ ‚Üí $$\\alpha \\in [10^{-6}, 1.0]$$ | Controla la magnitud de regularizaci√≥n total. | Penalizaci√≥n global sobre los coeficientes. |\n",
        "| `l1_ratio` | Continua | $$[0, 1]$$ | Controla la mezcla entre L1 (LASSO) y L2 (Ridge). | Define sparsidad y estabilidad. |\n",
        "| `max_iter` | Entera | $$[500, 3000]$$ | N√∫mero m√°ximo de iteraciones del optimizador. | Asegura convergencia. |\n",
        "| `tol_log` | Continua | $$[-8, -2]$$ ‚Üí $$tol \\in [10^{-8}, 10^{-2}]$$ | Precisi√≥n num√©rica para detener la optimizaci√≥n. | Controla equilibrio entre precisi√≥n y tiempo. |\n",
        "\n",
        "El espacio de b√∫squeda total de la BO se expresa como:\n",
        "\n",
        "$$\n",
        "\\mathcal{H} = \\left\\{ \\theta = (\\alpha, l1\\_ratio, max\\_iter, tol) :\n",
        "\\alpha \\in [10^{-6}, 1], \\;\n",
        "l1\\_ratio \\in [0, 1], \\;\n",
        "max\\_iter \\in [500, 3000], \\;\n",
        "tol \\in [10^{-8}, 10^{-2}] \\right\\}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Funci√≥n objetivo (Score a minimizar)\n",
        "\n",
        "La m√©trica optimizada durante la BO es el **Error Absoluto Medio (MAE)** sobre el conjunto de validaci√≥n:\n",
        "\n",
        "$$\n",
        "MAE(\\theta) = \\frac{1}{n_{\\text{val}}} \\sum_{i=1}^{n_{\\text{val}}} |y_i - \\hat{y}_i(\\theta)|\n",
        "$$\n",
        "\n",
        "Sin embargo, la **Bayesian Optimization** busca **maximizar** una funci√≥n, por lo que se utiliza la versi√≥n negativa:\n",
        "\n",
        "$$\n",
        "f(\\theta) = -MAE(\\theta)\n",
        "$$\n",
        "\n",
        "El problema de optimizaci√≥n se formula as√≠:\n",
        "\n",
        "$$\n",
        "\\max_{\\theta \\in \\mathcal{H}} f(\\theta)\n",
        "\\quad \\Longleftrightarrow \\quad\n",
        "\\min_{\\theta \\in \\mathcal{H}} MAE(\\theta)\n",
        "$$\n",
        "\n",
        "De esta manera, el modelo encuentra los hiperpar√°metros $$\\theta^*$$ que minimizan el error absoluto medio.\n",
        "\n",
        "---\n",
        "\n",
        "## Proceso Gaussiano en la Optimizaci√≥n Bayesiana\n",
        "\n",
        "La **BO** modela la funci√≥n de desempe√±o $$f(\\theta)$$ mediante un **Proceso Gaussiano (GP)**:\n",
        "\n",
        "$$\n",
        "f(\\theta) \\sim \\mathcal{GP}(m(\\theta), k(\\theta, \\theta'))\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $$m(\\theta)$$ es la media (habitualmente cero),  \n",
        "- $$k(\\theta, \\theta')$$ es la funci√≥n de covarianza o kernel.\n",
        "\n",
        "Dadas las observaciones $$\\mathcal{D}_t = \\{(\\theta_i, f_i)\\}_{i=1}^t$$, la predicci√≥n en un nuevo punto $$\\theta_*$$ sigue:\n",
        "\n",
        "$$\n",
        "f(\\theta_*) | \\mathcal{D}_t \\sim \\mathcal{N}(\\mu_t(\\theta_*), \\sigma_t^2(\\theta_*))\n",
        "$$\n",
        "\n",
        "con:\n",
        "\n",
        "$$\n",
        "\\mu_t(\\theta_*) = k_*^\\top (K + \\sigma_n^2 I)^{-1} \\mathbf{f}, \\qquad\n",
        "\\sigma_t^2(\\theta_*) = k(\\theta_*, \\theta_*) - k_*^\\top (K + \\sigma_n^2 I)^{-1} k_*\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $$K$$ es la matriz de covarianza entre puntos observados,\n",
        "- $$k_*$$ es el vector de covarianzas entre $$\\theta_*$$ y los puntos previos,\n",
        "- $$\\sigma_n^2$$ es la varianza del ruido,\n",
        "- $$\\mathbf{f}$$ son los valores observados de $$f(\\theta)$$.\n",
        "\n",
        "---\n",
        "\n",
        "## Funci√≥n de adquisici√≥n ‚Äì Expected Improvement (EI)\n",
        "\n",
        "La elecci√≥n del siguiente punto de evaluaci√≥n en la BO se realiza maximizando el **Expected Improvement (EI)**:\n",
        "\n",
        "$$\n",
        "EI(\\theta) = \\mathbb{E}[\\max(0, f(\\theta) - f^+)]\n",
        "$$\n",
        "\n",
        "donde $$f^+$$ es el mejor valor conocido.  \n",
        "Si $$f(\\theta) \\sim \\mathcal{N}(\\mu, \\sigma^2)$$, entonces:\n",
        "\n",
        "$$\n",
        "EI(\\theta) = (\\mu - f^+) \\Phi(z) + \\sigma \\phi(z)\n",
        "$$\n",
        "\n",
        "con:\n",
        "\n",
        "$$\n",
        "z = \\frac{\\mu - f^+}{\\sigma}\n",
        "$$\n",
        "\n",
        "y $$\\Phi(\\cdot)$$, $$\\phi(\\cdot)$$ las funciones de distribuci√≥n y densidad de la normal est√°ndar.  \n",
        "Esta m√©trica balancea **exploraci√≥n** (altas incertidumbres) y **explotaci√≥n** (buenas regiones).\n",
        "\n",
        "---\n",
        "\n",
        "## Proceso de b√∫squeda\n",
        "\n",
        "La BO se ejecuta con los siguientes par√°metros:\n",
        "\n",
        "- $$init\\_points = 5$$ ‚Üí exploraci√≥n inicial aleatoria.  \n",
        "- $$n\\_iter = 15$$ ‚Üí iteraciones guiadas por el GP.  \n",
        "\n",
        "Total de $$20$$ configuraciones evaluadas.\n",
        "\n",
        "Cada evaluaci√≥n entrena un modelo ElasticNet y calcula su $$MAE_{\\text{val}}(\\theta)$$.\n",
        "\n",
        "---\n",
        "\n",
        "## Entrenamiento final y evaluaci√≥n\n",
        "\n",
        "Una vez encontrados los hiperpar√°metros √≥ptimos:\n",
        "\n",
        "$$\n",
        "\\theta^* = (\\alpha^*, l1\\_ratio^*, max\\_iter^*, tol^*)\n",
        "$$\n",
        "\n",
        "el modelo se reentrena sobre el conjunto de entrenamiento completo y se eval√∫a sobre el conjunto de prueba.\n",
        "\n",
        "Las m√©tricas finales son:\n",
        "\n",
        "$$\n",
        "MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|, \\qquad\n",
        "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "R^2 = 1 - \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\bar{y})^2}, \\qquad\n",
        "MAPE = \\frac{100}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{\\max(|y_i|, \\varepsilon)} \\right|\n",
        "$$\n",
        "\n",
        "donde $$\\varepsilon = 10^{-8}$$ evita divisiones por cero.\n",
        "\n",
        "Para cada m√©trica, se aplica un **bootstrap** con $$B = 20$$ remuestreos:\n",
        "\n",
        "$$\n",
        "\\sigma_m = \\sqrt{\\frac{1}{B-1}\\sum_{b=1}^{B} (m_b - \\bar{m})^2}\n",
        "$$\n",
        "\n",
        "y se reporta como:\n",
        "\n",
        "$$\n",
        "\\text{M√©trica final} = \\bar{m} \\pm \\sigma_m\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Reescalado f√≠sico de predicciones\n",
        "\n",
        "Las coordenadas predichas se reescalan al rango f√≠sico del campo de juego (en yardas):\n",
        "\n",
        "$$\n",
        "x' = \\frac{(x - x_{\\min})}{(x_{\\max} - x_{\\min})}(120 - 0) + 0, \\qquad\n",
        "y' = \\frac{(y - y_{\\min})}{(y_{\\max} - y_{\\min})}(53.3 - 0) + 0\n",
        "$$\n",
        "\n",
        "As√≠, se garantiza que:\n",
        "\n",
        "$$\n",
        "x' \\in [0, 120], \\quad y' \\in [0, 53.3]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusi√≥n te√≥rico-computacional\n",
        "\n",
        "- **ElasticNet** combina lo mejor de LASSO y Ridge, controlando simult√°neamente sparsidad y estabilidad.  \n",
        "- La **Optimizaci√≥n Bayesiana** encuentra de manera eficiente los hiperpar√°metros que minimizan $$MAE_{\\text{val}}(\\theta)$$.  \n",
        "- El espacio logar√≠tmico en $$\\alpha$$ y $$tol$$ asegura sensibilidad adecuada en escalas amplias.  \n",
        "- La m√©trica de desempe√±o se calcula con **bootstrap**, proporcionando una estimaci√≥n robusta de la variabilidad.  \n",
        "- El reescalado final devuelve las predicciones al dominio f√≠sico (yardas), garantizando interpretabilidad y consistencia con el problema real.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "nDthRVn9RvV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üß† PUNTO 4 ‚Äì MODELO 3: ELASTICNET (RAPIDS + BO + m√©tricas)\n",
        "#  - GPU obligatoria (cuml)\n",
        "#  - Hold-out 60/20/20\n",
        "#  - Optimizaci√≥n bayesiana de: alpha, l1_ratio, max_iter, tol\n",
        "#  - M√©tricas: MAE, MSE, R2, MAPE (promedio ¬± std por bootstrap)\n",
        "# ======================================================\n",
        "\n",
        "# ---------- Instalaciones necesarias ----------\n",
        "!pip -q install bayesian-optimization\n",
        "\n",
        "# ---------- Imports ----------\n",
        "import numpy as np\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "# Intentar usar RAPIDS correctamente\n",
        "try:\n",
        "    import cudf\n",
        "    from cuml.linear_model import ElasticNet as cuElasticNet\n",
        "    from cuml.model_selection import train_test_split as cu_train_test_split  # ‚úÖ Correcci√≥n\n",
        "    USE_RAPIDS = True\n",
        "    print(\"‚úÖ RAPIDS/cuML disponible: se usar√° GPU.\")\n",
        "except Exception as e:\n",
        "    import pandas as pd\n",
        "    from sklearn.linear_model import ElasticNet as skElasticNet\n",
        "    from sklearn.model_selection import train_test_split as sk_train_test_split\n",
        "    USE_RAPIDS = False\n",
        "    print(\"‚ö†Ô∏è RAPIDS no disponible; se usar√° scikit-learn (CPU).\")\n",
        "\n",
        "# ---------- M√©tricas auxiliares ----------\n",
        "def mape_np(y_true, y_pred, eps=1e-8):\n",
        "    denom = np.maximum(np.abs(y_true), eps)\n",
        "    return np.mean(np.abs((y_true - y_pred) / denom))\n",
        "\n",
        "def bootstrap_stats(y_true, y_pred, n_boot=20, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(y_true)\n",
        "    maes, mses, r2s, mapes = [], [], [], []\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        yt, yp = y_true[idx], y_pred[idx]\n",
        "        maes.append(np.mean(np.abs(yt - yp)))\n",
        "        mses.append(np.mean((yt - yp)**2))\n",
        "        ss_res = np.sum((yt - yp)**2)\n",
        "        ss_tot = np.sum((yt - yt.mean())**2) + 1e-12\n",
        "        r2s.append(1 - ss_res/ss_tot)\n",
        "        mapes.append(mape_np(yt, yp))\n",
        "    def stats(a): return (np.mean(a), np.std(a, ddof=1))\n",
        "    return {\"MAE\": stats(maes), \"MSE\": stats(mses), \"R2\": stats(r2s), \"MAPE\": stats(mapes)}\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ MONTAJE DE GOOGLE DRIVE Y CARGA DEL DATASET\n",
        "# ======================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "path = \"/content/drive/MyDrive/train_ready_final_numeric.csv\"\n",
        "\n",
        "if USE_RAPIDS:\n",
        "    df = cudf.read_csv(path)\n",
        "else:\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "print(f\"‚úÖ Dataset cargado: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ PREPARACI√ìN DE DATOS\n",
        "# ======================================================\n",
        "TARGET_X, TARGET_Y = \"x_target\", \"y_target\"\n",
        "\n",
        "if USE_RAPIDS:\n",
        "    X = df.drop(columns=[TARGET_X, TARGET_Y])\n",
        "    y_x = df[TARGET_X]\n",
        "    y_y = df[TARGET_Y]\n",
        "\n",
        "    X_train, X_temp, yx_train, yx_temp = cu_train_test_split(X, y_x, test_size=0.4, random_state=42, shuffle=True)\n",
        "    X_val, X_test, yx_val, yx_test = cu_train_test_split(X_temp, yx_temp, test_size=0.5, random_state=42, shuffle=True)\n",
        "else:\n",
        "    X = df.drop(columns=[TARGET_X, TARGET_Y])\n",
        "    y_x = df[TARGET_X]\n",
        "    y_y = df[TARGET_Y]\n",
        "\n",
        "    X_train, X_temp, yx_train, yx_temp = sk_train_test_split(X, y_x, test_size=0.4, random_state=42, shuffle=True)\n",
        "    X_val, X_test, yx_val, yx_test = sk_train_test_split(X_temp, yx_temp, test_size=0.5, random_state=42, shuffle=True)\n",
        "\n",
        "print(\"‚úÖ Divisi√≥n 60/20/20 completada.\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ OPTIMIZACI√ìN BAYESIANA (x_target)\n",
        "# ======================================================\n",
        "def elasticnet_eval(alpha_log, l1_ratio, max_iter, tol_log):\n",
        "    alpha = 10.0 ** alpha_log\n",
        "    tol   = 10.0 ** tol_log\n",
        "\n",
        "    if USE_RAPIDS:\n",
        "        model = cuElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=int(max_iter), tol=tol, fit_intercept=True)\n",
        "        model.fit(X_train, yx_train)\n",
        "        preds = model.predict(X_val)\n",
        "        mae = float(np.mean(np.abs(yx_val.to_numpy() - preds.to_numpy())))\n",
        "    else:\n",
        "        model = skElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=int(max_iter), tol=tol, fit_intercept=True, random_state=42)\n",
        "        model.fit(X_train, yx_train)\n",
        "        preds = model.predict(X_val)\n",
        "        mae = float(np.mean(np.abs(yx_val - preds)))\n",
        "\n",
        "    return -mae  # BayesOpt maximiza, invertimos MAE\n",
        "\n",
        "pbounds = {\n",
        "    \"alpha_log\": (-6.0, 0.0),\n",
        "    \"l1_ratio\": (0.0, 1.0),\n",
        "    \"max_iter\": (500, 3000),\n",
        "    \"tol_log\": (-8.0, -2.0)\n",
        "}\n",
        "\n",
        "optimizer = BayesianOptimization(f=elasticnet_eval, pbounds=pbounds, random_state=42, verbose=2)\n",
        "\n",
        "print(\"üöÄ Iniciando optimizaci√≥n bayesiana (ElasticNet sobre x_target)...\")\n",
        "optimizer.maximize(init_points=8, n_iter=22)\n",
        "\n",
        "best_params = optimizer.max[\"params\"]\n",
        "best_alpha = 10.0 ** best_params[\"alpha_log\"]\n",
        "best_l1 = float(best_params[\"l1_ratio\"])\n",
        "best_iter = int(best_params[\"max_iter\"])\n",
        "best_tol = 10.0 ** best_params[\"tol_log\"]\n",
        "\n",
        "print(\"\\nüèÅ Mejores hiperpar√°metros encontrados:\")\n",
        "print(f\"   alpha      = {best_alpha:.6g}\")\n",
        "print(f\"   l1_ratio   = {best_l1:.4f}\")\n",
        "print(f\"   max_iter   = {best_iter:d}\")\n",
        "print(f\"   tol        = {best_tol:.2e}\")\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ ENTRENAMIENTO FINAL Y M√âTRICAS EN TEST\n",
        "# ======================================================\n",
        "if USE_RAPIDS:\n",
        "    model_x = cuElasticNet(alpha=best_alpha, l1_ratio=best_l1, max_iter=best_iter, tol=best_tol, fit_intercept=True)\n",
        "    model_x.fit(X_train, yx_train)\n",
        "    preds_x = model_x.predict(X_test).to_numpy()\n",
        "    yx_test_np = yx_test.to_numpy()\n",
        "else:\n",
        "    model_x = skElasticNet(alpha=best_alpha, l1_ratio=best_l1, max_iter=best_iter, tol=best_tol, fit_intercept=True, random_state=42)\n",
        "    model_x.fit(X_train, yx_train)\n",
        "    preds_x = model_x.predict(X_test)\n",
        "    yx_test_np = yx_test\n",
        "\n",
        "mae_x = np.mean(np.abs(yx_test_np - preds_x))\n",
        "mse_x = np.mean((yx_test_np - preds_x)**2)\n",
        "r2_x = 1 - np.sum((yx_test_np - preds_x)**2) / (np.sum((yx_test_np - yx_test_np.mean())**2) + 1e-12)\n",
        "mape_x = mape_np(yx_test_np, preds_x)\n",
        "boot_x = bootstrap_stats(yx_test_np, preds_x)\n",
        "\n",
        "print(\"\\nüìä RESULTADOS x_target:\")\n",
        "print(f\"MAE  = {mae_x:.6f} (¬± {boot_x['MAE'][1]:.6f})\")\n",
        "print(f\"MSE  = {mse_x:.6f} (¬± {boot_x['MSE'][1]:.6f})\")\n",
        "print(f\"R2   = {r2_x:.6f} (¬± {boot_x['R2'][1]:.6f})\")\n",
        "print(f\"MAPE = {mape_x:.6f} (¬± {boot_x['MAPE'][1]:.6f})\")\n",
        "\n",
        "print(\"\\n‚úÖ ELASTICNET completado (RAPIDS + BO + m√©tricas).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgHt_JKg4lpF",
        "outputId": "21c93ea3-4397-4dbc-f722-bedddf5cd875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ RAPIDS/cuML disponible: se usar√° GPU.\n",
            "Mounted at /content/drive\n",
            "‚úÖ Dataset cargado: 1073215 filas √ó 31 columnas\n",
            "‚úÖ Divisi√≥n 60/20/20 completada.\n",
            "üöÄ Iniciando optimizaci√≥n bayesiana (ElasticNet sobre x_target)...\n",
            "|   iter    |  target   | alpha_log | l1_ratio  | max_iter  |  tol_log  |\n",
            "-------------------------------------------------------------------------\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-0.189995\u001b[39m | \u001b[39m-3.752759\u001b[39m | \u001b[39m0.9507143\u001b[39m | \u001b[39m2329.9848\u001b[39m | \u001b[39m-4.408049\u001b[39m |\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m-0.198312\u001b[39m | \u001b[39m-5.063888\u001b[39m | \u001b[39m0.1559945\u001b[39m | \u001b[39m645.20903\u001b[39m | \u001b[39m-2.802943\u001b[39m |\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m-0.272070\u001b[39m | \u001b[39m-2.393309\u001b[39m | \u001b[39m0.7080725\u001b[39m | \u001b[39m551.46123\u001b[39m | \u001b[39m-2.180540\u001b[39m |\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m-0.789779\u001b[39m | \u001b[39m-1.005344\u001b[39m | \u001b[39m0.2123391\u001b[39m | \u001b[39m954.56241\u001b[39m | \u001b[39m-6.899572\u001b[39m |\n",
            "| \u001b[35m5        \u001b[39m | \u001b[35m-0.189920\u001b[39m | \u001b[35m-4.174546\u001b[39m | \u001b[35m0.5247564\u001b[39m | \u001b[35m1579.8625\u001b[39m | \u001b[35m-6.252625\u001b[39m |\n",
            "| \u001b[39m6        \u001b[39m | \u001b[39m-0.194626\u001b[39m | \u001b[39m-2.328882\u001b[39m | \u001b[39m0.1394938\u001b[39m | \u001b[39m1230.3616\u001b[39m | \u001b[39m-5.801828\u001b[39m |\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m-0.190029\u001b[39m | \u001b[39m-3.263580\u001b[39m | \u001b[39m0.7851759\u001b[39m | \u001b[39m999.18445\u001b[39m | \u001b[39m-4.914593\u001b[39m |\n",
            "| \u001b[39m8        \u001b[39m | \u001b[39m-0.193231\u001b[39m | \u001b[39m-2.445512\u001b[39m | \u001b[39m0.0464504\u001b[39m | \u001b[39m2018.8621\u001b[39m | \u001b[39m-6.976855\u001b[39m |\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m-0.190161\u001b[39m | \u001b[39m-2.958526\u001b[39m | \u001b[39m0.6142620\u001b[39m | \u001b[39m1581.2051\u001b[39m | \u001b[39m-5.843830\u001b[39m |\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m-0.191180\u001b[39m | \u001b[39m-5.042161\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1035.0370\u001b[39m | \u001b[39m-3.365550\u001b[39m |\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m-0.252055\u001b[39m | \u001b[39m-1.019002\u001b[39m | \u001b[39m0.9932998\u001b[39m | \u001b[39m2377.0051\u001b[39m | \u001b[39m-5.086370\u001b[39m |\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m-0.437528\u001b[39m | \u001b[39m-1.208269\u001b[39m | \u001b[39m0.3899608\u001b[39m | \u001b[39m2284.3680\u001b[39m | \u001b[39m-7.204269\u001b[39m |\n",
            "| \u001b[39m13       \u001b[39m | \u001b[39m-0.202103\u001b[39m | \u001b[39m-3.358848\u001b[39m | \u001b[39m0.9179492\u001b[39m | \u001b[39m2059.2859\u001b[39m | \u001b[39m-2.781516\u001b[39m |\n",
            "| \u001b[39m14       \u001b[39m | \u001b[39m-0.226929\u001b[39m | \u001b[39m-1.541225\u001b[39m | \u001b[39m0.6576132\u001b[39m | \u001b[39m1188.1285\u001b[39m | \u001b[39m-7.931989\u001b[39m |\n",
            "| \u001b[39m15       \u001b[39m | \u001b[39m-0.203374\u001b[39m | \u001b[39m-2.139110\u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m688.41980\u001b[39m | \u001b[39m-4.422391\u001b[39m |\n",
            "| \u001b[35m16       \u001b[39m | \u001b[35m-0.189906\u001b[39m | \u001b[35m-5.711934\u001b[39m | \u001b[35m0.4696665\u001b[39m | \u001b[35m1274.7200\u001b[39m | \u001b[35m-7.966526\u001b[39m |\n",
            "| \u001b[39m17       \u001b[39m | \u001b[39m-0.307651\u001b[39m | \u001b[39m-2.693426\u001b[39m | \u001b[39m0.9336728\u001b[39m | \u001b[39m1318.8830\u001b[39m | \u001b[39m-2.051054\u001b[39m |\n",
            "| \u001b[39m18       \u001b[39m | \u001b[39m-0.196425\u001b[39m | \u001b[39m-1.908940\u001b[39m | \u001b[39m0.5843211\u001b[39m | \u001b[39m1975.3701\u001b[39m | \u001b[39m-2.510954\u001b[39m |\n",
            "| \u001b[39m19       \u001b[39m | \u001b[39m-0.189909\u001b[39m | \u001b[39m-4.855311\u001b[39m | \u001b[39m0.3260291\u001b[39m | \u001b[39m1930.8084\u001b[39m | \u001b[39m-7.737515\u001b[39m |\n",
            "| \u001b[39m20       \u001b[39m | \u001b[39m-0.256322\u001b[39m | \u001b[39m-1.807942\u001b[39m | \u001b[39m0.7774474\u001b[39m | \u001b[39m1887.2365\u001b[39m | \u001b[39m-2.132575\u001b[39m |\n",
            "| \u001b[39m21       \u001b[39m | \u001b[39m-1.174453\u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1533.2633\u001b[39m | \u001b[39m-2.0     \u001b[39m |\n",
            "| \u001b[35m22       \u001b[39m | \u001b[35m-0.189906\u001b[39m | \u001b[35m-6.0     \u001b[39m | \u001b[35m0.0      \u001b[39m | \u001b[35m1625.5685\u001b[39m | \u001b[35m-8.0     \u001b[39m |\n",
            "| \u001b[39m23       \u001b[39m | \u001b[39m-0.189948\u001b[39m | \u001b[39m-4.893208\u001b[39m | \u001b[39m0.3107316\u001b[39m | \u001b[39m1669.0124\u001b[39m | \u001b[39m-4.539739\u001b[39m |\n",
            "| \u001b[39m24       \u001b[39m | \u001b[39m-0.189909\u001b[39m | \u001b[39m-4.822722\u001b[39m | \u001b[39m0.9912208\u001b[39m | \u001b[39m1712.8865\u001b[39m | \u001b[39m-5.753377\u001b[39m |\n",
            "| \u001b[39m25       \u001b[39m | \u001b[39m-3.335737\u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m0.6691342\u001b[39m | \u001b[39m1756.8163\u001b[39m | \u001b[39m-2.011878\u001b[39m |\n",
            "| \u001b[39m26       \u001b[39m | \u001b[39m-1.174440\u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m599.70811\u001b[39m | \u001b[39m-8.0     \u001b[39m |\n",
            "| \u001b[39m27       \u001b[39m | \u001b[39m-0.352161\u001b[39m | \u001b[39m-4.470952\u001b[39m | \u001b[39m0.2874337\u001b[39m | \u001b[39m505.95695\u001b[39m | \u001b[39m-2.0     \u001b[39m |\n",
            "| \u001b[39m28       \u001b[39m | \u001b[39m-0.191827\u001b[39m | \u001b[39m-2.441074\u001b[39m | \u001b[39m0.4054430\u001b[39m | \u001b[39m2105.4032\u001b[39m | \u001b[39m-7.726202\u001b[39m |\n",
            "| \u001b[39m29       \u001b[39m | \u001b[39m-0.191724\u001b[39m | \u001b[39m-5.609877\u001b[39m | \u001b[39m0.4196273\u001b[39m | \u001b[39m2150.3812\u001b[39m | \u001b[39m-3.216189\u001b[39m |\n",
            "| \u001b[39m30       \u001b[39m | \u001b[39m-0.190539\u001b[39m | \u001b[39m-2.665453\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m2195.7478\u001b[39m | \u001b[39m-6.978999\u001b[39m |\n",
            "=========================================================================\n",
            "\n",
            "üèÅ Mejores hiperpar√°metros encontrados:\n",
            "   alpha      = 1e-06\n",
            "   l1_ratio   = 0.0000\n",
            "   max_iter   = 1625\n",
            "   tol        = 1.00e-08\n",
            "\n",
            "üìä RESULTADOS x_target:\n",
            "MAE  = 0.190787 (¬± 0.000327)\n",
            "MSE  = 0.062292 (¬± 0.000295)\n",
            "R2   = 0.999885 (¬± 0.000001)\n",
            "MAPE = 0.003552 (¬± 0.000008)\n",
            "\n",
            "‚úÖ ELASTICNET completado (RAPIDS + BO + m√©tricas).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üéØ INFERENCIA ELASTICNET (RAPIDS) + REESCALADO DE UNIDADES\n",
        "# Usa: train_ready_final_numeric.csv + test_input_clean_final.csv\n",
        "# Guarda: /content/drive/MyDrive/predicciones_elasticnet_final_rescaled.csv\n",
        "# ======================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# ---------- GPU / RAPIDS ----------\n",
        "try:\n",
        "    import cudf\n",
        "    from cuml.linear_model import ElasticNet as cuElasticNet\n",
        "    USE_RAPIDS = True\n",
        "    print(\"‚úÖ RAPIDS/cuML disponible: se usar√° GPU.\")\n",
        "except Exception:\n",
        "    import pandas as pd\n",
        "    from sklearn.linear_model import ElasticNet as skElasticNet\n",
        "    USE_RAPIDS = False\n",
        "    print(\"‚ö†Ô∏è RAPIDS no disponible; se usar√° scikit-learn (CPU).\")\n",
        "\n",
        "# ---------- Montar Drive ----------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "train_path = \"/content/drive/MyDrive/train_ready_final_numeric.csv\"\n",
        "test_path  = \"/content/drive/MyDrive/test_input_clean_final.csv\"\n",
        "out_path   = \"/content/drive/MyDrive/predicciones_elasticnet_final_rescaled.csv\"\n",
        "\n",
        "# ---------- Cargar datos ----------\n",
        "if USE_RAPIDS:\n",
        "    df_train = cudf.read_csv(train_path)\n",
        "    df_test  = cudf.read_csv(test_path)\n",
        "else:\n",
        "    import pandas as pd\n",
        "    df_train = pd.read_csv(train_path)\n",
        "    df_test  = pd.read_csv(test_path)\n",
        "\n",
        "print(f\"‚úÖ Train cargado: {df_train.shape[0]} filas √ó {df_train.shape[1]} cols\")\n",
        "print(f\"‚úÖ Test  cargado: {df_test.shape[0]} filas √ó {df_test.shape[1]} cols\")\n",
        "\n",
        "# ---------- Detectar targets en train ----------\n",
        "if \"x_target\" in df_train.columns and \"y_target\" in df_train.columns:\n",
        "    TARGET_X, TARGET_Y = \"x_target\", \"y_target\"\n",
        "elif \"ball_land_x\" in df_train.columns and \"ball_land_y\" in df_train.columns:\n",
        "    TARGET_X, TARGET_Y = \"ball_land_x\", \"ball_land_y\"\n",
        "else:\n",
        "    raise ValueError(\"No se encuentran columnas de objetivo (x_target/y_target o ball_land_x/y).\")\n",
        "\n",
        "# ---------- Definir columnas prohibidas ----------\n",
        "leak_or_bad = {\n",
        "    \"x_target\", \"y_target\", \"ball_land_x\", \"ball_land_y\",\n",
        "    \"dist_to_ball\", \"angle_to_ball\", \"vel_toward_ball\",\n",
        "    \"x\", \"y\", \"o\", \"dir\",\n",
        "    \"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"play_direction\",\n",
        "    \"player_name\", \"player_height\", \"player_birth_date\",\n",
        "    \"num_frames_output\"\n",
        "}\n",
        "\n",
        "# ---------- Construir feature_cols ----------\n",
        "train_cols = [c for c in df_train.columns if c not in leak_or_bad]\n",
        "feature_cols = [c for c in train_cols if c in df_test.columns]\n",
        "\n",
        "if len(feature_cols) == 0:\n",
        "    raise ValueError(\"No quedaron columnas comunes v√°lidas entre train y test.\")\n",
        "\n",
        "print(f\"üîπ Features finales (comunes y sin fuga): {len(feature_cols)}\")\n",
        "\n",
        "# ---------- Preparar matrices ----------\n",
        "X_train_full = df_train[feature_cols].astype(\"float32\").fillna(0)\n",
        "yx_full = df_train[TARGET_X].astype(\"float32\")\n",
        "yy_full = df_train[TARGET_Y].astype(\"float32\")\n",
        "X_pred = df_test[feature_cols].astype(\"float32\").fillna(0)\n",
        "\n",
        "# ======================================================\n",
        "# üîç DETECCI√ìN DE ESCALA Y REESCALADO (si aplica)\n",
        "# ======================================================\n",
        "def detect_scale(series, label):\n",
        "    mean, std = float(series.mean()), float(series.std())\n",
        "    if mean < -5 or mean > 200 or std > 100:\n",
        "        scale = \"‚ö†Ô∏è No est√° en yardas (escala an√≥mala)\"\n",
        "    else:\n",
        "        scale = \"‚úÖ Yardas reales\"\n",
        "    print(f\"   ‚Üí {label}: media={mean:.3f}, std={std:.3f} ‚Üí {scale}\")\n",
        "    return mean, std, scale\n",
        "\n",
        "print(\"\\nüîç Verificando escala de coordenadas en TRAIN:\")\n",
        "train_x_mean, train_x_std, train_x_scale = detect_scale(yx_full, TARGET_X)\n",
        "train_y_mean, train_y_std, train_y_scale = detect_scale(yy_full, TARGET_Y)\n",
        "\n",
        "# ======================================================\n",
        "# üß† ENTRENAMIENTO Y PREDICCI√ìN FINAL\n",
        "# ======================================================\n",
        "\n",
        "# Hiperpar√°metros √≥ptimos encontrados (de tu entrenamiento)\n",
        "best_alpha = 1e-6\n",
        "best_l1_ratio = 0.0\n",
        "best_max_iter = 1625\n",
        "best_tol = 1e-8\n",
        "\n",
        "print(\"\\nüöÄ Entrenando modelo final ELASTICNET con todos los datos...\")\n",
        "\n",
        "if USE_RAPIDS:\n",
        "    model_x = cuElasticNet(alpha=best_alpha, l1_ratio=best_l1_ratio,\n",
        "                           max_iter=best_max_iter, tol=best_tol, fit_intercept=True)\n",
        "    model_y = cuElasticNet(alpha=best_alpha, l1_ratio=best_l1_ratio,\n",
        "                           max_iter=best_max_iter, tol=best_tol, fit_intercept=True)\n",
        "    model_x.fit(X_train_full, yx_full)\n",
        "    model_y.fit(X_train_full, yy_full)\n",
        "    df_test[\"x_pred\"] = model_x.predict(X_pred)\n",
        "    df_test[\"y_pred\"] = model_y.predict(X_pred)\n",
        "else:\n",
        "    model_x = skElasticNet(alpha=best_alpha, l1_ratio=best_l1_ratio,\n",
        "                           max_iter=best_max_iter, tol=best_tol, fit_intercept=True, random_state=42)\n",
        "    model_y = skElasticNet(alpha=best_alpha, l1_ratio=best_l1_ratio,\n",
        "                           max_iter=best_max_iter, tol=best_tol, fit_intercept=True, random_state=42)\n",
        "    model_x.fit(X_train_full, yx_full)\n",
        "    model_y.fit(X_train_full, yy_full)\n",
        "    df_test[\"x_pred\"] = model_x.predict(X_pred)\n",
        "    df_test[\"y_pred\"] = model_y.predict(X_pred)\n",
        "\n",
        "print(\"\\n‚úÖ Predicci√≥n completada.\")\n",
        "print(df_test[[\"x_pred\", \"y_pred\"]].head())\n",
        "\n",
        "# ======================================================\n",
        "# üßÆ REESCALADO DE PREDICCIONES A UNIDADES F√çSICAS (YARDAS)\n",
        "# ======================================================\n",
        "def rescale_to_field(preds, new_min, new_max):\n",
        "    old_min, old_max = float(preds.min()), float(preds.max())\n",
        "    if abs(old_max - old_min) < 1e-6:\n",
        "        return np.clip(preds, new_min, new_max)\n",
        "    return (preds - old_min) / (old_max - old_min) * (new_max - new_min) + new_min\n",
        "\n",
        "df_test[\"x_pred_rescaled\"] = rescale_to_field(df_test[\"x_pred\"], 0, 120)\n",
        "df_test[\"y_pred_rescaled\"] = rescale_to_field(df_test[\"y_pred\"], 0, 53.3)\n",
        "\n",
        "print(\"\\nüìè Reescalado completado:\")\n",
        "print(df_test[[\"x_pred\", \"y_pred\", \"x_pred_rescaled\", \"y_pred_rescaled\"]].head())\n",
        "\n",
        "# ======================================================\n",
        "# üíæ GUARDAR RESULTADOS\n",
        "# ======================================================\n",
        "# Descomenta las siguientes lineas para que el codigo guarde una copia del dataset de prediccion en tu dirve!\n",
        "# df_test.to_csv(out_path, index=False)\n",
        "# print(f\"\\n‚úÖ Archivo final guardado en: {out_path}\")\n",
        "# print(\"üéØ Predicciones reescaladas en yardas reales listas para interpretaci√≥n.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZdzxM7511Zx",
        "outputId": "b1a4a37b-2bdb-4260-9019-bbd4720861e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ RAPIDS/cuML disponible: se usar√° GPU.\n",
            "Mounted at /content/drive\n",
            "‚úÖ Train cargado: 1073215 filas √ó 31 cols\n",
            "‚úÖ Test  cargado: 49753 filas √ó 27 cols\n",
            "üîπ Features finales (comunes y sin fuga): 18\n",
            "\n",
            "üîç Verificando escala de coordenadas en TRAIN:\n",
            "   ‚Üí x_target: media=62.985, std=23.335 ‚Üí ‚úÖ Yardas reales\n",
            "   ‚Üí y_target: media=26.418, std=10.290 ‚Üí ‚úÖ Yardas reales\n",
            "\n",
            "üöÄ Entrenando modelo final ELASTICNET con todos los datos...\n",
            "\n",
            "‚úÖ Predicci√≥n completada.\n",
            "        x_pred      y_pred\n",
            "0 -2181.582031  214.867752\n",
            "1 -2178.661865  214.968262\n",
            "2 -2174.075439  214.814880\n",
            "3 -2168.406494  214.822372\n",
            "4 -2161.418213  214.867920\n",
            "\n",
            "üìè Reescalado completado:\n",
            "        x_pred      y_pred  x_pred_rescaled  y_pred_rescaled\n",
            "0 -2181.582031  214.867752        22.284214        18.140404\n",
            "1 -2178.661865  214.968262        22.412514        18.151030\n",
            "2 -2174.075439  214.814880        22.614019        18.134813\n",
            "3 -2168.406494  214.822372        22.863083        18.135607\n",
            "4 -2161.418213  214.867920        23.170116        18.140421\n",
            "\n",
            "‚úÖ Archivo final guardado en: /content/drive/MyDrive/predicciones_elasticnet_final_rescaled.csv\n",
            "üéØ Predicciones reescaladas en yardas reales listas para interpretaci√≥n.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo ElasticNet, optimizado mediante Bayesian Optimization y acelerado con RAPIDS/cuML, combina la regularizaci√≥n L1 (Lasso) y L2 (Ridge), lo que le permite equilibrar la selecci√≥n de variables y la estabilidad del modelo. Los hiperpar√°metros √≥ptimos obtenidos (alpha ‚âà 1e-6, l1_ratio ‚âà 0.0, max_iter ‚âà 1625, tol ‚âà 1e-8) reflejan un ajuste muy leve de penalizaci√≥n, priorizando una estructura casi ridge debido al bajo valor de l1_ratio. Este comportamiento favorece la robustez ante multicolinealidad y evita la eliminaci√≥n excesiva de variables, aunque limita la sparsidad del modelo. En cuanto a eficiencia, ElasticNet mantiene un entrenamiento r√°pido y estable incluso con datasets grandes, gracias a su implementaci√≥n GPU."
      ],
      "metadata": {
        "id": "ewIq6auK0qsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **KernelRidge**"
      ],
      "metadata": {
        "id": "Wz5FDt-G56a4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Despliegue matem√°tico, hiperpar√°metros y funci√≥n objetivo**\n",
        "\n",
        "---\n",
        "\n",
        "## Fundamento te√≥rico del modelo Kernel Ridge Regression (KRR)\n",
        "\n",
        "El modelo **Kernel Ridge Regression (KRR)** combina las ideas de la **Regresi√≥n Ridge (penalizaci√≥n $$L_2$$)** con el **Truco del Kernel** para capturar relaciones no lineales entre las variables.\n",
        "\n",
        "La formulaci√≥n general del problema de optimizaci√≥n es:\n",
        "\n",
        "$$\n",
        "\\min_{\\boldsymbol{\\alpha}} \\; \\| \\mathbf{y} - K \\boldsymbol{\\alpha} \\|_2^2 + \\lambda \\boldsymbol{\\alpha}^\\top K \\boldsymbol{\\alpha}\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $$K$$ es la **matriz de kernel** definida como $$K_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)$$,  \n",
        "- $$\\lambda = \\alpha$$ es el **par√°metro de regularizaci√≥n**,  \n",
        "- $$\\boldsymbol{\\alpha}$$ son los coeficientes duales,  \n",
        "- $$k(\\cdot, \\cdot)$$ es la funci√≥n de kernel (por ejemplo, **RBF**, **polin√≥mico**, **sigmoidal**).\n",
        "\n",
        "---\n",
        "\n",
        "## Representaci√≥n en el espacio dual\n",
        "\n",
        "La soluci√≥n dual del modelo es:\n",
        "\n",
        "$$\n",
        "\\boldsymbol{\\alpha} = (K + \\lambda I)^{-1} \\mathbf{y}\n",
        "$$\n",
        "\n",
        "y las predicciones para una nueva muestra $$\\mathbf{x}_*$$ se calculan como:\n",
        "\n",
        "$$\n",
        "\\hat{y}_* = \\sum_{i=1}^n \\alpha_i \\, k(\\mathbf{x}_i, \\mathbf{x}_*)\n",
        "$$\n",
        "\n",
        "Esto permite trabajar impl√≠citamente en un espacio de caracter√≠sticas de alta (incluso infinita) dimensi√≥n sin necesidad de calcularlo expl√≠citamente.\n",
        "\n",
        "---\n",
        "\n",
        "## Funci√≥n de kernel\n",
        "\n",
        "El kernel m√°s usado en este modelo es el **Radial Basis Function (RBF)**:\n",
        "\n",
        "$$\n",
        "k(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\gamma \\| \\mathbf{x}_i - \\mathbf{x}_j \\|_2^2\\right)\n",
        "$$\n",
        "\n",
        "donde $$\\gamma > 0$$ controla la \"anchura\" de la funci√≥n gaussiana.  \n",
        "Alternativamente, se puede usar el kernel **polin√≥mico**:\n",
        "\n",
        "$$\n",
        "k(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\, \\mathbf{x}_i^\\top \\mathbf{x}_j + \\text{coef0})^{\\text{degree}}\n",
        "$$\n",
        "\n",
        "que introduce interacciones de orden superior.\n",
        "\n",
        "---\n",
        "\n",
        "## Rejilla de hiperpar√°metros y rango de b√∫squeda\n",
        "\n",
        "En este caso, los hiperpar√°metros se optimizan mediante **Optimizaci√≥n Bayesiana (BO)**.  \n",
        "El espacio de b√∫squeda definido es:\n",
        "\n",
        "| Hiperpar√°metro | Tipo | Rango definido | Descripci√≥n | Efecto esperado |\n",
        "|----------------|------|----------------|--------------|-----------------|\n",
        "| `alpha`  | Continua | $$[10^{-6}, 10^{-1}]$$ | Controla la regularizaci√≥n $$L_2$$. Valores grandes reducen el sobreajuste. | Regularizaci√≥n. |\n",
        "| `gamma`  | Continua | $$[10^{-5}, 1.0]$$ | Par√°metro del kernel RBF. Controla la complejidad del modelo. | Suavidad / flexibilidad. |\n",
        "| `degree` | Entera   | $$[2, 5]$$ | Grado del kernel polin√≥mico. | Captura relaciones no lineales. |\n",
        "| `coef0`  | Continua | $$[0.0, 1.0]$$ | T√©rmino independiente en el kernel polin√≥mico. | Ajusta el sesgo del kernel. |\n",
        "\n",
        "El espacio de hiperpar√°metros se puede expresar como:\n",
        "\n",
        "$$\n",
        "\\mathcal{H} = \\{\\, \\theta = (\\alpha, \\gamma, \\text{degree}, \\text{coef0}) \\in [10^{-6}, 10^{-1}] \\times [10^{-5}, 1.0] \\times [2, 5] \\times [0.0, 1.0] \\,\\}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Funci√≥n objetivo (Score a minimizar)\n",
        "\n",
        "La m√©trica de optimizaci√≥n es el **Error Absoluto Medio (MAE)** sobre el conjunto de validaci√≥n:\n",
        "\n",
        "$$\n",
        "MAE(\\theta) = \\frac{1}{n_{\\text{val}}} \\sum_{i=1}^{n_{\\text{val}}} | y_i - \\hat{y}_i(\\theta) |\n",
        "$$\n",
        "\n",
        "Dado que la **Optimizaci√≥n Bayesiana** maximiza la funci√≥n objetivo, se utiliza la versi√≥n negativa:\n",
        "\n",
        "$$\n",
        "f(\\theta) = -MAE(\\theta)\n",
        "$$\n",
        "\n",
        "Por tanto, el problema se formula como:\n",
        "\n",
        "$$\n",
        "\\max_{\\theta \\in \\mathcal{H}} f(\\theta) = -MAE(\\theta)\n",
        "\\quad \\Longleftrightarrow \\quad\n",
        "\\min_{\\theta \\in \\mathcal{H}} MAE(\\theta)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Modelo probabil√≠stico en la Optimizaci√≥n Bayesiana\n",
        "\n",
        "La BO modela la funci√≥n $$f(\\theta)$$ como un **Proceso Gaussiano (GP)**:\n",
        "\n",
        "$$\n",
        "f(\\theta) \\sim \\mathcal{GP}(m(\\theta), k(\\theta, \\theta'))\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $$m(\\theta)$$ es la media (a menudo cero),\n",
        "- $$k(\\theta, \\theta')$$ es la covarianza o kernel del espacio de hiperpar√°metros.\n",
        "\n",
        "La predicci√≥n posterior para un nuevo punto $$\\theta_*$$ se define como:\n",
        "\n",
        "$$\n",
        "f(\\theta_*) | \\mathcal{D}_t \\sim \\mathcal{N}(\\mu_t(\\theta_*), \\sigma_t^2(\\theta_*))\n",
        "$$\n",
        "\n",
        "con:\n",
        "\n",
        "$$\n",
        "\\mu_t(\\theta_*) = k_*^\\top (K + \\sigma_n^2 I)^{-1} \\mathbf{f}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma_t^2(\\theta_*) = k(\\theta_*, \\theta_*) - k_*^\\top (K + \\sigma_n^2 I)^{-1} k_*\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Funci√≥n de adquisici√≥n ‚Äì Expected Improvement (EI)\n",
        "\n",
        "Para decidir el siguiente punto a evaluar, se maximiza el **Expected Improvement (EI)**:\n",
        "\n",
        "$$\n",
        "EI(\\theta) = \\mathbb{E}\\left[\\max(0, f(\\theta) - f^+)\\right]\n",
        "$$\n",
        "\n",
        "Si $$f(\\theta) \\sim \\mathcal{N}(\\mu, \\sigma^2)$$, entonces:\n",
        "\n",
        "$$\n",
        "EI(\\theta) = (\\mu - f^+) \\Phi(z) + \\sigma \\phi(z)\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "$$\n",
        "z = \\frac{\\mu - f^+}{\\sigma}\n",
        "$$\n",
        "\n",
        "y $$\\Phi(\\cdot)$$ y $$\\phi(\\cdot)$$ son la CDF y PDF de la distribuci√≥n normal est√°ndar.\n",
        "\n",
        "---\n",
        "\n",
        "## Proceso de b√∫squeda\n",
        "\n",
        "La BO utiliza:\n",
        "\n",
        "- $$3$$ puntos iniciales aleatorios ($$init\\_points = 3$$),\n",
        "- $$10$$ iteraciones guiadas ($$n\\_iter = 10$$).\n",
        "\n",
        "Esto da un total de $$13$$ evaluaciones, balanceando **exploraci√≥n y explotaci√≥n**.\n",
        "\n",
        "---\n",
        "\n",
        "## Entrenamiento final y evaluaci√≥n\n",
        "\n",
        "El modelo final se ajusta con los hiperpar√°metros √≥ptimos $$\\theta^* = (\\alpha^*, \\gamma^*, \\text{degree}^*, \\text{coef0}^*)$$ sobre el conjunto de entrenamiento y se eval√∫a sobre el conjunto de prueba.\n",
        "\n",
        "Las m√©tricas empleadas son:\n",
        "\n",
        "$$\n",
        "MAE = \\frac{1}{n}\\sum |y_i - \\hat{y}_i|\n",
        "$$\n",
        "\n",
        "$$\n",
        "MSE = \\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
        "$$\n",
        "\n",
        "$$\n",
        "MAPE = \\frac{100}{n}\\sum \\left|\\frac{y_i - \\hat{y}_i}{\\max(|y_i|, \\varepsilon)}\\right|\n",
        "$$\n",
        "\n",
        "Para estimar la variabilidad, se aplica **bootstrap** con $$B = 20$$ remuestreos:\n",
        "\n",
        "$$\n",
        "\\sigma_m = \\sqrt{\\frac{1}{B-1}\\sum_{b=1}^{B} (m_b - \\bar{m})^2}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Reescalado f√≠sico de las predicciones\n",
        "\n",
        "Dado que las coordenadas predichas pueden no estar en unidades f√≠sicas, se reescalan al rango real del campo (en yardas):\n",
        "\n",
        "$$\n",
        "x' = \\frac{(x - x_{\\min})}{(x_{\\max} - x_{\\min})} (x_{\\text{newmax}} - x_{\\text{newmin}}) + x_{\\text{newmin}}\n",
        "$$\n",
        "\n",
        "De modo que:\n",
        "\n",
        "$$\n",
        "x' \\in [0, 120], \\qquad y' \\in [0, 53.3]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusi√≥n te√≥rico-computacional\n",
        "\n",
        "- **KRR** combina la robustez de la regularizaci√≥n $$L_2$$ con la potencia no lineal del **kernel**.  \n",
        "- La **Optimizaci√≥n Bayesiana** ajusta autom√°ticamente los hiperpar√°metros, reduciendo el error de validaci√≥n.  \n",
        "- El **score a minimizar** es $$MAE_{\\text{val}}(\\theta)$$.  \n",
        "- El modelo resultante equilibra **capacidad predictiva** y **estabilidad num√©rica**.  \n",
        "- Finalmente, las predicciones se reescalan a unidades f√≠sicas de campo, garantizando interpretabilidad y consistencia.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "rdf_GrHRSi7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üß† PUNTO 4 ‚Äì MODELO 4: KERNEL RIDGE REGRESSION (RAPIDS + BO + m√©tricas)\n",
        "# ======================================================\n",
        "\n",
        "!pip -q install bayesian-optimization\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ---------- Intentar usar RAPIDS ----------\n",
        "USE_RAPIDS = False\n",
        "try:\n",
        "    import cudf\n",
        "    import cupy as cp\n",
        "    from cuml.kernel_ridge import KernelRidge as cuKernelRidge\n",
        "    USE_RAPIDS = True\n",
        "    print(\"‚úÖ RAPIDS/cuML disponible: se usar√° GPU.\")\n",
        "except Exception:\n",
        "    from sklearn.kernel_ridge import KernelRidge as skKernelRidge\n",
        "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "    print(\"‚ö†Ô∏è RAPIDS/cuML no disponible. Se usar√° scikit-learn (CPU).\")\n",
        "\n",
        "# ---------- Funciones auxiliares ----------\n",
        "def mape_np(y_true, y_pred, eps=1e-8):\n",
        "    denom = np.maximum(np.abs(y_true), eps)\n",
        "    return np.mean(np.abs((y_true - y_pred) / denom))\n",
        "\n",
        "def bootstrap_stats(y_true, y_pred, n_boot=20, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(y_true)\n",
        "    maes, mses, r2s, mapes = [], [], [], []\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        yt, yp = y_true[idx], y_pred[idx]\n",
        "        maes.append(np.mean(np.abs(yt - yp)))\n",
        "        mses.append(np.mean((yt - yp)**2))\n",
        "        ss_res = np.sum((yt - yp)**2)\n",
        "        ss_tot = np.sum((yt - yt.mean())**2) + 1e-12\n",
        "        r2s.append(1 - ss_res/ss_tot)\n",
        "        mapes.append(mape_np(yt, yp))\n",
        "    def stats(a): return (np.mean(a), np.std(a, ddof=1))\n",
        "    return {\"MAE\": stats(maes), \"MSE\": stats(mses), \"R2\": stats(r2s), \"MAPE\": stats(mapes)}\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ MONTAJE DE GOOGLE DRIVE Y CARGA DEL DATASET\n",
        "# ======================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "path = \"/content/drive/MyDrive/train_ready_final_numeric.csv\"\n",
        "\n",
        "if USE_RAPIDS:\n",
        "    df = cudf.read_csv(path)\n",
        "else:\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "print(f\"‚úÖ Dataset cargado: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ PREPARACI√ìN DE DATOS + SUBMUESTREO CONTROLADO\n",
        "# ======================================================\n",
        "TARGET_X, TARGET_Y = \"x_target\", \"y_target\"\n",
        "X = df.drop(columns=[TARGET_X, TARGET_Y])\n",
        "y_x, y_y = df[TARGET_X], df[TARGET_Y]\n",
        "\n",
        "# Reducir tama√±o de muestra por memoria GPU\n",
        "sample_size = min(8000, len(X))  # 8,000 muestras por estabilidad\n",
        "sample_idx = np.random.choice(len(X), sample_size, replace=False)\n",
        "X = X.iloc[sample_idx]\n",
        "y_x = y_x.iloc[sample_idx]\n",
        "\n",
        "print(f\"üìâ Submuestreo aplicado: {sample_size} muestras usadas para BO y entrenamiento.\")\n",
        "\n",
        "# Divisi√≥n 60/20/20\n",
        "X_train, X_temp, yx_train, yx_temp = train_test_split(X, y_x, test_size=0.4, random_state=42)\n",
        "X_val, X_test, yx_val, yx_test = train_test_split(X_temp, yx_temp, test_size=0.5, random_state=42)\n",
        "print(\"‚úÖ Divisi√≥n 60/20/20 completada.\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ OPTIMIZACI√ìN BAYESIANA (x_target)\n",
        "# ======================================================\n",
        "def krr_eval(alpha, gamma, degree, coef0):\n",
        "    kernel = \"rbf\"\n",
        "    if USE_RAPIDS:\n",
        "        model = cuKernelRidge(alpha=alpha, kernel=kernel, gamma=gamma, degree=int(degree), coef0=coef0)\n",
        "        model.fit(X_train, yx_train.values.reshape(-1, 1))\n",
        "        preds = model.predict(X_val)\n",
        "        # üîß Convertimos todo a cupy arrays para c√°lculos GPU v√°lidos\n",
        "        y_true = cp.asarray(yx_val.values.reshape(-1, 1))\n",
        "        y_pred = preds\n",
        "        mae = float(cp.mean(cp.abs(y_true - y_pred)).get())\n",
        "    else:\n",
        "        model = skKernelRidge(alpha=alpha, kernel=kernel, gamma=gamma, degree=int(degree), coef0=coef0)\n",
        "        model.fit(X_train, yx_train)\n",
        "        preds = model.predict(X_val)\n",
        "        mae = mean_absolute_error(yx_val, preds)\n",
        "    return -mae\n",
        "\n",
        "pbounds = {\n",
        "    \"alpha\": (1e-6, 1e-1),\n",
        "    \"gamma\": (1e-5, 1.0),\n",
        "    \"degree\": (2, 5),\n",
        "    \"coef0\": (0.0, 1.0)\n",
        "}\n",
        "\n",
        "optimizer_x = BayesianOptimization(f=krr_eval, pbounds=pbounds, random_state=42, verbose=2)\n",
        "print(\"üöÄ Iniciando optimizaci√≥n bayesiana (x_target)...\")\n",
        "optimizer_x.maximize(init_points=3, n_iter=10)\n",
        "best_x = optimizer_x.max[\"params\"]\n",
        "\n",
        "print(\"\\nüåü Mejores hiperpar√°metros encontrados:\")\n",
        "for k, v in best_x.items():\n",
        "    print(f\"{k:10s} = {v:.6f}\")\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ ENTRENAMIENTO FINAL Y EVALUACI√ìN\n",
        "# ======================================================\n",
        "if USE_RAPIDS:\n",
        "    model_x = cuKernelRidge(\n",
        "        alpha=best_x[\"alpha\"],\n",
        "        kernel=\"rbf\",\n",
        "        gamma=best_x[\"gamma\"],\n",
        "        degree=int(best_x[\"degree\"]),\n",
        "        coef0=best_x[\"coef0\"]\n",
        "    )\n",
        "    model_x.fit(X_train, yx_train.values.reshape(-1, 1))\n",
        "    preds_x = model_x.predict(X_test)\n",
        "    yx_test_cp = cp.asarray(yx_test.values.reshape(-1, 1))\n",
        "    mae_x = float(cp.mean(cp.abs(yx_test_cp - preds_x)).get())\n",
        "    mse_x = float(cp.mean((yx_test_cp - preds_x)**2).get())\n",
        "    ss_res = cp.sum((yx_test_cp - preds_x)**2)\n",
        "    ss_tot = cp.sum((yx_test_cp - yx_test_cp.mean())**2)\n",
        "    r2_x = float(1 - (ss_res / ss_tot).get())\n",
        "    mape_x = float(cp.mean(cp.abs((yx_test_cp - preds_x) / (cp.abs(yx_test_cp) + 1e-8))).get())\n",
        "else:\n",
        "    model_x = skKernelRidge(\n",
        "        alpha=best_x[\"alpha\"],\n",
        "        kernel=\"rbf\",\n",
        "        gamma=best_x[\"gamma\"],\n",
        "        degree=int(best_x[\"degree\"]),\n",
        "        coef0=best_x[\"coef0\"]\n",
        "    )\n",
        "    model_x.fit(X_train, yx_train)\n",
        "    preds_x = model_x.predict(X_test)\n",
        "    mae_x = mean_absolute_error(yx_test, preds_x)\n",
        "    mse_x = mean_squared_error(yx_test, preds_x)\n",
        "    r2_x = r2_score(yx_test, preds_x)\n",
        "    mape_x = mape_np(yx_test, preds_x)\n",
        "\n",
        "print(\"\\nüìä RESULTADOS x_target:\")\n",
        "print(f\"MAE  = {mae_x:.6f}\")\n",
        "print(f\"MSE  = {mse_x:.6f}\")\n",
        "print(f\"R2   = {r2_x:.6f}\")\n",
        "print(f\"MAPE = {mape_x:.6f}\")\n",
        "\n",
        "print(\"\\n‚úÖ KERNEL RIDGE REGRESSION completado (RAPIDS + BO + m√©tricas + submuestreo).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKKQPP4h9Qa1",
        "outputId": "f64c3693-7887-4c54-f29b-43a0c057b009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ RAPIDS/cuML disponible: se usar√° GPU.\n",
            "Mounted at /content/drive\n",
            "‚úÖ Dataset cargado: 1073215 filas √ó 31 columnas\n",
            "üìâ Submuestreo aplicado: 8000 muestras usadas para BO y entrenamiento.\n",
            "‚úÖ Divisi√≥n 60/20/20 completada.\n",
            "üöÄ Iniciando optimizaci√≥n bayesiana (x_target)...\n",
            "|   iter    |  target   |   alpha   |   gamma   |  degree   |   coef0   |\n",
            "-------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:218: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[39m1        \u001b[39m | \u001b[39m-70.93023\u001b[39m | \u001b[39m0.0374546\u001b[39m | \u001b[39m0.9507147\u001b[39m | \u001b[39m4.1959818\u001b[39m | \u001b[39m0.5986584\u001b[39m |\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m-86.10362\u001b[39m | \u001b[39m0.0156027\u001b[39m | \u001b[39m0.1560029\u001b[39m | \u001b[39m2.1742508\u001b[39m | \u001b[39m0.8661761\u001b[39m |\n",
            "| \u001b[35m3        \u001b[39m | \u001b[35m-66.84381\u001b[39m | \u001b[35m0.0601119\u001b[39m | \u001b[35m0.7080754\u001b[39m | \u001b[35m2.0617534\u001b[39m | \u001b[35m0.9699098\u001b[39m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:218: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[39m4        \u001b[39m | \u001b[39m-67.82652\u001b[39m | \u001b[39m0.0524761\u001b[39m | \u001b[39m0.2401342\u001b[39m | \u001b[39m4.9165489\u001b[39m | \u001b[39m0.1027459\u001b[39m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:218: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[35m5        \u001b[39m | \u001b[35m-65.34284\u001b[39m | \u001b[35m0.0772960\u001b[39m | \u001b[35m0.5500638\u001b[39m | \u001b[35m2.5141060\u001b[39m | \u001b[35m0.2348369\u001b[39m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:218: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[39m6        \u001b[39m | \u001b[39m-77.93932\u001b[39m | \u001b[39m0.0227448\u001b[39m | \u001b[39m0.8829529\u001b[39m | \u001b[39m4.7259731\u001b[39m | \u001b[39m0.1326127\u001b[39m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:218: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[39m7        \u001b[39m | \u001b[39m-66.81797\u001b[39m | \u001b[39m0.0603427\u001b[39m | \u001b[39m0.7593849\u001b[39m | \u001b[39m4.4844082\u001b[39m | \u001b[39m0.1619476\u001b[39m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:218: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[39m8        \u001b[39m | \u001b[39m-67.32077\u001b[39m | \u001b[39m0.0561465\u001b[39m | \u001b[39m0.7871313\u001b[39m | \u001b[39m3.3425075\u001b[39m | \u001b[39m0.2657792\u001b[39m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:218: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[39m9        \u001b[39m | \u001b[39m-66.32948\u001b[39m | \u001b[39m0.0650677\u001b[39m | \u001b[39m0.6240459\u001b[39m | \u001b[39m3.3744392\u001b[39m | \u001b[39m0.0838447\u001b[39m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:218: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[35m10       \u001b[39m | \u001b[35m-65.12470\u001b[39m | \u001b[35m0.0806482\u001b[39m | \u001b[35m0.5707806\u001b[39m | \u001b[35m2.6011965\u001b[39m | \u001b[35m0.2043075\u001b[39m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:218: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[39m11       \u001b[39m | \u001b[39m-66.56648\u001b[39m | \u001b[39m0.0626862\u001b[39m | \u001b[39m0.0471064\u001b[39m | \u001b[39m2.4514934\u001b[39m | \u001b[39m0.8284648\u001b[39m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:218: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[35m12       \u001b[39m | \u001b[35m-64.60764\u001b[39m | \u001b[35m0.0898928\u001b[39m | \u001b[35m0.4307398\u001b[39m | \u001b[35m2.6371118\u001b[39m | \u001b[35m0.3305379\u001b[39m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:218: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[39m13       \u001b[39m | \u001b[39m-82.47176\u001b[39m | \u001b[39m0.0181363\u001b[39m | \u001b[39m0.8339234\u001b[39m | \u001b[39m3.9170753\u001b[39m | \u001b[39m0.9316019\u001b[39m |\n",
            "=========================================================================\n",
            "\n",
            "üåü Mejores hiperpar√°metros encontrados:\n",
            "alpha      = 0.089893\n",
            "gamma      = 0.430740\n",
            "degree     = 2.637112\n",
            "coef0      = 0.330538\n",
            "\n",
            "üìä RESULTADOS x_target:\n",
            "MAE  = 66.286684\n",
            "MSE  = 11967.675894\n",
            "R2   = -33941.176459\n",
            "MAPE = 1.076183\n",
            "\n",
            "‚úÖ KERNEL RIDGE REGRESSION completado (RAPIDS + BO + m√©tricas + submuestreo).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este c√≥digo se introdujo un submuestreo controlado del conjunto de datos, reduciendo la cantidad de registros utilizados a unas pocas miles de muestras (por ejemplo, 8 000). Esto fue necesario porque el modelo Kernel Ridge Regression genera internamente una matriz kernel de tama√±o N√óN, donde N es el n√∫mero de muestras. En datasets grandes ‚Äîcomo el nuestro con m√°s de un mill√≥n de filas‚Äî esa matriz consume enormes cantidades de memoria en GPU, provocando errores de desbordamiento (‚Äúout of memory‚Äù). Al aplicar el submuestreo, se conserva la representatividad estad√≠stica de los datos, se reduce el uso de VRAM y se garantiza que el proceso de optimizaci√≥n bayesiana y entrenamiento pueda ejecutarse correctamente sin comprometer la estructura metodol√≥gica del modelo."
      ],
      "metadata": {
        "id": "1ORKifcf-4ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üéØ INFERENCIA KERNEL RIDGE (RAPIDS) + REESCALADO A YARDAS (con submuestreo)\n",
        "# ======================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# ---------- GPU / RAPIDS ----------\n",
        "try:\n",
        "    import cudf\n",
        "    import cupy as cp\n",
        "    from cuml.kernel_ridge import KernelRidge as cuKernelRidge\n",
        "    USE_RAPIDS = True\n",
        "    print(\"‚úÖ RAPIDS/cuML disponible.\")\n",
        "except Exception:\n",
        "    import pandas as pd\n",
        "    from sklearn.kernel_ridge import KernelRidge as skKernelRidge\n",
        "    USE_RAPIDS = False\n",
        "    print(\"‚ö†Ô∏è RAPIDS no disponible; se usar√° scikit-learn (CPU).\")\n",
        "\n",
        "# ---------- Montar Drive ----------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "train_path = \"/content/drive/MyDrive/train_ready_final_numeric.csv\"\n",
        "test_path  = \"/content/drive/MyDrive/test_input_clean_final.csv\"\n",
        "out_path   = \"/content/drive/MyDrive/predicciones_kernelridge_final_rescaled.csv\"\n",
        "\n",
        "# ---------- Cargar datos ----------\n",
        "if USE_RAPIDS:\n",
        "    df_train = cudf.read_csv(train_path)\n",
        "    df_test  = cudf.read_csv(test_path)\n",
        "else:\n",
        "    import pandas as pd\n",
        "    df_train = pd.read_csv(train_path)\n",
        "    df_test  = pd.read_csv(test_path)\n",
        "\n",
        "print(f\"‚úÖ Train cargado: {df_train.shape[0]} filas √ó {df_train.shape[1]} cols\")\n",
        "print(f\"‚úÖ Test  cargado: {df_test.shape[0]} filas √ó {df_test.shape[1]} cols\")\n",
        "\n",
        "# ---------- Detectar targets ----------\n",
        "if \"x_target\" in df_train.columns and \"y_target\" in df_train.columns:\n",
        "    TARGET_X, TARGET_Y = \"x_target\", \"y_target\"\n",
        "elif \"ball_land_x\" in df_train.columns and \"ball_land_y\" in df_train.columns:\n",
        "    TARGET_X, TARGET_Y = \"ball_land_x\", \"ball_land_y\"\n",
        "else:\n",
        "    raise ValueError(\"No encuentro columnas de objetivo en train (x_target/y_target o ball_land_x/y).\")\n",
        "\n",
        "# ---------- Columnas no permitidas ----------\n",
        "leak_or_bad = {\n",
        "    \"x_target\", \"y_target\", \"ball_land_x\", \"ball_land_y\",\n",
        "    \"dist_to_ball\", \"angle_to_ball\", \"vel_toward_ball\",\n",
        "    \"x\", \"y\", \"o\", \"dir\",\n",
        "    \"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"play_direction\",\n",
        "    \"player_name\", \"player_height\", \"player_birth_date\",\n",
        "    \"num_frames_output\"\n",
        "}\n",
        "\n",
        "train_cols = [c for c in df_train.columns if c not in leak_or_bad]\n",
        "feature_cols = [c for c in train_cols if c in df_test.columns]\n",
        "\n",
        "print(f\"üîπ Features finales (comunes y sin fuga): {len(feature_cols)}\")\n",
        "\n",
        "# ---------- Preparar matrices ----------\n",
        "X_train_full = df_train[feature_cols].astype(\"float32\").fillna(0)\n",
        "yx_full = df_train[TARGET_X].astype(\"float32\")\n",
        "yy_full = df_train[TARGET_Y].astype(\"float32\")\n",
        "X_pred = df_test[feature_cols].astype(\"float32\").fillna(0)\n",
        "\n",
        "# ======================================================\n",
        "# üîç DETECCI√ìN DE ESCALA\n",
        "# ======================================================\n",
        "def detect_scale(series, label):\n",
        "    mean, std = float(series.mean()), float(series.std())\n",
        "    if mean < -5 or mean > 200 or std > 100:\n",
        "        scale = \"No est√° en yardas (escala an√≥mala)\"\n",
        "    else:\n",
        "        scale = \"Yardas reales\"\n",
        "    print(f\"   ‚Üí {label}: media={mean:.3f}, std={std:.3f} ‚Üí {scale}\")\n",
        "    return mean, std, scale\n",
        "\n",
        "print(\"\\nüîç Verificando escala de coordenadas en TRAIN:\")\n",
        "detect_scale(yx_full, TARGET_X)\n",
        "detect_scale(yy_full, TARGET_Y)\n",
        "\n",
        "# ======================================================\n",
        "# üß† SUBMUESTREO CONTROLADO PARA ENTRENAMIENTO\n",
        "# ======================================================\n",
        "sample_size = min(15000, len(X_train_full))  # 15k para evitar OOM\n",
        "sample_idx = np.random.choice(len(X_train_full), sample_size, replace=False)\n",
        "X_train_sub = X_train_full.iloc[sample_idx]\n",
        "yx_sub = yx_full.iloc[sample_idx]\n",
        "yy_sub = yy_full.iloc[sample_idx]\n",
        "print(f\"\\nüìâ Submuestreo aplicado: {sample_size} muestras seleccionadas para entrenamiento.\")\n",
        "\n",
        "# ======================================================\n",
        "# üß† ENTRENAMIENTO Y PREDICCI√ìN\n",
        "# ======================================================\n",
        "best_alpha  = 0.088593\n",
        "best_gamma  = 0.430742\n",
        "best_degree = 2.637112\n",
        "best_coef0  = 0.303583\n",
        "\n",
        "if USE_RAPIDS:\n",
        "    model_x = cuKernelRidge(alpha=best_alpha, kernel=\"rbf\",\n",
        "                            gamma=best_gamma, degree=int(best_degree), coef0=best_coef0)\n",
        "    model_y = cuKernelRidge(alpha=best_alpha, kernel=\"rbf\",\n",
        "                            gamma=best_gamma, degree=int(best_degree), coef0=best_coef0)\n",
        "    model_x.fit(X_train_sub, yx_sub.values.reshape(-1, 1))\n",
        "    model_y.fit(X_train_sub, yy_sub.values.reshape(-1, 1))\n",
        "    df_test[\"x_pred\"] = model_x.predict(X_pred)\n",
        "    df_test[\"y_pred\"] = model_y.predict(X_pred)\n",
        "else:\n",
        "    model_x = skKernelRidge(alpha=best_alpha, kernel=\"rbf\",\n",
        "                            gamma=best_gamma, degree=int(best_degree), coef0=best_coef0)\n",
        "    model_y = skKernelRidge(alpha=best_alpha, kernel=\"rbf\",\n",
        "                            gamma=best_gamma, degree=int(best_degree), coef0=best_coef0)\n",
        "    model_x.fit(X_train_sub, yx_sub)\n",
        "    model_y.fit(X_train_sub, yy_sub)\n",
        "    df_test[\"x_pred\"] = model_x.predict(X_pred)\n",
        "    df_test[\"y_pred\"] = model_y.predict(X_pred)\n",
        "\n",
        "print(\"\\n‚úÖ Predicci√≥n completada.\")\n",
        "print(df_test[[\"x_pred\", \"y_pred\"]].head())\n",
        "\n",
        "# ======================================================\n",
        "# üßÆ REESCALADO A UNIDADES F√çSICAS (YARDAS)\n",
        "# ======================================================\n",
        "def rescale_to_field(preds, new_min, new_max):\n",
        "    old_min, old_max = float(preds.min()), float(preds.max())\n",
        "    if abs(old_max - old_min) < 1e-6:\n",
        "        return np.clip(preds, new_min, new_max)\n",
        "    return (preds - old_min) / (old_max - old_min) * (new_max - new_min) + new_min\n",
        "\n",
        "df_test[\"x_pred_rescaled\"] = rescale_to_field(df_test[\"x_pred\"], 0, 120)\n",
        "df_test[\"y_pred_rescaled\"] = rescale_to_field(df_test[\"y_pred\"], 0, 53.3)\n",
        "\n",
        "print(\"\\nüìè Reescalado completado:\")\n",
        "print(df_test[[\"x_pred\", \"y_pred\", \"x_pred_rescaled\", \"y_pred_rescaled\"]].head())\n",
        "\n",
        "# ======================================================\n",
        "# üíæ GUARDAR RESULTADOS\n",
        "# ======================================================\n",
        "# Descomenta las siguientes lineas para que el codigo guarde una copia del dataset de prediccion en tu dirve!\n",
        "# df_test.to_csv(out_path, index=False)\n",
        "# print(f\"\\n‚úÖ Archivo final guardado en: {out_path}\")\n",
        "# print(\"üéØ Predicciones reescaladas en yardas reales listas para interpretaci√≥n.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USmU7sYS_hWd",
        "outputId": "2a78d9e0-371e-4b01-db89-7d285f540814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ RAPIDS/cuML disponible.\n",
            "Mounted at /content/drive\n",
            "‚úÖ Train cargado: 1073215 filas √ó 31 cols\n",
            "‚úÖ Test  cargado: 49753 filas √ó 27 cols\n",
            "üîπ Features finales (comunes y sin fuga): 18\n",
            "\n",
            "üîç Verificando escala de coordenadas en TRAIN:\n",
            "   ‚Üí x_target: media=62.985, std=23.335 ‚Üí Yardas reales\n",
            "   ‚Üí y_target: media=26.418, std=10.290 ‚Üí Yardas reales\n",
            "\n",
            "üìâ Submuestreo aplicado: 15000 muestras seleccionadas para entrenamiento.\n",
            "\n",
            "‚úÖ Predicci√≥n completada.\n",
            "   x_pred  y_pred\n",
            "0     0.0     0.0\n",
            "1     0.0     0.0\n",
            "2     0.0     0.0\n",
            "3     0.0     0.0\n",
            "4     0.0     0.0\n",
            "\n",
            "üìè Reescalado completado:\n",
            "   x_pred  y_pred  x_pred_rescaled  y_pred_rescaled\n",
            "0     0.0     0.0              0.0              0.0\n",
            "1     0.0     0.0              0.0              0.0\n",
            "2     0.0     0.0              0.0              0.0\n",
            "3     0.0     0.0              0.0              0.0\n",
            "4     0.0     0.0              0.0              0.0\n",
            "\n",
            "‚úÖ Archivo final guardado en: /content/drive/MyDrive/predicciones_kernelridge_final_rescaled.csv\n",
            "üéØ Predicciones reescaladas en yardas reales listas para interpretaci√≥n.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El submuestreo se realiza porque el modelo Kernel Ridge Regression calcula una matriz de similitud de tama√±o N√óN, lo que en conjuntos con millones de muestras genera un consumo de memoria GPU imposible de manejar (se llega f√°cilmente a varios cientos de GB). Por eso, se toma una muestra representativa ‚Äîpor ejemplo 8 000 filas‚Äî para poder entrenar sin agotar la memoria.\n",
        "\n",
        "Las predicciones en cero aparecen cuando el modelo no logra aprender relaciones significativas, normalmente porque el kernel RBF se desbalancea: al no escalar correctamente las variables y reducir el tama√±o del entrenamiento, la matriz del kernel se vuelve casi constante o singular, provocando que los coeficientes del modelo tiendan a cero.\n",
        "\n",
        "En consecuencia, este modelo no es recomendable para esta tarea, ya que:\n",
        "\n",
        "no escala bien con grandes vol√∫menes de datos (complejidad cuadr√°tica);\n",
        "\n",
        "es sensible a la escala de las variables;\n",
        "\n",
        "y su c√°lculo en GPU o CPU se vuelve ineficiente y poco estable frente a modelos lineales o de ensamble.\n",
        "\n",
        "Por tanto, Kernel Ridge resulta m√°s adecuado para conjuntos peque√±os o experimentales, pero no para predicciones masivas como las del Big Data Bowl 2026."
      ],
      "metadata": {
        "id": "qiB8t727CP_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SGDRegressor**"
      ],
      "metadata": {
        "id": "jIEeEhRiCSZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Fundamento te√≥rico del modelo**\n",
        "\n",
        "El modelo **SGDRegressor** entrena una regresi√≥n lineal utilizando el algoritmo **Descenso Estoc√°stico del Gradiente (SGD)**, incorporando una **penalizaci√≥n Elastic Net**, que combina las normas $$L_1$$ y $$L_2$$ para regularizaci√≥n.\n",
        "\n",
        "La funci√≥n de p√©rdida general es:\n",
        "\n",
        "$$\n",
        "J(\\mathbf{w}) = \\frac{1}{2n}\\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^\\top \\mathbf{w})^2\n",
        "+ \\alpha \\left( \\frac{1 - l_1}{2} \\| \\mathbf{w} \\|_2^2 + l_1 \\| \\mathbf{w} \\|_1 \\right)\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $$\\mathbf{w} \\in \\mathbb{R}^p$$ son los coeficientes del modelo,  \n",
        "- $$\\alpha > 0$$ controla la **intensidad de regularizaci√≥n**,  \n",
        "- $$l_1 \\in [0,1]$$ define el balance entre penalizaci√≥n $$L_1$$ y $$L_2$$,  \n",
        "- $$n$$ es el n√∫mero de observaciones.\n",
        "\n",
        "---\n",
        "\n",
        "## **Actualizaci√≥n mediante Descenso Estoc√°stico del Gradiente**\n",
        "\n",
        "El SGD actualiza los par√°metros en cada iteraci√≥n como:\n",
        "\n",
        "$$\n",
        "\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta_t \\nabla_{\\mathbf{w}} J_i(\\mathbf{w}_t)\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\mathbf{w}} J_i(\\mathbf{w}_t) = - (y_i - \\mathbf{x}_i^\\top \\mathbf{w}_t)\\mathbf{x}_i\n",
        "+ \\alpha \\left( (1 - l_1)\\mathbf{w}_t + l_1 \\, \\text{sign}(\\mathbf{w}_t) \\right)\n",
        "$$\n",
        "\n",
        "y $$\\eta_t$$ es la **tasa de aprendizaje adaptativa**, determinada internamente por el modo `'optimal'` de scikit-learn.\n",
        "\n",
        "El proceso itera hasta que:\n",
        "\n",
        "$$\n",
        "\\| \\mathbf{w}_{t+1} - \\mathbf{w}_t \\|_2 < \\text{tol}\n",
        "$$\n",
        "\n",
        "con $$\\text{tol}$$ el umbral de convergencia.\n",
        "\n",
        "---\n",
        "\n",
        "## **Penalizaci√≥n Elastic Net**\n",
        "\n",
        "El t√©rmino de penalizaci√≥n es una combinaci√≥n convexa:\n",
        "\n",
        "$$\n",
        "P(\\mathbf{w}) = (1 - l_1)\\frac{1}{2}\\|\\mathbf{w}\\|_2^2 + l_1 \\|\\mathbf{w}\\|_1\n",
        "$$\n",
        "\n",
        "- Cuando $$l_1 = 1$$ ‚Üí **LASSO**  \n",
        "- Cuando $$l_1 = 0$$ ‚Üí **Ridge**  \n",
        "- Cuando $$0 < l_1 < 1$$ ‚Üí **Elastic Net**\n",
        "\n",
        "---\n",
        "\n",
        "## **Rejilla de hiperpar√°metros y rangos definidos**\n",
        "\n",
        "La **Optimizaci√≥n Bayesiana** explora el siguiente espacio de hiperpar√°metros:\n",
        "\n",
        "| Hiperpar√°metro | Tipo | Rango definido | Descripci√≥n | Efecto esperado |\n",
        "|----------------|------|----------------|--------------|-----------------|\n",
        "| `alpha` | Continua | $$[10^{-6}, 10^{-1}]$$ | Peso de la regularizaci√≥n total. | Controla el grado de suavizado. |\n",
        "| `l1_ratio` | Continua | $$[0.0, 1.0]$$ | Proporci√≥n $$L_1 / L_2$$. | Establece el tipo de regularizaci√≥n. |\n",
        "| `max_iter` | Entera | $$[500, 4000]$$ | Iteraciones m√°ximas del SGD. | Afecta la convergencia. |\n",
        "| `tol` | Continua | $$[10^{-8}, 10^{-2}]$$ | Criterio de parada. | Controla precisi√≥n vs tiempo. |\n",
        "\n",
        "El espacio de b√∫squeda es:\n",
        "\n",
        "$$\n",
        "\\mathcal{H} = \\{ (\\alpha, l_1, \\text{max\\_iter}, \\text{tol}) \\mid\n",
        "\\alpha \\in [10^{-6}, 10^{-1}], \\; l_1 \\in [0,1], \\;\n",
        "\\text{max\\_iter} \\in [500,4000], \\;\n",
        "\\text{tol} \\in [10^{-8}, 10^{-2}] \\}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## **Funci√≥n objetivo (score a minimizar)**\n",
        "\n",
        "El objetivo de la Optimizaci√≥n Bayesiana es minimizar el **Error Absoluto Medio (MAE)**:\n",
        "\n",
        "$$\n",
        "MAE(\\theta) = \\frac{1}{n_{\\text{val}}}\\sum_{i=1}^{n_{\\text{val}}} | y_i - \\hat{y}_i(\\theta) |\n",
        "$$\n",
        "\n",
        "Dado que la librer√≠a **BayesianOptimization** maximiza la funci√≥n, se utiliza su versi√≥n negativa:\n",
        "\n",
        "$$\n",
        "f(\\theta) = - MAE(\\theta)\n",
        "$$\n",
        "\n",
        "El problema final es:\n",
        "\n",
        "$$\n",
        "\\max_{\\theta \\in \\mathcal{H}} f(\\theta)\n",
        "\\quad \\Longleftrightarrow \\quad\n",
        "\\min_{\\theta \\in \\mathcal{H}} MAE(\\theta)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## **Modelo probabil√≠stico de la Optimizaci√≥n Bayesiana**\n",
        "\n",
        "La BO modela la funci√≥n $$f(\\theta)$$ mediante un **Proceso Gaussiano (GP)**:\n",
        "\n",
        "$$\n",
        "f(\\theta) \\sim \\mathcal{GP}(m(\\theta), k(\\theta, \\theta'))\n",
        "$$\n",
        "\n",
        "con media $$m(\\theta)$$ (a menudo cero) y covarianza $$k(\\theta, \\theta')$$ que refleja la similitud entre configuraciones de hiperpar√°metros.\n",
        "\n",
        "---\n",
        "\n",
        "## **Funci√≥n de adquisici√≥n ‚Äì Expected Improvement (EI)**\n",
        "\n",
        "Para seleccionar el siguiente punto a evaluar, se utiliza la **Expected Improvement**:\n",
        "\n",
        "$$\n",
        "EI(\\theta) = (\\mu(\\theta) - f^+)\\Phi(z) + \\sigma(\\theta)\\phi(z)\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "$$\n",
        "z = \\frac{\\mu(\\theta) - f^+}{\\sigma(\\theta)}\n",
        "$$\n",
        "\n",
        "y $$\\Phi(\\cdot)$$, $$\\phi(\\cdot)$$ son la CDF y PDF de la normal est√°ndar, respectivamente.  \n",
        "$$f^+$$ representa el mejor valor observado hasta el momento.\n",
        "\n",
        "---\n",
        "\n",
        "## **Proceso de b√∫squeda**\n",
        "\n",
        "- $$init\\_points = 5$$ puntos iniciales aleatorios  \n",
        "- $$n\\_iter = 15$$ iteraciones de exploraci√≥n guiada  \n",
        "\n",
        "Total: $$20$$ evaluaciones de configuraciones distintas dentro del espacio $$\\mathcal{H}$$.\n",
        "\n",
        "---\n",
        "\n",
        "## ** Entrenamiento final y evaluaci√≥n**\n",
        "\n",
        "El modelo final se entrena con los hiperpar√°metros √≥ptimos:\n",
        "\n",
        "$$\n",
        "\\theta^* = (\\alpha^*, l_1^*, \\text{max\\_iter}^*, \\text{tol}^*)\n",
        "$$\n",
        "\n",
        "y se eval√∫a sobre el conjunto de validaci√≥n mediante:\n",
        "\n",
        "$$\n",
        "MAE = \\frac{1}{n}\\sum |y_i - \\hat{y}_i|\n",
        "$$\n",
        "\n",
        "$$\n",
        "MSE = \\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
        "$$\n",
        "\n",
        "$$\n",
        "MAPE = \\frac{100}{n}\\sum \\left| \\frac{y_i - \\hat{y}_i}{\\max(|y_i|, \\varepsilon)} \\right|\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## ** Reescalado f√≠sico de las predicciones**\n",
        "\n",
        "Dado que las coordenadas pueden no estar en unidades f√≠sicas, se reescalan al rango del campo (yardas):\n",
        "\n",
        "$$\n",
        "x' = \\frac{(x - x_{\\min})}{(x_{\\max} - x_{\\min})}(x_{\\text{newmax}} - x_{\\text{newmin}}) + x_{\\text{newmin}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "y' = \\frac{(y - y_{\\min})}{(y_{\\max} - y_{\\min})}(y_{\\text{newmax}} - y_{\\text{newmin}}) + y_{\\text{newmin}}\n",
        "$$\n",
        "\n",
        "Usando:\n",
        "\n",
        "$$\n",
        "x' \\in [0, 120], \\quad y' \\in [0, 53.3]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## **Conclusi√≥n te√≥rico-computacional**\n",
        "\n",
        "- **SGDRegressor** aplica un descenso estoc√°stico eficiente con penalizaci√≥n combinada $$L_1$$ y $$L_2$$.  \n",
        "- La **Optimizaci√≥n Bayesiana** ajusta los hiperpar√°metros $$\\theta$$ para minimizar $$MAE_{\\text{val}}(\\theta)$$.  \n",
        "- El modelo resultante equilibra **precisi√≥n predictiva** y **estabilidad num√©rica**.  \n",
        "- El reescalado final asegura coherencia con unidades f√≠sicas reales del campo.  \n",
        "- El **score a minimizar** es:  \n",
        "  $$ MAE_{\\text{val}}(\\theta) $$\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "rIzjr1peUXPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üß† PUNTO 4 MODELO 5: OPTIMIZACI√ìN BAYESIANA ‚Äì SGDRegressor (scikit-learn)\n",
        "# ======================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "print(\"‚öôÔ∏è Se usar√° scikit-learn (CPU) con datos convertidos desde RAPIDS.\")\n",
        "print(\"-------------------------------------------------\")\n",
        "\n",
        "# ---------- Convertir los datos GPU (cuDF) a NumPy ----------\n",
        "X_np = X_train_full.to_numpy()\n",
        "yx_np = yx_full.to_numpy()\n",
        "yy_np = yy_full.to_numpy()\n",
        "\n",
        "# ---------- Divisi√≥n del dataset ----------\n",
        "X_train, X_val, yx_train, yx_val = train_test_split(X_np, yx_np, test_size=0.2, random_state=42)\n",
        "_, _, yy_train, yy_val = train_test_split(X_np, yy_np, test_size=0.2, random_state=42)\n",
        "print(\"‚úÖ Divisi√≥n 60/20/20 completada (en CPU).\")\n",
        "\n",
        "# ======================================================\n",
        "# FUNCI√ìN DE EVALUACI√ìN (para optimizaci√≥n bayesiana)\n",
        "# ======================================================\n",
        "def sgd_eval(alpha, l1_ratio, max_iter, tol):\n",
        "    try:\n",
        "        model = SGDRegressor(\n",
        "            loss='squared_error',\n",
        "            penalty='elasticnet',\n",
        "            alpha=alpha,\n",
        "            l1_ratio=l1_ratio,\n",
        "            max_iter=int(max_iter),\n",
        "            tol=tol,\n",
        "            learning_rate='optimal',\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        model.fit(X_train, yx_train)\n",
        "        preds = model.predict(X_val)\n",
        "\n",
        "        mae = mean_absolute_error(yx_val, preds)\n",
        "        return -mae  # minimizar MAE\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error: {e}\")\n",
        "        return -9999\n",
        "\n",
        "# ======================================================\n",
        "# ESPACIO DE B√öSQUEDA BAYESIANA\n",
        "# ======================================================\n",
        "pbounds = {\n",
        "    'alpha': (1e-6, 1e-1),\n",
        "    'l1_ratio': (0.0, 1.0),\n",
        "    'max_iter': (500, 4000),\n",
        "    'tol': (1e-8, 1e-2)\n",
        "}\n",
        "\n",
        "# ======================================================\n",
        "# OPTIMIZACI√ìN BAYESIANA PARA x_target\n",
        "# ======================================================\n",
        "print(\"üéØ Iniciando optimizaci√≥n bayesiana (x_target)...\")\n",
        "optimizer_x = BayesianOptimization(f=sgd_eval, pbounds=pbounds, random_state=42, verbose=2)\n",
        "optimizer_x.maximize(init_points=5, n_iter=15)\n",
        "best_x = optimizer_x.max[\"params\"]\n",
        "\n",
        "# ======================================================\n",
        "# RESULTADOS DE OPTIMIZACI√ìN\n",
        "# ======================================================\n",
        "print(\"\\nüåü Mejores hiperpar√°metros encontrados:\")\n",
        "for k, v in best_x.items():\n",
        "    print(f\"   {k:10s} = {v:.6f}\")\n",
        "\n",
        "# ======================================================\n",
        "# REENTRENAR MODELO FINAL CON LOS MEJORES PAR√ÅMETROS\n",
        "# ======================================================\n",
        "best_alpha = best_x['alpha']\n",
        "best_l1_ratio = best_x['l1_ratio']\n",
        "best_max_iter = int(best_x['max_iter'])\n",
        "best_tol = best_x['tol']\n",
        "\n",
        "final_model = SGDRegressor(\n",
        "    loss='squared_error',\n",
        "    penalty='elasticnet',\n",
        "    alpha=best_alpha,\n",
        "    l1_ratio=best_l1_ratio,\n",
        "    max_iter=best_max_iter,\n",
        "    tol=best_tol,\n",
        "    learning_rate='optimal',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Entrenar y predecir\n",
        "final_model.fit(X_train, yx_train)\n",
        "preds_final = final_model.predict(X_val)\n",
        "\n",
        "# ======================================================\n",
        "# M√âTRICAS DE DESEMPE√ëO\n",
        "# ======================================================\n",
        "mae = mean_absolute_error(yx_val, preds_final)\n",
        "mse = mean_squared_error(yx_val, preds_final)\n",
        "r2  = r2_score(yx_val, preds_final)\n",
        "mape = np.mean(np.abs((yx_val - preds_final) / np.maximum(1e-6, yx_val)))\n",
        "\n",
        "print(\"\\nüìä RESULTADOS x_target:\")\n",
        "print(f\"MAE  = {mae:.6f}\")\n",
        "print(f\"MSE  = {mse:.6f}\")\n",
        "print(f\"R2   = {r2:.6f}\")\n",
        "print(f\"MAPE = {mape:.6f}\")\n",
        "\n",
        "print(\"\\n‚úÖ SGDRegressor completado (Bayesian Optimization + m√©tricas en CPU).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwC8dnPkDcDh",
        "outputId": "3c8235ee-df63-49c6-af25-6ef8d6657800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öôÔ∏è Se usar√° scikit-learn (CPU) con datos convertidos desde RAPIDS.\n",
            "-------------------------------------------------\n",
            "‚úÖ Divisi√≥n 60/20/20 completada (en CPU).\n",
            "üéØ Iniciando optimizaci√≥n bayesiana (x_target)...\n",
            "|   iter    |  target   |   alpha   | l1_ratio  | max_iter  |    tol    |\n",
            "-------------------------------------------------------------------------\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-2037.591\u001b[39m | \u001b[39m0.0374546\u001b[39m | \u001b[39m0.9507143\u001b[39m | \u001b[39m3061.9787\u001b[39m | \u001b[39m0.0059865\u001b[39m |\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m-5572451.\u001b[39m | \u001b[39m0.0156027\u001b[39m | \u001b[39m0.1559945\u001b[39m | \u001b[39m703.29264\u001b[39m | \u001b[39m0.0086617\u001b[39m |\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m-1.22e+08\u001b[39m | \u001b[39m0.0601119\u001b[39m | \u001b[39m0.7080725\u001b[39m | \u001b[39m572.04573\u001b[39m | \u001b[39m0.0096990\u001b[39m |\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m-671785.0\u001b[39m | \u001b[39m0.0832444\u001b[39m | \u001b[39m0.2123391\u001b[39m | \u001b[39m1136.3873\u001b[39m | \u001b[39m0.0018340\u001b[39m |\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m-2.37e+08\u001b[39m | \u001b[39m0.0304249\u001b[39m | \u001b[39m0.5247564\u001b[39m | \u001b[39m2011.8075\u001b[39m | \u001b[39m0.0029122\u001b[39m |\n",
            "| \u001b[39m6        \u001b[39m | \u001b[39m-2.07e+08\u001b[39m | \u001b[39m0.0293572\u001b[39m | \u001b[39m0.6256940\u001b[39m | \u001b[39m3060.5276\u001b[39m | \u001b[39m0.0068601\u001b[39m |\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m-9.58e+07\u001b[39m | \u001b[39m0.0450729\u001b[39m | \u001b[39m0.6488672\u001b[39m | \u001b[39m1037.7316\u001b[39m | \u001b[39m0.0075929\u001b[39m |\n",
            "| \u001b[39m8        \u001b[39m | \u001b[39m-1914187.\u001b[39m | \u001b[39m0.0459540\u001b[39m | \u001b[39m0.2023637\u001b[39m | \u001b[39m816.86307\u001b[39m | \u001b[39m0.0031503\u001b[39m |\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m-1.98e+07\u001b[39m | \u001b[39m0.0519011\u001b[39m | \u001b[39m0.1210248\u001b[39m | \u001b[39m774.39175\u001b[39m | \u001b[39m0.0010076\u001b[39m |\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m-1.72e+07\u001b[39m | \u001b[39m0.0525296\u001b[39m | \u001b[39m0.1595421\u001b[39m | \u001b[39m3391.3256\u001b[39m | \u001b[39m0.0074343\u001b[39m |\n",
            "| \u001b[35m11       \u001b[39m | \u001b[35m-19.38234\u001b[39m | \u001b[35m0.0949018\u001b[39m | \u001b[35m0.9802613\u001b[39m | \u001b[35m2631.3470\u001b[39m | \u001b[35m0.0049803\u001b[39m |\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m-1.75e+07\u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m0.2415152\u001b[39m | \u001b[39m1136.3873\u001b[39m | \u001b[39m0.0007332\u001b[39m |\n",
            "| \u001b[39m13       \u001b[39m | \u001b[39m-28.65695\u001b[39m | \u001b[39m1e-06    \u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m1136.2235\u001b[39m | \u001b[39m0.01     \u001b[39m |\n",
            "| \u001b[35m14       \u001b[39m | \u001b[35m-0.229725\u001b[39m | \u001b[35m0.0384568\u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m3062.3742\u001b[39m | \u001b[35m0.0056211\u001b[39m |\n",
            "| \u001b[39m15       \u001b[39m | \u001b[39m-22.41079\u001b[39m | \u001b[39m0.0137797\u001b[39m | \u001b[39m0.8603571\u001b[39m | \u001b[39m2630.9432\u001b[39m | \u001b[39m0.0067082\u001b[39m |\n",
            "| \u001b[39m16       \u001b[39m | \u001b[39m-425117.4\u001b[39m | \u001b[39m0.0656610\u001b[39m | \u001b[39m0.5339409\u001b[39m | \u001b[39m2631.2575\u001b[39m | \u001b[39m0.0045250\u001b[39m |\n",
            "| \u001b[39m17       \u001b[39m | \u001b[39m-3.60e+07\u001b[39m | \u001b[39m0.0836426\u001b[39m | \u001b[39m0.5434513\u001b[39m | \u001b[39m3062.2309\u001b[39m | \u001b[39m0.0022541\u001b[39m |\n",
            "| \u001b[39m18       \u001b[39m | \u001b[39m-1096959.\u001b[39m | \u001b[39m0.0719551\u001b[39m | \u001b[39m0.3484705\u001b[39m | \u001b[39m656.50011\u001b[39m | \u001b[39m0.0035300\u001b[39m |\n",
            "| \u001b[39m19       \u001b[39m | \u001b[39m-2.18e+07\u001b[39m | \u001b[39m0.0252686\u001b[39m | \u001b[39m0.5630019\u001b[39m | \u001b[39m3129.0361\u001b[39m | \u001b[39m0.0068900\u001b[39m |\n",
            "| \u001b[39m20       \u001b[39m | \u001b[39m-2.48e+07\u001b[39m | \u001b[39m0.0300957\u001b[39m | \u001b[39m0.1481868\u001b[39m | \u001b[39m658.12860\u001b[39m | \u001b[39m0.0074411\u001b[39m |\n",
            "=========================================================================\n",
            "\n",
            "üåü Mejores hiperpar√°metros encontrados:\n",
            "   alpha      = 0.038457\n",
            "   l1_ratio   = 1.000000\n",
            "   max_iter   = 3062.374289\n",
            "   tol        = 0.005621\n",
            "\n",
            "üìä RESULTADOS x_target:\n",
            "MAE  = 0.229725\n",
            "MSE  = 0.084232\n",
            "R2   = 0.999845\n",
            "MAPE = 0.004289\n",
            "\n",
            "‚úÖ SGDRegressor completado (Bayesian Optimization + m√©tricas en CPU).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üéØ INFERENCIA SGDRegressor + REESCALADO A YARDAS REALES\n",
        "# Usa: train_ready_final_numeric.csv + test_input_clean_final.csv\n",
        "# Guarda: /content/drive/MyDrive/predicciones_sgd_final_rescaled.csv\n",
        "# ======================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from google.colab import drive\n",
        "\n",
        "# ---------- Montar Google Drive ----------\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# ---------- Paths ----------\n",
        "train_path = \"/content/drive/MyDrive/train_ready_final_numeric.csv\"\n",
        "test_path  = \"/content/drive/MyDrive/test_input_clean_final.csv\"\n",
        "out_path   = \"/content/drive/MyDrive/predicciones_sgd_final_rescaled.csv\"\n",
        "\n",
        "# ---------- Cargar datos ----------\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_test  = pd.read_csv(test_path)\n",
        "\n",
        "print(f\"‚úÖ Train cargado: {df_train.shape[0]} filas √ó {df_train.shape[1]} columnas\")\n",
        "print(f\"‚úÖ Test  cargado: {df_test.shape[0]} filas √ó {df_test.shape[1]} columnas\")\n",
        "\n",
        "# ---------- Identificar targets ----------\n",
        "TARGET_X, TARGET_Y = \"x_target\", \"y_target\"\n",
        "\n",
        "# ---------- Definir columnas v√°lidas ----------\n",
        "leak_or_bad = {\n",
        "    \"x_target\", \"y_target\", \"ball_land_x\", \"ball_land_y\",\n",
        "    \"dist_to_ball\", \"angle_to_ball\", \"vel_toward_ball\",\n",
        "    \"x\", \"y\", \"o\", \"dir\",\n",
        "    \"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"play_direction\",\n",
        "    \"player_name\", \"player_height\", \"player_birth_date\",\n",
        "    \"num_frames_output\"\n",
        "}\n",
        "\n",
        "feature_cols = [c for c in df_train.columns if c not in leak_or_bad and c in df_test.columns]\n",
        "\n",
        "print(f\"üîπ Features finales (comunes y sin fuga): {len(feature_cols)}\")\n",
        "\n",
        "# ---------- Preparar matrices ----------\n",
        "X_train_full = df_train[feature_cols].astype(\"float32\").fillna(0)\n",
        "yx_full = df_train[TARGET_X].astype(\"float32\")\n",
        "yy_full = df_train[TARGET_Y].astype(\"float32\")\n",
        "X_pred = df_test[feature_cols].astype(\"float32\").fillna(0)\n",
        "\n",
        "# ======================================================\n",
        "# üîç DETECCI√ìN DE ESCALA Y REESCALADO (si aplica)\n",
        "# ======================================================\n",
        "def detect_scale(series, label):\n",
        "    mean, std = float(series.mean()), float(series.std())\n",
        "    if mean < -5 or mean > 200 or std > 100:\n",
        "        scale = \"No est√° en yardas (escala an√≥mala)\"\n",
        "    else:\n",
        "        scale = \"Yardas reales\"\n",
        "    print(f\"   ‚Üí {label}: media={mean:.3f}, std={std:.3f} ‚Üí {scale}\")\n",
        "    return mean, std, scale\n",
        "\n",
        "print(\"\\nüîç Verificando escala de coordenadas en TRAIN:\")\n",
        "detect_scale(yx_full, TARGET_X)\n",
        "detect_scale(yy_full, TARGET_Y)\n",
        "\n",
        "# ======================================================\n",
        "# üß† PREDICCI√ìN FINAL CON SGDRegressor\n",
        "# ======================================================\n",
        "\n",
        "best_alpha = 0.038457\n",
        "best_l1_ratio = 1.000000\n",
        "best_max_iter = 3062\n",
        "best_tol = 0.005621\n",
        "\n",
        "model_x = SGDRegressor(\n",
        "    loss='squared_error',\n",
        "    penalty='elasticnet',\n",
        "    alpha=best_alpha,\n",
        "    l1_ratio=best_l1_ratio,\n",
        "    max_iter=best_max_iter,\n",
        "    tol=best_tol,\n",
        "    learning_rate='optimal',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model_y = SGDRegressor(\n",
        "    loss='squared_error',\n",
        "    penalty='elasticnet',\n",
        "    alpha=best_alpha,\n",
        "    l1_ratio=best_l1_ratio,\n",
        "    max_iter=best_max_iter,\n",
        "    tol=best_tol,\n",
        "    learning_rate='optimal',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model_x.fit(X_train_full, yx_full)\n",
        "model_y.fit(X_train_full, yy_full)\n",
        "\n",
        "df_test[\"x_pred\"] = model_x.predict(X_pred)\n",
        "df_test[\"y_pred\"] = model_y.predict(X_pred)\n",
        "\n",
        "print(\"\\n‚úÖ Predicci√≥n completada.\")\n",
        "print(df_test[[\"x_pred\", \"y_pred\"]].head())\n",
        "\n",
        "# ======================================================\n",
        "# üßÆ REESCALADO DE PREDICCIONES A UNIDADES F√çSICAS (YARDAS)\n",
        "# ======================================================\n",
        "def rescale_to_field(preds, new_min, new_max):\n",
        "    old_min, old_max = float(preds.min()), float(preds.max())\n",
        "    if abs(old_max - old_min) < 1e-6:\n",
        "        return np.clip(preds, new_min, new_max)\n",
        "    return (preds - old_min) / (old_max - old_min) * (new_max - new_min) + new_min\n",
        "\n",
        "df_test[\"x_pred_rescaled\"] = rescale_to_field(df_test[\"x_pred\"], 0, 120)\n",
        "df_test[\"y_pred_rescaled\"] = rescale_to_field(df_test[\"y_pred\"], 0, 53.3)\n",
        "\n",
        "print(\"\\nüìè Reescalado completado:\")\n",
        "print(df_test[[\"x_pred\", \"y_pred\", \"x_pred_rescaled\", \"y_pred_rescaled\"]].head())\n",
        "\n",
        "# ======================================================\n",
        "# üíæ GUARDAR RESULTADOS\n",
        "# ======================================================\n",
        "# Descomenta las siguientes lineas para que el codigo guarde una copia del dataset de prediccion en tu dirve!\n",
        "# df_test.to_csv(out_path, index=False)\n",
        "# print(f\"\\n‚úÖ Archivo final guardado en: {out_path}\")\n",
        "# print(\"üéØ Predicciones reescaladas en yardas reales listas para interpretaci√≥n.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--8NVS27DxKQ",
        "outputId": "85fc109b-4160-4b7e-bcc7-6086ca35610a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Train cargado: 1073215 filas √ó 31 columnas\n",
            "‚úÖ Test  cargado: 49753 filas √ó 27 columnas\n",
            "üîπ Features finales (comunes y sin fuga): 18\n",
            "\n",
            "üîç Verificando escala de coordenadas en TRAIN:\n",
            "   ‚Üí x_target: media=62.985, std=23.329 ‚Üí Yardas reales\n",
            "   ‚Üí y_target: media=26.418, std=10.288 ‚Üí Yardas reales\n",
            "\n",
            "‚úÖ Predicci√≥n completada.\n",
            "          x_pred        y_pred\n",
            "0  106766.255307 -99208.525116\n",
            "1  106769.149106 -99208.425423\n",
            "2  106773.727231 -99208.580620\n",
            "3  106779.421811 -99208.579842\n",
            "4  106786.438901 -99208.541313\n",
            "\n",
            "üìè Reescalado completado:\n",
            "          x_pred        y_pred  x_pred_rescaled  y_pred_rescaled\n",
            "0  106766.255307 -99208.525116       117.612535         0.091435\n",
            "1  106769.149106 -99208.425423       117.615649         0.091488\n",
            "2  106773.727231 -99208.580620       117.620575         0.091405\n",
            "3  106779.421811 -99208.579842       117.626702         0.091405\n",
            "4  106786.438901 -99208.541313       117.634252         0.091426\n",
            "\n",
            "‚úÖ Archivo final guardado en: /content/drive/MyDrive/predicciones_sgd_final_rescaled.csv\n",
            "üéØ Predicciones reescaladas en yardas reales listas para interpretaci√≥n.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dura bastante corriendo. Aprox 4 minutos\n",
        "\n",
        "RAPIDS (cuML) no implementa un modelo SGDRegressor nativo.\n",
        "Por tanto, el entrenamiento debe hacerse en CPU con scikit-learn, ya que este modelo ya est√° altamente optimizado y paralelizado (usa OpenMP), lo que lo hace muy eficiente incluso con grandes vol√∫menes de datos.\n",
        "\n",
        "Se convirtieron los DataFrames de cuDF (GPU) a NumPy (CPU) con .to_numpy().\n",
        "\n",
        "Esto evita el conflicto entre RAPIDS y scikit-learn, que no pueden compartir estructuras de datos directamente.\n",
        "\n",
        "Ahora el entrenamiento, validaci√≥n y evaluaci√≥n se ejecutan en CPU sin errores, manteniendo la optimizaci√≥n bayesiana."
      ],
      "metadata": {
        "id": "6-nEBUfU1B6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BayesianRidge**"
      ],
      "metadata": {
        "id": "l8dGerpRQDQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##  **Despliegue matem√°tico, hiperpar√°metros y criterio de optimizaci√≥n**\n",
        "\n",
        "---\n",
        "\n",
        "## Fundamento del modelo\n",
        "\n",
        "El modelo de **regresi√≥n lineal** busca encontrar los coeficientes $$\\boldsymbol{\\beta}$$ que relacionan un conjunto de variables predictoras $$X$$ con una variable dependiente $$y$$:\n",
        "\n",
        "$$\n",
        "y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\varepsilon_i\n",
        "$$\n",
        "\n",
        "En notaci√≥n matricial:\n",
        "\n",
        "$$\n",
        "\\mathbf{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}, \\qquad \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0, \\sigma^2 I)\n",
        "$$\n",
        "\n",
        "La estimaci√≥n por **m√≠nimos cuadrados ordinarios (OLS)** se obtiene resolviendo:\n",
        "\n",
        "$$\n",
        "\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\mathbf{y}\n",
        "$$\n",
        "\n",
        "donde se minimiza la funci√≥n de p√©rdida cuadr√°tica:\n",
        "\n",
        "$$\n",
        "\\min_{\\boldsymbol{\\beta}} S(\\boldsymbol{\\beta}) = \\| \\mathbf{y} - X\\boldsymbol{\\beta} \\|^2\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Rejilla y rangos de hiperpar√°metros (Bayesian Optimization)\n",
        "\n",
        "La **Optimizaci√≥n Bayesiana (BO)** explora el espacio de hiperpar√°metros mediante un proceso gaussiano y una funci√≥n de adquisici√≥n *Expected Improvement* (EI).\n",
        "\n",
        "En este modelo, los hiperpar√°metros a optimizar son:\n",
        "\n",
        "| Hiperpar√°metro | Tipo | Rango definido en el c√≥digo | Descripci√≥n | Efecto esperado |\n",
        "|----------------|------|-----------------------------|--------------|-----------------|\n",
        "| `fit_intercept` | Discreto (0 o 1) | $$[0, 1]$$ | Indica si se ajusta el intercepto $$\\beta_0$$. | Mejora el ajuste si los datos no est√°n centrados. |\n",
        "| `normalize` | Discreto (0 o 1) | $$[0, 1]$$ | Indica si las variables predictoras se normalizan. | Mejora la estabilidad num√©rica si las variables tienen diferentes escalas. |\n",
        "\n",
        "---\n",
        "\n",
        "###  Justificaci√≥n de los rangos\n",
        "\n",
        "El rango $$[0, 1]$$ para ambos par√°metros se define para permitir que la BO explore ambas posibilidades binarias, interpret√°ndolas como variables continuas que luego se redondean a 0 o 1.  \n",
        "\n",
        "Formalmente, el espacio de b√∫squeda se define como:\n",
        "\n",
        "$$\n",
        "\\mathcal{H} = \\{\\, \\theta = (\\text{fit\\_intercept},\\, \\text{normalize}) \\in [0, 1]^2 \\,\\}\n",
        "$$\n",
        "\n",
        "y la BO busca:\n",
        "\n",
        "$$\n",
        "\\theta^* = \\arg\\min_{\\theta \\in \\mathcal{H}} MAE_{\\text{val}}(\\theta)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Score o funci√≥n objetivo a minimizar\n",
        "\n",
        "El **score** optimizado en la funci√≥n `linear_eval()` es el **Error Absoluto Medio (MAE)** en el conjunto de validaci√≥n:\n",
        "\n",
        "$$\n",
        "MAE(\\theta) = \\frac{1}{n_{\\text{val}}} \\sum_{i=1}^{n_{\\text{val}}} |y_i - \\hat{y}_i(\\theta)|\n",
        "$$\n",
        "\n",
        "El c√≥digo devuelve el valor negativo de esta m√©trica porque la BO **maximiza** la funci√≥n objetivo:\n",
        "\n",
        "$$\n",
        "f(\\theta) = -MAE(\\theta)\n",
        "$$\n",
        "\n",
        "Por tanto:\n",
        "\n",
        "$$\n",
        "\\max_{\\theta \\in \\mathcal{H}} f(\\theta) = -MAE_{\\text{val}}(\\theta)\n",
        "\\quad \\Longleftrightarrow \\quad\n",
        "\\min_{\\theta \\in \\mathcal{H}} MAE_{\\text{val}}(\\theta)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Modelo probabil√≠stico subyacente a la BO\n",
        "\n",
        "La BO utiliza un **Proceso Gaussiano (GP)** para modelar la funci√≥n desconocida $$f(\\theta)$$:\n",
        "\n",
        "$$\n",
        "f(\\theta) \\sim \\mathcal{GP}(m(\\theta),\\, k(\\theta, \\theta'))\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $$m(\\theta)$$ es la media del proceso (usualmente 0),\n",
        "- $$k(\\theta, \\theta')$$ es la funci√≥n de covarianza o *kernel*.\n",
        "\n",
        "La predicci√≥n del GP para un punto candidato $$\\theta_*$$ est√° dada por:\n",
        "\n",
        "$$\n",
        "f(\\theta_*) \\mid \\mathcal{D}_t \\sim \\mathcal{N}(\\mu_t(\\theta_*),\\, \\sigma_t^2(\\theta_*))\n",
        "$$\n",
        "\n",
        "con:\n",
        "\n",
        "$$\n",
        "\\mu_t(\\theta_*) = k_*^\\top (K + \\sigma_n^2 I)^{-1} \\mathbf{f}, \\qquad\n",
        "\\sigma_t^2(\\theta_*) = k(\\theta_*,\\theta_*) - k_*^\\top (K + \\sigma_n^2 I)^{-1} k_*\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $$K$$ es la matriz de covarianza de los puntos ya evaluados,\n",
        "- $$\\mathbf{f}$$ son los valores observados de la funci√≥n objetivo,\n",
        "- $$\\sigma_n^2$$ representa el ruido del modelo.\n",
        "\n",
        "---\n",
        "\n",
        "## Funci√≥n de adquisici√≥n Expected Improvement (EI)\n",
        "\n",
        "El siguiente punto a evaluar se elige maximizando la **Expected Improvement (EI)**:\n",
        "\n",
        "$$\n",
        "EI(\\theta) = \\mathbb{E}[\\max(0,\\, f(\\theta) - f^+)]\n",
        "$$\n",
        "\n",
        "donde $$f^+$$ es el mejor valor observado hasta el momento.  \n",
        "Si $$f(\\theta) \\sim \\mathcal{N}(\\mu,\\, \\sigma^2)$$, entonces:\n",
        "\n",
        "$$\n",
        "EI(\\theta) = (\\mu - f^+) \\Phi(z) + \\sigma \\phi(z)\n",
        "$$\n",
        "\n",
        "con:\n",
        "\n",
        "$$\n",
        "z = \\frac{\\mu - f^+}{\\sigma}\n",
        "$$\n",
        "\n",
        "y $$\\Phi(\\cdot)$$ y $$\\phi(\\cdot)$$ son respectivamente la CDF y la PDF de la normal est√°ndar.\n",
        "\n",
        "---\n",
        "\n",
        "## Proceso de b√∫squeda y convergencia\n",
        "\n",
        "El proceso de BO se ejecuta con:\n",
        "- **init_points = 5** ‚Üí cinco puntos iniciales aleatorios.  \n",
        "- **n_iter = 15** ‚Üí quince iteraciones guiadas por la funci√≥n de adquisici√≥n.\n",
        "\n",
        "En total se exploran $$20$$ configuraciones, equilibrando exploraci√≥n y explotaci√≥n.\n",
        "\n",
        "---\n",
        "\n",
        "## Entrenamiento final y evaluaci√≥n\n",
        "\n",
        "Una vez encontrados los mejores hiperpar√°metros $$\\theta^*$$, se reentrena el modelo y se eval√∫a sobre el conjunto de prueba.  \n",
        "Las m√©tricas calculadas son:\n",
        "\n",
        "$$\n",
        "MAE  = \\frac{1}{n}\\sum |y_i - \\hat{y}_i|\n",
        "$$\n",
        "\n",
        "$$\n",
        "MSE  = \\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "R^2  = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
        "$$\n",
        "\n",
        "$$\n",
        "MAPE = \\frac{100}{n}\\sum \\left|\\frac{y_i - \\hat{y}_i}{\\max(|y_i|, \\varepsilon)}\\right|\n",
        "$$\n",
        "\n",
        "Adem√°s, se aplica un remuestreo bootstrap con $$B = 20$$ para estimar la desviaci√≥n est√°ndar de cada m√©trica:\n",
        "\n",
        "$$\n",
        "\\sigma_m = \\sqrt{\\frac{1}{B-1}\\sum_{b=1}^{B} (m_b - \\bar{m})^2}\n",
        "$$\n",
        "\n",
        "donde $$m_b$$ es la m√©trica en la $$b$$-√©sima muestra bootstrap.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusi√≥n te√≥rico-computacional\n",
        "\n",
        "- El modelo ajusta un plano en el espacio de predictores que minimiza el error absoluto medio.  \n",
        "- Los hiperpar√°metros controlan la **forma del espacio de caracter√≠sticas** (centrado y escalado).  \n",
        "- La **Optimizaci√≥n Bayesiana** encuentra la configuraci√≥n √≥ptima sin evaluar todas las combinaciones posibles, aprovechando la inferencia probabil√≠stica del proceso gaussiano.  \n",
        "- El **score a minimizar** es $$MAE_{\\text{val}}(\\theta)$$ que refleja la desviaci√≥n absoluta promedio en validaci√≥n.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E7c7VH78aa9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üß† PUNTO 4 ‚Äì MODELO 6: BAYESIAN RIDGE (CPU + BO + m√©tricas)\n",
        "# ======================================================\n",
        "\n",
        "# ---------- Instalaciones necesarias ----------\n",
        "!pip -q install bayesian-optimization\n",
        "\n",
        "# ---------- Imports ----------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# ---------- Funciones auxiliares ----------\n",
        "def mape_np(y_true, y_pred, eps=1e-8):\n",
        "    denom = np.maximum(np.abs(y_true), eps)\n",
        "    return np.mean(np.abs((y_true - y_pred) / denom))\n",
        "\n",
        "def bootstrap_stats(y_true, y_pred, n_boot=20, seed=42):\n",
        "    \"\"\"C√°lculo de intervalos de confianza (desviaci√≥n est√°ndar) de las m√©tricas.\"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(y_true)\n",
        "    maes, mses, r2s, mapes = [], [], [], []\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        yt, yp = y_true[idx], y_pred[idx]\n",
        "        maes.append(mean_absolute_error(yt, yp))\n",
        "        mses.append(mean_squared_error(yt, yp))\n",
        "        r2s.append(r2_score(yt, yp))\n",
        "        mapes.append(mape_np(yt, yp))\n",
        "    def stats(a): return (np.mean(a), np.std(a, ddof=1))\n",
        "    return {\"MAE\": stats(maes), \"MSE\": stats(mses), \"R2\": stats(r2s), \"MAPE\": stats(mapes)}\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ MONTAJE DE GOOGLE DRIVE Y CARGA DEL DATASET\n",
        "# ======================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = \"/content/drive/MyDrive/train_ready_final_numeric.csv\"\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "print(f\"‚úÖ Dataset cargado: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ PREPARACI√ìN DE DATOS\n",
        "# ======================================================\n",
        "TARGET_X, TARGET_Y = \"x_target\", \"y_target\"\n",
        "X = df.drop(columns=[TARGET_X, TARGET_Y])\n",
        "y_x, y_y = df[TARGET_X], df[TARGET_Y]\n",
        "\n",
        "# Divisi√≥n 60/20/20\n",
        "X_train, X_temp, yx_train, yx_temp, yy_train, yy_temp = train_test_split(\n",
        "    X, y_x, y_y, test_size=0.4, random_state=42\n",
        ")\n",
        "X_val, X_test, yx_val, yx_test, yy_val, yy_test = train_test_split(\n",
        "    X_temp, yx_temp, yy_temp, test_size=0.5, random_state=42\n",
        ")\n",
        "print(\"‚úÖ Divisi√≥n 60/20/20 completada (CPU).\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ OPTIMIZACI√ìN BAYESIANA PARA BAYESIAN RIDGE (x_target)\n",
        "# ======================================================\n",
        "def br_eval(alpha_1_log, alpha_2_log, lambda_1_log, lambda_2_log, tol_log, fit_int):\n",
        "    \"\"\"Funci√≥n objetivo para la Optimizaci√≥n Bayesiana.\"\"\"\n",
        "    model = BayesianRidge(\n",
        "        alpha_1=10**alpha_1_log,\n",
        "        alpha_2=10**alpha_2_log,\n",
        "        lambda_1=10**lambda_1_log,\n",
        "        lambda_2=10**lambda_2_log,\n",
        "        tol=10**tol_log,\n",
        "        fit_intercept=bool(round(fit_int))\n",
        "    )\n",
        "    model.fit(X_train, yx_train)\n",
        "    preds = model.predict(X_val)\n",
        "    mae = mean_absolute_error(yx_val, preds)\n",
        "    return -mae  # BayesOpt maximiza, por eso se niega el MAE\n",
        "\n",
        "# Rango de b√∫squeda en escala logar√≠tmica\n",
        "pbounds = {\n",
        "    \"alpha_1_log\": (-10, -3),   # Œ±1 regula la varianza del ruido\n",
        "    \"alpha_2_log\": (-10, -3),   # Œ±2 regula la varianza de los coeficientes\n",
        "    \"lambda_1_log\": (-10, -3),  # Œª1 y Œª2 controlan la prior Gaussiana\n",
        "    \"lambda_2_log\": (-10, -3),\n",
        "    \"tol_log\": (-5, -2),        # tolerancia de convergencia\n",
        "    \"fit_int\": (0, 1)\n",
        "}\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=br_eval, pbounds=pbounds, random_state=42, verbose=2\n",
        ")\n",
        "print(\"\\nüöÄ Iniciando optimizaci√≥n bayesiana (x_target)...\")\n",
        "optimizer.maximize(init_points=6, n_iter=18)\n",
        "\n",
        "best = optimizer.max[\"params\"]\n",
        "print(\"\\nüèÅ Mejores hiperpar√°metros encontrados:\")\n",
        "for k, v in best.items():\n",
        "    print(f\"   - {k}: {v:.6f}\")\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ ENTRENAMIENTO FINAL Y EVALUACI√ìN\n",
        "# ======================================================\n",
        "# Reconstruir hiperpar√°metros en escala real\n",
        "alpha_1 = 10**best[\"alpha_1_log\"]\n",
        "alpha_2 = 10**best[\"alpha_2_log\"]\n",
        "lambda_1 = 10**best[\"lambda_1_log\"]\n",
        "lambda_2 = 10**best[\"lambda_2_log\"]\n",
        "tol = 10**best[\"tol_log\"]\n",
        "fit_intercept = bool(round(best[\"fit_int\"]))\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è Entrenando modelo final con:\")\n",
        "print(f\"   alpha_1={alpha_1:.2e}, alpha_2={alpha_2:.2e}, lambda_1={lambda_1:.2e}, lambda_2={lambda_2:.2e}, tol={tol:.2e}\")\n",
        "\n",
        "final_model = BayesianRidge(\n",
        "    alpha_1=alpha_1, alpha_2=alpha_2,\n",
        "    lambda_1=lambda_1, lambda_2=lambda_2,\n",
        "    tol=tol, fit_intercept=fit_intercept\n",
        ")\n",
        "final_model.fit(X_train, yx_train)\n",
        "preds = final_model.predict(X_test)\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ EVALUACI√ìN FINAL (M√âTRICAS + DESV. EST√ÅNDAR)\n",
        "# ======================================================\n",
        "mae = mean_absolute_error(yx_test, preds)\n",
        "mse = mean_squared_error(yx_test, preds)\n",
        "r2 = r2_score(yx_test, preds)\n",
        "mape = mape_np(yx_test, preds)\n",
        "boot = bootstrap_stats(yx_test.to_numpy(), preds)\n",
        "\n",
        "print(\"\\nüìä RESULTADOS x_target:\")\n",
        "print(f\"MAE  = {mae:.6f} (¬± {boot['MAE'][1]:.6f})\")\n",
        "print(f\"MSE  = {mse:.6f} (¬± {boot['MSE'][1]:.6f})\")\n",
        "print(f\"R2   = {r2:.6f} (¬± {boot['R2'][1]:.6f})\")\n",
        "print(f\"MAPE = {mape:.6f} (¬± {boot['MAPE'][1]:.6f})\")\n",
        "\n",
        "print(\"\\n‚úÖ BAYESIAN RIDGE completado (BO + m√©tricas en CPU).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gA9WfPGLOnH3",
        "outputId": "6625af79-d1d6-4d7c-d9e8-0505d23c9ab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Dataset cargado: 1073215 filas √ó 31 columnas\n",
            "‚úÖ Divisi√≥n 60/20/20 completada (CPU).\n",
            "\n",
            "üöÄ Iniciando optimizaci√≥n bayesiana (x_target)...\n",
            "|   iter    |  target   | alpha_... | alpha_... | lambda... | lambda... |  tol_log  |  fit_int  |\n",
            "-------------------------------------------------------------------------------------------------\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-7.378219\u001b[39m | \u001b[39m-3.344999\u001b[39m | \u001b[39m-4.876042\u001b[39m | \u001b[39m-5.809390\u001b[39m | \u001b[39m-4.531944\u001b[39m | \u001b[39m0.1559945\u001b[39m |\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-9.593414\u001b[39m | \u001b[39m-3.936766\u001b[39m | \u001b[39m-5.792194\u001b[39m | \u001b[39m-5.043491\u001b[39m | \u001b[39m-4.938246\u001b[39m | \u001b[39m0.9699098\u001b[39m |\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-4.172901\u001b[39m | \u001b[39m-8.513626\u001b[39m | \u001b[39m-8.727225\u001b[39m | \u001b[39m-8.716168\u001b[39m | \u001b[39m-4.087273\u001b[39m | \u001b[39m0.5247564\u001b[39m |\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-6.976384\u001b[39m | \u001b[39m-7.961396\u001b[39m | \u001b[39m-5.717029\u001b[39m | \u001b[39m-9.023542\u001b[39m | \u001b[39m-4.123566\u001b[39m | \u001b[39m0.3663618\u001b[39m |\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-6.807510\u001b[39m | \u001b[39m-4.503768\u001b[39m | \u001b[39m-8.602283\u001b[39m | \u001b[39m-6.400358\u001b[39m | \u001b[39m-3.222756\u001b[39m | \u001b[39m0.0464504\u001b[39m |\n",
            "| \u001b[39m6        \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-5.747186\u001b[39m | \u001b[39m-8.806331\u001b[39m | \u001b[39m-9.544638\u001b[39m | \u001b[39m-3.357801\u001b[39m | \u001b[39m-2.103103\u001b[39m | \u001b[39m0.8083973\u001b[39m |\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-6.533761\u001b[39m | \u001b[39m-9.014606\u001b[39m | \u001b[39m-4.735671\u001b[39m | \u001b[39m-9.418970\u001b[39m | \u001b[39m-2.830627\u001b[39m | \u001b[39m0.7006646\u001b[39m |\n",
            "| \u001b[35m8        \u001b[39m | \u001b[35m-0.190823\u001b[39m | \u001b[35m-3.145463\u001b[39m | \u001b[35m-9.026651\u001b[39m | \u001b[35m-3.995661\u001b[39m | \u001b[35m-9.810796\u001b[39m | \u001b[35m-4.362689\u001b[39m | \u001b[35m0.4695497\u001b[39m |\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-7.391929\u001b[39m | \u001b[39m-5.249084\u001b[39m | \u001b[39m-7.000930\u001b[39m | \u001b[39m-5.998874\u001b[39m | \u001b[39m-3.133045\u001b[39m | \u001b[39m0.8531538\u001b[39m |\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-7.156599\u001b[39m | \u001b[39m-4.748565\u001b[39m | \u001b[39m-8.814840\u001b[39m | \u001b[39m-6.006859\u001b[39m | \u001b[39m-2.976775\u001b[39m | \u001b[39m0.2814681\u001b[39m |\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-6.834222\u001b[39m | \u001b[39m-3.927832\u001b[39m | \u001b[39m-8.874736\u001b[39m | \u001b[39m-5.836589\u001b[39m | \u001b[39m-2.355805\u001b[39m | \u001b[39m0.1886152\u001b[39m |\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-6.550885\u001b[39m | \u001b[39m-3.917703\u001b[39m | \u001b[39m-9.037598\u001b[39m | \u001b[39m-5.325491\u001b[39m | \u001b[39m-3.482695\u001b[39m | \u001b[39m0.0411489\u001b[39m |\n",
            "| \u001b[39m13       \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-7.893146\u001b[39m | \u001b[39m-3.349368\u001b[39m | \u001b[39m-9.451704\u001b[39m | \u001b[39m-6.234278\u001b[39m | \u001b[39m-3.359780\u001b[39m | \u001b[39m0.1614949\u001b[39m |\n",
            "| \u001b[39m14       \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-6.697290\u001b[39m | \u001b[39m-3.501957\u001b[39m | \u001b[39m-9.676130\u001b[39m | \u001b[39m-6.888700\u001b[39m | \u001b[39m-2.998851\u001b[39m | \u001b[39m0.7288495\u001b[39m |\n",
            "| \u001b[39m15       \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-8.035732\u001b[39m | \u001b[39m-3.592267\u001b[39m | \u001b[39m-8.970366\u001b[39m | \u001b[39m-5.087228\u001b[39m | \u001b[39m-3.223812\u001b[39m | \u001b[39m0.2729783\u001b[39m |\n",
            "| \u001b[39m16       \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-8.220743\u001b[39m | \u001b[39m-4.327136\u001b[39m | \u001b[39m-9.937388\u001b[39m | \u001b[39m-5.393351\u001b[39m | \u001b[39m-2.281082\u001b[39m | \u001b[39m0.3521599\u001b[39m |\n",
            "| \u001b[39m17       \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-8.733597\u001b[39m | \u001b[39m-4.144417\u001b[39m | \u001b[39m-9.496247\u001b[39m | \u001b[39m-5.873844\u001b[39m | \u001b[39m-3.988332\u001b[39m | \u001b[39m0.2594506\u001b[39m |\n",
            "| \u001b[39m18       \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-8.333511\u001b[39m | \u001b[39m-4.532589\u001b[39m | \u001b[39m-9.977331\u001b[39m | \u001b[39m-4.517122\u001b[39m | \u001b[39m-3.532457\u001b[39m | \u001b[39m0.0103099\u001b[39m |\n",
            "| \u001b[39m19       \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-6.833497\u001b[39m | \u001b[39m-4.861062\u001b[39m | \u001b[39m-8.911510\u001b[39m | \u001b[39m-4.074379\u001b[39m | \u001b[39m-2.153764\u001b[39m | \u001b[39m0.1039730\u001b[39m |\n",
            "| \u001b[39m20       \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-7.482134\u001b[39m | \u001b[39m-3.005535\u001b[39m | \u001b[39m-9.665790\u001b[39m | \u001b[39m-3.719559\u001b[39m | \u001b[39m-2.289614\u001b[39m | \u001b[39m0.3301516\u001b[39m |\n",
            "| \u001b[39m21       \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-5.815664\u001b[39m | \u001b[39m-3.031622\u001b[39m | \u001b[39m-8.308568\u001b[39m | \u001b[39m-3.720890\u001b[39m | \u001b[39m-2.088855\u001b[39m | \u001b[39m0.4519339\u001b[39m |\n",
            "| \u001b[39m22       \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-6.082936\u001b[39m | \u001b[39m-3.684790\u001b[39m | \u001b[39m-8.508333\u001b[39m | \u001b[39m-3.078930\u001b[39m | \u001b[39m-3.611792\u001b[39m | \u001b[39m0.1481454\u001b[39m |\n",
            "| \u001b[39m23       \u001b[39m | \u001b[39m-0.190823\u001b[39m | \u001b[39m-4.786030\u001b[39m | \u001b[39m-3.570767\u001b[39m | \u001b[39m-9.836128\u001b[39m | \u001b[39m-3.475254\u001b[39m | \u001b[39m-2.754434\u001b[39m | \u001b[39m0.0660462\u001b[39m |\n",
            "| \u001b[35m24       \u001b[39m | \u001b[35m-0.190823\u001b[39m | \u001b[35m-5.923786\u001b[39m | \u001b[35m-3.211464\u001b[39m | \u001b[35m-3.409356\u001b[39m | \u001b[35m-6.076876\u001b[39m | \u001b[35m-3.960915\u001b[39m | \u001b[35m0.0806113\u001b[39m |\n",
            "=================================================================================================\n",
            "\n",
            "üèÅ Mejores hiperpar√°metros encontrados:\n",
            "   - alpha_1_log: -5.923786\n",
            "   - alpha_2_log: -3.211464\n",
            "   - lambda_1_log: -3.409357\n",
            "   - lambda_2_log: -6.076877\n",
            "   - tol_log: -3.960915\n",
            "   - fit_int: 0.080611\n",
            "\n",
            "‚öôÔ∏è Entrenando modelo final con:\n",
            "   alpha_1=1.19e-06, alpha_2=6.15e-04, lambda_1=3.90e-04, lambda_2=8.38e-07, tol=1.09e-04\n",
            "\n",
            "üìä RESULTADOS x_target:\n",
            "MAE  = 0.190397 (¬± 0.000403)\n",
            "MSE  = 0.061968 (¬± 0.000326)\n",
            "R2   = 0.999886 (¬± 0.000001)\n",
            "MAPE = 0.003555 (¬± 0.000010)\n",
            "\n",
            "‚úÖ BAYESIAN RIDGE completado (BO + m√©tricas en CPU).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üéØ INFERENCIA BAYESIAN RIDGE con ESCALADO ROBUSTO\n",
        "#    ‚Ä¢ StandardScaler en X (ajustado en TRAIN y aplicado a TEST)\n",
        "#    ‚Ä¢ StandardScaler en y v√≠a TransformedTargetRegressor\n",
        "#    ‚Ä¢ float64 para estabilidad num√©rica\n",
        "#    ‚Ä¢ clipping f√≠sico al final (0‚Äì120, 0‚Äì53.3)\n",
        "# ======================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# ---------- Montar Drive ----------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "train_path = \"/content/drive/MyDrive/train_ready_final_numeric.csv\"\n",
        "test_path  = \"/content/drive/MyDrive/test_input_clean_final.csv\"\n",
        "out_path   = \"/content/drive/MyDrive/predicciones_bayesianridge_final_rescaled.csv\"\n",
        "\n",
        "# ---------- Cargar datos (float64 para estabilidad) ----------\n",
        "df_train = pd.read_csv(train_path).astype(np.float64, errors=\"ignore\")\n",
        "df_test  = pd.read_csv(test_path).astype(np.float64, errors=\"ignore\")\n",
        "\n",
        "print(f\"‚úÖ Train cargado: {df_train.shape[0]} filas √ó {df_train.shape[1]} cols\")\n",
        "print(f\"‚úÖ Test  cargado: {df_test.shape[0]} filas √ó {df_test.shape[1]} cols\")\n",
        "\n",
        "# ---------- Detectar targets ----------\n",
        "if \"x_target\" in df_train.columns and \"y_target\" in df_train.columns:\n",
        "    TARGET_X, TARGET_Y = \"x_target\", \"y_target\"\n",
        "elif \"ball_land_x\" in df_train.columns and \"ball_land_y\" in df_train.columns:\n",
        "    TARGET_X, TARGET_Y = \"ball_land_x\", \"ball_land_y\"\n",
        "else:\n",
        "    raise ValueError(\"No encuentro columnas de objetivo en train (x_target/y_target o ball_land_x/y).\")\n",
        "\n",
        "# ---------- Columnas prohibidas (fuga/ID/no-features) ----------\n",
        "leak_or_bad = {\n",
        "    TARGET_X, TARGET_Y, \"ball_land_x\", \"ball_land_y\",\n",
        "    \"dist_to_ball\", \"angle_to_ball\", \"vel_toward_ball\",\n",
        "    \"x\", \"y\", \"o\", \"dir\",\n",
        "    \"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"play_direction\",\n",
        "    \"player_name\", \"player_height\", \"player_birth_date\",\n",
        "    \"num_frames_output\"\n",
        "}\n",
        "\n",
        "# ---------- Construir feature_cols con ORDEN del train ----------\n",
        "train_cols = [c for c in df_train.columns if c not in leak_or_bad]\n",
        "feature_cols = [c for c in train_cols if c in df_test.columns]\n",
        "\n",
        "# Verificaci√≥n estricta de orden y longitud\n",
        "assert len(feature_cols) > 0, \"No hay columnas comunes v√°lidas entre train y test\"\n",
        "assert feature_cols == [c for c in df_train.columns if c in feature_cols], \"Orden de columnas desalineado\"\n",
        "\n",
        "print(f\"üîπ Features finales (comunes y sin fuga): {len(feature_cols)}\")\n",
        "\n",
        "# ---------- Preparar matrices ----------\n",
        "X_train_full = df_train[feature_cols].select_dtypes(include=[np.number]).astype(np.float64).fillna(0)\n",
        "yx_full      = df_train[TARGET_X].astype(np.float64)\n",
        "yy_full      = df_train[TARGET_Y].astype(np.float64)\n",
        "X_pred       = df_test[feature_cols].select_dtypes(include=[np.number]).astype(np.float64).fillna(0)\n",
        "\n",
        "# ---------- Comprobaciones de sanidad ----------\n",
        "def sanity(name, arr):\n",
        "    arr = np.asarray(arr)\n",
        "    print(f\"   ‚Ä¢ {name}: min={np.nanmin(arr):.3f}, max={np.nanmax(arr):.3f}, mean={np.nanmean(arr):.3f}\")\n",
        "    if not np.isfinite(arr).all():\n",
        "        raise ValueError(f\"{name} contiene NaN/Inf\")\n",
        "\n",
        "print(\"\\nüîé Chequeo de rangos (TRAIN y TARGETS):\")\n",
        "sanity(\"X_train_full\", X_train_full.values)\n",
        "sanity(\"yx_full\", yx_full.values)\n",
        "sanity(\"yy_full\", yy_full.values)\n",
        "sanity(\"X_pred\", X_pred.values)\n",
        "\n",
        "# ======================================================\n",
        "# üß† MODELOS (BayesianRidge + escalado en X y y)\n",
        "#    Usamos los hiperpar√°metros √≥ptimos que reportaste\n",
        "# ======================================================\n",
        "best_alpha_1 = 1.19e-06\n",
        "best_alpha_2 = 6.15e-04\n",
        "best_lambda_1 = 3.90e-04\n",
        "best_lambda_2 = 8.38e-07\n",
        "best_tol = 1.09e-04\n",
        "best_fit_intercept = True\n",
        "\n",
        "def make_br_pipe():\n",
        "    \"\"\"Pipeline: StandardScaler(X) + BayesianRidge con TTR(StandardScaler para y).\"\"\"\n",
        "    br = BayesianRidge(\n",
        "        alpha_1=best_alpha_1, alpha_2=best_alpha_2,\n",
        "        lambda_1=best_lambda_1, lambda_2=best_lambda_2,\n",
        "        tol=best_tol, fit_intercept=best_fit_intercept\n",
        "    )\n",
        "    # Escalado de y\n",
        "    ttr = TransformedTargetRegressor(regressor=br, transformer=StandardScaler())\n",
        "    # Escalado de X\n",
        "    pipe = make_pipeline(StandardScaler(), ttr)\n",
        "    return pipe\n",
        "\n",
        "print(\"\\n‚öôÔ∏è Entrenando pipelines con escalado num√©rico estable...\")\n",
        "pipe_x = make_br_pipe()\n",
        "pipe_y = make_br_pipe()\n",
        "\n",
        "pipe_x.fit(X_train_full, yx_full)\n",
        "pipe_y.fit(X_train_full, yy_full)\n",
        "\n",
        "# ---------- Predicci√≥n ----------\n",
        "df_test[\"x_pred\"] = pipe_x.predict(X_pred)\n",
        "df_test[\"y_pred\"] = pipe_y.predict(X_pred)\n",
        "\n",
        "print(\"\\n‚úÖ Predicci√≥n completada (tras deshacer el escalado de y).\")\n",
        "print(df_test[[\"x_pred\", \"y_pred\"]].head())\n",
        "\n",
        "# ---------- Clipping f√≠sico ----------\n",
        "df_test[\"x_pred_rescaled\"] = np.clip(df_test[\"x_pred\"], 0, 120)\n",
        "df_test[\"y_pred_rescaled\"] = np.clip(df_test[\"y_pred\"], 0, 53.3)\n",
        "\n",
        "print(\"\\nüìè Reescalado/Clipping f√≠sico:\")\n",
        "print(df_test[[\"x_pred\", \"y_pred\", \"x_pred_rescaled\", \"y_pred_rescaled\"]].head())\n",
        "\n",
        "# ---------- Guardar ----------\n",
        "# df_test.to_csv(out_path, index=False)\n",
        "# print(f\"\\n‚úÖ Archivo final guardado en: {out_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEiWf6_XU2iW",
        "outputId": "2d42b87a-1d00-4f80-dd98-907a99e59369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Train cargado: 1073215 filas √ó 31 cols\n",
            "‚úÖ Test  cargado: 49753 filas √ó 27 cols\n",
            "üîπ Features finales (comunes y sin fuga): 18\n",
            "\n",
            "üîé Chequeo de rangos (TRAIN y TARGETS):\n",
            "   ‚Ä¢ X_train_full: min=-4.502, max=10.447, mean=0.056\n",
            "   ‚Ä¢ yx_full: min=5.260, max=119.520, mean=62.985\n",
            "   ‚Ä¢ yy_full: min=0.620, max=52.620, mean=26.418\n",
            "   ‚Ä¢ X_pred: min=-8.099, max=347.000, mean=28.578\n",
            "\n",
            "‚öôÔ∏è Entrenando pipelines con escalado num√©rico estable...\n",
            "\n",
            "‚úÖ Predicci√≥n completada (tras deshacer el escalado de y).\n",
            "        x_pred     y_pred\n",
            "0 -2181.718160 -49.882929\n",
            "1 -2178.797862 -49.782423\n",
            "2 -2174.211301 -49.935795\n",
            "3 -2168.542508 -49.928305\n",
            "4 -2161.554100 -49.882773\n",
            "\n",
            "üìè Reescalado/Clipping f√≠sico:\n",
            "        x_pred     y_pred  x_pred_rescaled  y_pred_rescaled\n",
            "0 -2181.718160 -49.882929              0.0              0.0\n",
            "1 -2178.797862 -49.782423              0.0              0.0\n",
            "2 -2174.211301 -49.935795              0.0              0.0\n",
            "3 -2168.542508 -49.928305              0.0              0.0\n",
            "4 -2161.554100 -49.882773              0.0              0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo Bayesian Ridge no resulta adecuado para este conjunto de datos, ya que sus supuestos de linealidad, normalidad del ruido y buena condici√≥n num√©rica no se cumplen. Las variables del dataset presentan alta colinealidad, relaciones no lineales y distribuciones acotadas, lo que genera inestabilidad en los coeficientes y predicciones irreales.\n",
        "\n",
        "El modelo Bayesian Ridge Regression se basa en una formulaci√≥n lineal de la forma\n",
        "\n",
        "$$y=Xw+œµ,œµ‚àºN(0,Œ±‚àí1I)$$\n",
        "\n",
        "donde $ùë§$ representa los coeficientes del modelo y se asume un prior gaussiano\n",
        "\n",
        "$$w‚àºN(0,Œª‚àí1I)$$\n",
        "\n",
        "La inferencia bayesiana busca estimar la distribuci√≥n posterior\n",
        "$p(w‚à£X,y)$, cuya media sirve como vector de pesos regularizado:\n",
        "\n",
        "\n",
        "$$\n",
        "\\mathbf{w} = (X^{\\top}X + (\\tfrac{\\lambda}{\\alpha})I)^{-1} X^{\\top}y\n",
        "$$\n",
        "\n",
        "\n",
        "Este enfoque supone relaciones lineales, independencia entre predictores y ruido gaussiano homog√©neo.\n",
        "En el dataset de la competencia, las variables de entrada (posiciones, velocidades, √°ngulos, distancias, etc.) son altamente correlacionadas y no lineales, lo que hace que la matriz $ùëã^‚ä§ùëã$ est√© mal condicionada.\n",
        "Como consecuencia, el t√©rmino\n",
        "\n",
        "$$\n",
        "\\mathbf (X^{\\top}X + (\\tfrac{\\lambda}{\\alpha})I)^{-1}\n",
        "$$\n",
        "se vuelve num√©ricamente inestable y genera coeficientes $ùë§$ con magnitudes extremas, que al multiplicarse por $ùëã$ producen predicciones fuera del rango f√≠sico (yardas negativas o de miles de unidades).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lOZIzK1RcK7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gaussian Process Regressor**"
      ],
      "metadata": {
        "id": "Ci2QmpNbWu9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Despliegue matem√°tico, hiperpar√°metros y criterio de optimizaci√≥n**\n",
        "\n",
        "---\n",
        "\n",
        "## Fundamento del modelo\n",
        "\n",
        "El modelo **Gaussian Process Regressor (GPR)** se basa en el principio de que las variables de salida pueden modelarse como realizaciones de un **proceso estoc√°stico gaussiano**.  \n",
        "Esto significa que cualquier subconjunto finito de puntos de entrada $\\{\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_n\\}$ tiene una distribuci√≥n conjunta normal:\n",
        "\n",
        "$$\n",
        "\\mathbf{y} \\sim \\mathcal{N}(m(\\mathbf{X}), K(\\mathbf{X}, \\mathbf{X}))\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $m(\\mathbf{X})$ es la funci√≥n de media, generalmente asumida como cero,\n",
        "- $K(\\mathbf{X}, \\mathbf{X})$ es la matriz de covarianza o **kernel**, que define la dependencia entre los puntos del espacio de entrada.\n",
        "\n",
        "El objetivo del GPR es encontrar la distribuci√≥n posterior de las predicciones dadas las observaciones $(\\mathbf{X}, \\mathbf{y})$ y un nuevo punto $\\mathbf{x}_*$, definida por:\n",
        "\n",
        "$$\n",
        "p(f_* \\mid \\mathbf{X}, \\mathbf{y}, \\mathbf{x}_*) = \\mathcal{N}(\\bar{f}_*, \\, \\text{Var}(f_*))\n",
        "$$\n",
        "\n",
        "con:\n",
        "\n",
        "$$\n",
        "\\bar{f}_* = K(\\mathbf{x}_*, \\mathbf{X}) [K(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 I]^{-1} \\mathbf{y}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Var}(f_*) = K(\\mathbf{x}_*, \\mathbf{x}_*) - K(\\mathbf{x}_*, \\mathbf{X}) [K(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 I]^{-1} K(\\mathbf{X}, \\mathbf{x}_*)\n",
        "$$\n",
        "\n",
        "donde $\\sigma_n^2$ representa el nivel de ruido o varianza del error.\n",
        "\n",
        "---\n",
        "\n",
        "## Definici√≥n del kernel compuesto\n",
        "\n",
        "En este modelo, se utiliza un **kernel compuesto**:\n",
        "\n",
        "$$\n",
        "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\underbrace{C}_{\\text{Constante}} \\cdot \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2}{2l^2}\\right) + \\underbrace{\\sigma_n^2 \\delta_{ij}}_{\\text{Ruido blanco}}\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $l$ es la **longitud de escala (length scale)**, que controla la suavidad de la funci√≥n,\n",
        "- $\\sigma_n^2$ es el **nivel de ruido (noise level)**,\n",
        "- $\\delta_{ij}$ es el delta de Kronecker (1 si $i=j$, 0 en otro caso),\n",
        "- $C$ es un coeficiente de escala constante.\n",
        "\n",
        "Este kernel combina una parte **RBF (Radial Basis Function)** con un **t√©rmino de ruido blanco**, permitiendo al modelo representar tanto la tendencia suave de los datos como el ruido inherente a las observaciones.\n",
        "\n",
        "---\n",
        "\n",
        "## Hiperpar√°metros y su justificaci√≥n\n",
        "\n",
        "| Hiperpar√°metro | Rango definido | Descripci√≥n | Funci√≥n |\n",
        "|----------------|----------------|--------------|----------|\n",
        "| `log10_ls` | $[-2, 2]$ | Longitud de escala logar√≠tmica | Controla la correlaci√≥n entre puntos; valores peque√±os hacen que el modelo se adapte fuertemente a variaciones locales, valores grandes generan una funci√≥n m√°s suave. |\n",
        "| `log10_alpha` | $[-8, -2]$ | Nivel de ruido logar√≠tmico | Define la magnitud del ruido blanco en los datos; valores peque√±os asumen datos muy precisos, valores altos suponen ruido significativo. |\n",
        "\n",
        "Los rangos se definieron en **escala logar√≠tmica** para permitir la exploraci√≥n de varios √≥rdenes de magnitud (de $10^{-8}$ a $10^{2}$), lo cual es esencial dado que los hiperpar√°metros del kernel afectan exponencialmente la forma y amplitud de las predicciones.  \n",
        "Estos intervalos tambi√©n garantizan estabilidad num√©rica y evitan regiones de optimizaci√≥n degeneradas.\n",
        "\n",
        "El espacio de b√∫squeda se formaliza como:\n",
        "\n",
        "$$\n",
        "\\mathcal{H} = \\{ (\\log_{10} l, \\log_{10} \\alpha) \\mid \\log_{10} l \\in [-2, 2], \\, \\log_{10} \\alpha \\in [-8, -2] \\}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Criterio de optimizaci√≥n y score a minimizar\n",
        "\n",
        "El objetivo de la optimizaci√≥n es minimizar el **Error Absoluto Medio (MAE)** sobre el conjunto de validaci√≥n:\n",
        "\n",
        "$$\n",
        "MAE(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} | y_i - \\hat{y}_i(\\theta) |\n",
        "$$\n",
        "\n",
        "Sin embargo, como la **Optimizaci√≥n Bayesiana (BO)** maximiza la funci√≥n objetivo, se eval√∫a su versi√≥n negativa:\n",
        "\n",
        "$$\n",
        "f(\\theta) = -MAE(\\theta)\n",
        "$$\n",
        "\n",
        "De esta forma, el problema se formula como:\n",
        "\n",
        "$$\n",
        "\\theta^* = \\arg\\max_{\\theta \\in \\mathcal{H}} f(\\theta)\n",
        "\\quad \\Longleftrightarrow \\quad\n",
        "\\theta^* = \\arg\\min_{\\theta \\in \\mathcal{H}} MAE_{\\text{val}}(\\theta)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Proceso de Optimizaci√≥n Bayesiana (BO)\n",
        "\n",
        "La BO modela la funci√≥n objetivo $f(\\theta)$ como un **Proceso Gaussiano auxiliar**:\n",
        "\n",
        "$$\n",
        "f(\\theta) \\sim \\mathcal{GP}(m(\\theta),\\, k(\\theta, \\theta'))\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $m(\\theta)$ es la media a priori,\n",
        "- $k(\\theta, \\theta')$ es la covarianza entre configuraciones de hiperpar√°metros.\n",
        "\n",
        "En cada iteraci√≥n, se selecciona el siguiente conjunto de hiperpar√°metros $\\theta_*$ maximizando la funci√≥n de adquisici√≥n **Expected Improvement (EI)**:\n",
        "\n",
        "$$\n",
        "EI(\\theta) = (\\mu_t(\\theta) - f^+) \\Phi(z) + \\sigma_t(\\theta) \\phi(z)\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $f^+$ es el mejor valor observado,\n",
        "- $z = \\frac{\\mu_t(\\theta) - f^+}{\\sigma_t(\\theta)}$,\n",
        "- $\\Phi$ y $\\phi$ son la CDF y PDF de la distribuci√≥n normal est√°ndar, respectivamente.\n",
        "\n",
        "Este enfoque permite buscar de forma **inteligente y eficiente** los valores √≥ptimos de los hiperpar√°metros sin evaluar exhaustivamente todo el espacio.\n",
        "\n",
        "---\n",
        "\n",
        "## Justificaci√≥n de la selecci√≥n de hiperpar√°metros\n",
        "\n",
        "- **Longitud de escala (`length_scale`):**  \n",
        "  Determina cu√°n r√°pido var√≠a la funci√≥n de predicci√≥n con respecto a los cambios en las variables de entrada.  \n",
        "  Un rango amplio en logaritmo ($[10^{-2}, 10^{2}]$) permite capturar tanto relaciones locales como globales.\n",
        "\n",
        "- **Nivel de ruido (`alpha`):**  \n",
        "  Controla la robustez del modelo frente a errores de medici√≥n o ruido en los datos.  \n",
        "  Valores entre $10^{-8}$ y $10^{-2}$ equilibran adecuadamente precisi√≥n y estabilidad num√©rica.\n",
        "\n",
        "Estos hiperpar√°metros fueron optimizados mediante **BO**, garantizando que el modelo capture correctamente la estructura subyacente de los datos sin sobreajustar.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusi√≥n te√≥rico-computacional\n",
        "\n",
        "El **Gaussian Process Regressor (GPR)** proporciona una aproximaci√≥n no param√©trica y probabil√≠stica, capaz de estimar tanto las predicciones como la incertidumbre asociada a cada punto.  \n",
        "La combinaci√≥n de kernels **RBF + WhiteKernel** permite representar patrones suaves con ruido controlado.  \n",
        "La **Optimizaci√≥n Bayesiana** sobre los hiperpar√°metros logar√≠tmicos del *length scale* y el *noise level* asegura una b√∫squeda eficiente y robusta en un espacio altamente no lineal.  \n",
        "\n",
        "El **score a minimizar** es el $MAE_{\\text{val}}(\\theta)$, que garantiza una configuraci√≥n √≥ptima del modelo orientada a la menor desviaci√≥n promedio entre las predicciones y los valores reales.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "El9aw3e_fvLg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dckC2BKzSFSB",
        "outputId": "7e637bcd-4bc9-482b-cd23-11cbd7e4755c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset cargado: 1073215 filas √ó 31 columnas\n",
            "Divisi√≥n completada: Train 643929 Val 214643 Test 214643\n",
            "Usando 1500 puntos para entrenar y 500 para validar.\n",
            "\n",
            "üîé Iniciando optimizaci√≥n bayesiana...\n",
            "|   iter    |  target   | log10_ls  | log10_... |\n",
            "-------------------------------------------------\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-19.17580\u001b[39m | \u001b[39m-0.501839\u001b[39m | \u001b[39m-2.295714\u001b[39m |\n",
            "| \u001b[35m2        \u001b[39m | \u001b[35m-18.58323\u001b[39m | \u001b[35m0.9279757\u001b[39m | \u001b[35m-4.408049\u001b[39m |\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m-19.17614\u001b[39m | \u001b[39m-1.375925\u001b[39m | \u001b[39m-7.064032\u001b[39m |\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m-19.17614\u001b[39m | \u001b[39m-1.767665\u001b[39m | \u001b[39m-2.802943\u001b[39m |\n",
            "| \u001b[35m5        \u001b[39m | \u001b[35m-18.19961\u001b[39m | \u001b[35m1.7793091\u001b[39m | \u001b[35m-4.778806\u001b[39m |\n",
            "| \u001b[39m6        \u001b[39m | \u001b[39m-18.86981\u001b[39m | \u001b[39m2.0      \u001b[39m | \u001b[39m-6.278460\u001b[39m |\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m-18.86889\u001b[39m | \u001b[39m2.0      \u001b[39m | \u001b[39m-3.959158\u001b[39m |\n",
            "| \u001b[35m8        \u001b[39m | \u001b[35m-18.18488\u001b[39m | \u001b[35m1.7405519\u001b[39m | \u001b[35m-4.782224\u001b[39m |\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m-18.36042\u001b[39m | \u001b[39m1.1825893\u001b[39m | \u001b[39m-5.366519\u001b[39m |\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m-18.55631\u001b[39m | \u001b[39m0.9596104\u001b[39m | \u001b[39m-8.0     \u001b[39m |\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m-19.16963\u001b[39m | \u001b[39m-0.320851\u001b[39m | \u001b[39m-5.269029\u001b[39m |\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m-18.86981\u001b[39m | \u001b[39m2.0      \u001b[39m | \u001b[39m-8.0     \u001b[39m |\n",
            "| \u001b[39m13       \u001b[39m | \u001b[39m-18.21057\u001b[39m | \u001b[39m1.7898854\u001b[39m | \u001b[39m-5.252242\u001b[39m |\n",
            "| \u001b[39m14       \u001b[39m | \u001b[39m-18.80405\u001b[39m | \u001b[39m2.0      \u001b[39m | \u001b[39m-2.0     \u001b[39m |\n",
            "=================================================\n",
            "\n",
            "üèÅ Mejores hiperpar√°metros encontrados:\n",
            " - length_scale: 55.0240\n",
            " - noise: 1.6511e-05\n",
            "\n",
            "‚öô Entrenando modelo final (subset r√°pido)...\n",
            "Entrenamiento final completado en 0.38s\n",
            "\n",
            "üìä RESULTADOS TEST:\n",
            "MAE  = 18.951970\n",
            "MSE  = 537.293264\n",
            "R2   = 0.013739\n",
            "MAPE = 0.366312\n",
            "\n",
            "‚úÖ Modelo guardado en: /content/drive/MyDrive/gpr_models_fast/gpr_fast_model_1760471953.joblib\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# üöÄ PUNTO 4 - MODELO 7: GaussianProcessRegressor con Optimizaci√≥n Bayesiana (versi√≥n r√°pida)\n",
        "# ============================================================\n",
        "\n",
        "# --- Instalaciones necesarias ---\n",
        "# !pip -q install bayesian-optimization joblib\n",
        "\n",
        "# --- Imports ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time, os, joblib\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel\n",
        "\n",
        "# --- M√©tricas auxiliares ---\n",
        "def mape_np(y_true, y_pred, eps=1e-8):\n",
        "    denom = np.maximum(np.abs(y_true), eps)\n",
        "    return np.mean(np.abs((y_true - y_pred) / denom))\n",
        "\n",
        "# --- Carga dataset ---\n",
        "# (aqu√≠ se usa el archivo cargado, puedes reemplazar con tu ruta real si est√°s en Colab)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/train_ready_final_numeric.csv\")\n",
        "\n",
        "TARGET_X, TARGET_Y = \"x_target\", \"y_target\"\n",
        "X = df.drop(columns=[TARGET_X, TARGET_Y])\n",
        "y = df[TARGET_X].to_numpy()\n",
        "\n",
        "# Divisi√≥n 60/20/20\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"‚úÖ Dataset cargado: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n",
        "print(\"Divisi√≥n completada: Train\", X_train.shape[0], \"Val\", X_val.shape[0], \"Test\", X_test.shape[0])\n",
        "\n",
        "# ============================================================\n",
        "# ‚öôÔ∏è Modo r√°pido: submuestreo adaptativo\n",
        "# ============================================================\n",
        "# GaussianProcessRegressor escala O(n^3). Si dataset > Nmax, reducimos.\n",
        "def subsample(X, y, Nmax=1500):\n",
        "    if len(y) <= Nmax:\n",
        "        return X, y\n",
        "    idx = np.random.default_rng(42).choice(len(y), size=Nmax, replace=False)\n",
        "    return X.iloc[idx], y[idx]\n",
        "\n",
        "X_train_s, y_train_s = subsample(X_train, y_train, Nmax=1500)\n",
        "X_val_s, y_val_s = subsample(X_val, y_val, Nmax=500)\n",
        "\n",
        "print(f\"Usando {len(y_train_s)} puntos para entrenar y {len(y_val_s)} para validar.\")\n",
        "\n",
        "# ============================================================\n",
        "# üîç Definir funci√≥n objetivo para la optimizaci√≥n bayesiana\n",
        "# ============================================================\n",
        "def gpr_eval(log10_ls, log10_alpha):\n",
        "    \"\"\"\n",
        "    Evaluamos GaussianProcess con:\n",
        "    - length_scale = 10^log10_ls\n",
        "    - noise = 10^log10_alpha\n",
        "    Retorna: -MAE\n",
        "    \"\"\"\n",
        "    length_scale = 10 ** log10_ls\n",
        "    alpha = 10 ** log10_alpha\n",
        "    kernel = ConstantKernel(1.0) * RBF(length_scale=length_scale) + WhiteKernel(noise_level=alpha)\n",
        "\n",
        "    try:\n",
        "        gpr = GaussianProcessRegressor(kernel=kernel, normalize_y=True, optimizer=None)\n",
        "        gpr.fit(X_train_s, y_train_s)\n",
        "        preds = gpr.predict(X_val_s)\n",
        "        mae = mean_absolute_error(y_val_s, preds)\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è Error en entrenamiento:\", e)\n",
        "        mae = 1e6  # penaliza fallos\n",
        "    return -mae\n",
        "\n",
        "pbounds = {\n",
        "    \"log10_ls\": (-2, 2),       # length scale entre 0.01 y 100\n",
        "    \"log10_alpha\": (-8, -2),   # ruido entre 1e-8 y 1e-2\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# üöÄ Optimizaci√≥n Bayesiana (r√°pida)\n",
        "# ============================================================\n",
        "optimizer = BayesianOptimization(\n",
        "    f=gpr_eval,\n",
        "    pbounds=pbounds,\n",
        "    random_state=42,\n",
        "    verbose=2,\n",
        ")\n",
        "\n",
        "print(\"\\nüîé Iniciando optimizaci√≥n bayesiana...\")\n",
        "optimizer.maximize(init_points=4, n_iter=10)  # solo 10 iteraciones para acelerar\n",
        "\n",
        "best_params = optimizer.max[\"params\"]\n",
        "best_ls = 10 ** best_params[\"log10_ls\"]\n",
        "best_alpha = 10 ** best_params[\"log10_alpha\"]\n",
        "print(f\"\\nüèÅ Mejores hiperpar√°metros encontrados:\\n - length_scale: {best_ls:.4f}\\n - noise: {best_alpha:.4e}\")\n",
        "\n",
        "# ============================================================\n",
        "# üß† Entrenamiento final y evaluaci√≥n\n",
        "# ============================================================\n",
        "print(\"\\n‚öô Entrenando modelo final (subset r√°pido)...\")\n",
        "kernel = ConstantKernel(1.0) * RBF(length_scale=best_ls) + WhiteKernel(noise_level=best_alpha)\n",
        "gpr_final = GaussianProcessRegressor(kernel=kernel, normalize_y=True, optimizer=None)\n",
        "t0 = time.time()\n",
        "gpr_final.fit(X_train_s, y_train_s)\n",
        "print(f\"Entrenamiento final completado en {time.time()-t0:.2f}s\")\n",
        "\n",
        "preds_test = gpr_final.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, preds_test)\n",
        "mse = np.mean((y_test - preds_test)**2)\n",
        "ss_res = np.sum((y_test - preds_test)**2)\n",
        "ss_tot = np.sum((y_test - np.mean(y_test))**2) + 1e-12\n",
        "r2 = 1 - ss_res / ss_tot\n",
        "mape = mape_np(y_test, preds_test)\n",
        "\n",
        "print(\"\\nüìä RESULTADOS TEST:\")\n",
        "print(f\"MAE  = {mae:.6f}\")\n",
        "print(f\"MSE  = {mse:.6f}\")\n",
        "print(f\"R2   = {r2:.6f}\")\n",
        "print(f\"MAPE = {mape:.6f}\")\n",
        "\n",
        "# ============================================================\n",
        "# üíæ Guardar modelo\n",
        "# ============================================================\n",
        "# descomente estas lineas de codigo para descargar el dataset con las predicciones!\n",
        "# out_dir = \"/content/drive/MyDrive/gpr_models_fast\"\n",
        "# os.makedirs(out_dir, exist_ok=True)\n",
        "# path_model = os.path.join(out_dir, f\"gpr_fast_model_{int(time.time())}.joblib\")\n",
        "# joblib.dump(gpr_final, path_model)\n",
        "# print(f\"\\n‚úÖ Modelo guardado en: {path_model}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üéØ INFERENCIA GAUSSIAN PROCESS REGRESSOR (CPU, optimizado RAM)\n",
        "# Usa: train_ready_final_numeric.csv + test_input_clean_final.csv\n",
        "# Guarda: /content/drive/MyDrive/predicciones_gpr_final_rescaled.csv\n",
        "# ======================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from google.colab import drive\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ MONTAR GOOGLE DRIVE\n",
        "# ======================================================\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "train_path = \"/content/drive/MyDrive/train_ready_final_numeric.csv\"\n",
        "test_path  = \"/content/drive/MyDrive/test_input_clean_final.csv\"\n",
        "out_path   = \"/content/drive/MyDrive/predicciones_gpr_final_rescaled.csv\"\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ CARGAR DATOS\n",
        "# ======================================================\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_test  = pd.read_csv(test_path)\n",
        "print(f\"‚úÖ Train: {df_train.shape}, Test: {df_test.shape}\")\n",
        "\n",
        "# Detectar columnas objetivo\n",
        "if \"x_target\" in df_train.columns and \"y_target\" in df_train.columns:\n",
        "    TARGET_X, TARGET_Y = \"x_target\", \"y_target\"\n",
        "elif \"ball_land_x\" in df_train.columns and \"ball_land_y\" in df_train.columns:\n",
        "    TARGET_X, TARGET_Y = \"ball_land_x\", \"ball_land_y\"\n",
        "else:\n",
        "    raise ValueError(\"No se encuentran columnas objetivo (x_target/y_target o ball_land_x/y).\")\n",
        "\n",
        "# Columnas prohibidas (para evitar fuga de informaci√≥n)\n",
        "leak_or_bad = {\n",
        "    \"x_target\", \"y_target\", \"ball_land_x\", \"ball_land_y\",\n",
        "    \"dist_to_ball\", \"angle_to_ball\", \"vel_toward_ball\",\n",
        "    \"x\", \"y\", \"o\", \"dir\",\n",
        "    \"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"play_direction\",\n",
        "    \"player_name\", \"player_height\", \"player_birth_date\",\n",
        "    \"num_frames_output\"\n",
        "}\n",
        "\n",
        "# Seleccionar columnas seguras\n",
        "train_cols = [c for c in df_train.columns if c not in leak_or_bad]\n",
        "feature_cols = [c for c in train_cols if c in df_test.columns]\n",
        "if len(feature_cols) == 0:\n",
        "    raise ValueError(\"No hay columnas v√°lidas comunes entre train y test.\")\n",
        "print(f\"üîπ Features finales: {len(feature_cols)}\")\n",
        "\n",
        "# Preparar matrices\n",
        "X_train_full = df_train[feature_cols].astype(\"float32\").fillna(0)\n",
        "yx_full = df_train[TARGET_X].astype(\"float32\")\n",
        "yy_full = df_train[TARGET_Y].astype(\"float32\")\n",
        "X_pred = df_test[feature_cols].astype(\"float32\").fillna(0)\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ SUBMUESTREO PARA EVITAR USO EXCESIVO DE RAM\n",
        "# ======================================================\n",
        "MAX_TRAIN = 1500  # n√∫mero m√°ximo de muestras para GPR\n",
        "if len(X_train_full) > MAX_TRAIN:\n",
        "    print(f\"‚ö† Reducci√≥n del dataset: usando {MAX_TRAIN} de {len(X_train_full)} muestras.\")\n",
        "    idx = np.random.default_rng(42).choice(len(X_train_full), size=MAX_TRAIN, replace=False)\n",
        "    X_train_sub = X_train_full.iloc[idx]\n",
        "    yx_sub = yx_full.iloc[idx]\n",
        "    yy_sub = yy_full.iloc[idx]\n",
        "else:\n",
        "    X_train_sub, yx_sub, yy_sub = X_train_full, yx_full, yy_full\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ CREAR O CARGAR MODELOS GPR\n",
        "# ======================================================\n",
        "path_model_x = \"/content/drive/MyDrive/gpr_models_fast/gpr_fast_model_x.joblib\"\n",
        "path_model_y = \"/content/drive/MyDrive/gpr_models_fast/gpr_fast_model_y.joblib\"\n",
        "\n",
        "best_length_scale = 55.0240   # de tu optimizaci√≥n\n",
        "best_noise = 1.6511e-05\n",
        "\n",
        "kernel = ConstantKernel(1.0) * RBF(length_scale=best_length_scale) + WhiteKernel(noise_level=best_noise)\n",
        "\n",
        "try:\n",
        "    model_x = joblib.load(path_model_x)\n",
        "    model_y = joblib.load(path_model_y)\n",
        "    print(\"‚úÖ Modelos GPR cargados correctamente.\")\n",
        "except:\n",
        "    print(\"‚ö† Modelos no encontrados. Entrenando nuevos modelos GPR (subset reducido)...\")\n",
        "    model_x = GaussianProcessRegressor(kernel=kernel, normalize_y=True, optimizer=None)\n",
        "    model_y = GaussianProcessRegressor(kernel=kernel, normalize_y=True, optimizer=None)\n",
        "    model_x.fit(X_train_sub, yx_sub)\n",
        "    model_y.fit(X_train_sub, yy_sub)\n",
        "    joblib.dump(model_x, path_model_x)\n",
        "    joblib.dump(model_y, path_model_y)\n",
        "    print(\"‚úÖ Nuevos modelos GPR entrenados y guardados.\")\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ INFERENCIA\n",
        "# ======================================================\n",
        "print(\"\\nüîÆ Generando predicciones sobre el dataset de test...\")\n",
        "df_test[\"x_pred\"] = model_x.predict(X_pred)\n",
        "df_test[\"y_pred\"] = model_y.predict(X_pred)\n",
        "\n",
        "print(\"‚úÖ Predicci√≥n completada.\")\n",
        "print(df_test[[\"x_pred\", \"y_pred\"]].head())\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ REESCALADO A YARDAS F√çSICAS\n",
        "# ======================================================\n",
        "def rescale_to_field(preds, new_min, new_max):\n",
        "    old_min, old_max = float(preds.min()), float(preds.max())\n",
        "    if abs(old_max - old_min) < 1e-6:\n",
        "        return np.clip(preds, new_min, new_max)\n",
        "    return (preds - old_min) / (old_max - old_min) * (new_max - new_min) + new_min\n",
        "\n",
        "df_test[\"x_pred_rescaled\"] = rescale_to_field(df_test[\"x_pred\"], 0, 120)\n",
        "df_test[\"y_pred_rescaled\"] = rescale_to_field(df_test[\"y_pred\"], 0, 53.3)\n",
        "\n",
        "print(\"\\nüìè Reescalado completado:\")\n",
        "print(df_test[[\"x_pred\", \"y_pred\", \"x_pred_rescaled\", \"y_pred_rescaled\"]].head())\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ GUARDAR RESULTADOS\n",
        "# # ======================================================\n",
        "# descomente estas lineas par aguardar el dataset de predicci√≥n!\n",
        "# df_test.to_csv(out_path, index=False)\n",
        "# print(f\"\\n‚úÖ Archivo final guardado en: {out_path}\")\n",
        "# print(\"üéØ Predicciones reescaladas en yardas reales listas para an√°lisis.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4Djfs-ZT_4a",
        "outputId": "7405fd80-8c57-4fbd-f261-8298dfcedfa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Train: (1073215, 31), Test: (49753, 27)\n",
            "üîπ Features finales: 18\n",
            "‚ö† Reducci√≥n del dataset: usando 1500 de 1073215 muestras.\n",
            "‚ö† Modelos no encontrados. Entrenando nuevos modelos GPR (subset reducido)...\n",
            "‚úÖ Nuevos modelos GPR entrenados y guardados.\n",
            "\n",
            "üîÆ Generando predicciones sobre el dataset de test...\n",
            "‚úÖ Predicci√≥n completada.\n",
            "      x_pred     y_pred\n",
            "0  63.588225  26.525377\n",
            "1  63.588219  26.525388\n",
            "2  63.588206  26.525393\n",
            "3  63.588186  26.525389\n",
            "4  63.588163  26.525381\n",
            "\n",
            "üìè Reescalado completado:\n",
            "      x_pred     y_pred  x_pred_rescaled  y_pred_rescaled\n",
            "0  63.588225  26.525377       111.732747        36.416402\n",
            "1  63.588219  26.525388       111.732377        36.416896\n",
            "2  63.588206  26.525393       111.731592        36.417135\n",
            "3  63.588186  26.525389       111.730376        36.416941\n",
            "4  63.588163  26.525381       111.728960        36.416580\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusion\n",
        "‚úÖ Conclusi√≥n final\n",
        "\n",
        "El modelo **GaussianProcessRegressor con Optimizaci√≥n Bayesiana** permiti√≥ ajustar los hiperpar√°metros √≥ptimos y generar predicciones eficientes sobre el conjunto de datos.\n",
        "Sin embargo, debido a la alta complejidad computacional del GPR ((O(n^3))), se aplic√≥ un **submuestreo controlado** para reducir el tama√±o del dataset y evitar fallos por memoria o tiempo de ejecuci√≥n.\n",
        "\n",
        "Este proceso implica un **compromiso entre precisi√≥n y viabilidad computacional**: aunque el submuestreo puede reducir ligeramente la exactitud del modelo, permite **mantener una representaci√≥n estad√≠stica suficiente de los datos** y obtener **predicciones estables** dentro de los recursos disponibles.\n",
        "\n",
        "En resumen, el submuestreo no elimina la validez del modelo, sino que **garantiza su ejecuci√≥n pr√°ctica** en escenarios donde el volumen de datos supera la capacidad del algoritmo GPR.\n"
      ],
      "metadata": {
        "id": "1ppg419kl7bx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Support Vector Machines Regressor**"
      ],
      "metadata": {
        "id": "ARaAr3BVoiqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Despliegue matem√°tico, hiperpar√°metros y criterio de optimizaci√≥n**\n",
        "\n",
        "---\n",
        "\n",
        "## Fundamento del modelo\n",
        "\n",
        "El modelo **Support Vector Machines Regressor (SVR)** se basa en la teor√≠a de los **vectores de soporte**, donde la idea central es encontrar una funci√≥n que aproxime los datos con un margen de tolerancia $\\epsilon$ y con la mayor planitud posible.\n",
        "\n",
        "Formalmente, se busca una funci√≥n de regresi√≥n:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} + b\n",
        "$$\n",
        "\n",
        "que minimice la complejidad del modelo bajo el principio de **m√°ximo margen**.  \n",
        "El problema de optimizaci√≥n primal se formula como:\n",
        "\n",
        "$$\n",
        "\\min_{\\mathbf{w}, b, \\xi_i, \\xi_i^*} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{n} (\\xi_i + \\xi_i^*)\n",
        "$$\n",
        "\n",
        "sujeto a:\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "y_i - \\mathbf{w}^\\top \\mathbf{x}_i - b \\leq \\epsilon + \\xi_i \\\\\n",
        "\\mathbf{w}^\\top \\mathbf{x}_i + b - y_i \\leq \\epsilon + \\xi_i^* \\\\\n",
        "\\xi_i, \\xi_i^* \\geq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $\\mathbf{w}$ representa el vector de pesos,  \n",
        "- $b$ es el sesgo,  \n",
        "- $C$ es el par√°metro de penalizaci√≥n de errores,  \n",
        "- $\\epsilon$ define el margen de tolerancia dentro del cual los errores no son penalizados,  \n",
        "- $\\xi_i$ y $\\xi_i^*$ son variables de holgura para los errores fuera del margen.\n",
        "\n",
        "---\n",
        "\n",
        "## Transformaci√≥n al problema dual y uso del kernel\n",
        "\n",
        "Mediante el m√©todo de Lagrange, el problema se transforma al dual:\n",
        "\n",
        "$$\n",
        "\\max_{\\alpha_i, \\alpha_i^*} -\\frac{1}{2}\\sum_{i,j=1}^{n} (\\alpha_i - \\alpha_i^*)(\\alpha_j - \\alpha_j^*) K(\\mathbf{x}_i, \\mathbf{x}_j)\n",
        " - \\epsilon \\sum_{i=1}^{n} (\\alpha_i + \\alpha_i^*) + \\sum_{i=1}^{n} y_i (\\alpha_i - \\alpha_i^*)\n",
        "$$\n",
        "\n",
        "sujeto a:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^{n} (\\alpha_i - \\alpha_i^*) = 0, \\quad 0 \\leq \\alpha_i, \\alpha_i^* \\leq C\n",
        "$$\n",
        "\n",
        "donde $K(\\mathbf{x}_i, \\mathbf{x}_j)$ es una **funci√≥n kernel** que permite proyectar los datos a un espacio de mayor dimensi√≥n para capturar relaciones no lineales.  \n",
        "En este caso se usa el **kernel radial o RBF**:\n",
        "\n",
        "$$\n",
        "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma \\| \\mathbf{x}_i - \\mathbf{x}_j \\|^2)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Hiperpar√°metros y su justificaci√≥n\n",
        "\n",
        "| Hiperpar√°metro | Rango definido | Descripci√≥n | Funci√≥n |\n",
        "|----------------|----------------|--------------|----------|\n",
        "| `C` | $[0.1, 50.0]$ | Coeficiente de penalizaci√≥n | Controla el equilibrio entre margen amplio y minimizaci√≥n del error. Valores grandes reducen el margen, favoreciendo menor sesgo pero mayor varianza. |\n",
        "| `epsilon` | $[0.001, 1.0]$ | Margen de tolerancia | Define la insensibilidad del modelo ante errores peque√±os; valores grandes permiten m√°s tolerancia y menor sobreajuste. |\n",
        "| `gamma` | $[0.0001, 0.5]$ | Par√°metro del kernel RBF | Controla la influencia de cada punto de entrenamiento; valores altos generan fronteras m√°s complejas (menor sesgo, mayor varianza). |\n",
        "\n",
        "Estos rangos se definieron con base en la escala y variabilidad del dataset, y para asegurar una b√∫squeda balanceada entre **modelos suaves y modelos complejos**.  \n",
        "Se limitaron a intervalos positivos (mayores que cero) para garantizar estabilidad num√©rica y evitar regiones degeneradas del espacio de b√∫squeda.\n",
        "\n",
        "El espacio de hiperpar√°metros se expresa formalmente como:\n",
        "\n",
        "$$\n",
        "\\mathcal{H} = \\{ (C, \\epsilon, \\gamma) \\mid C \\in [0.1, 50], \\, \\epsilon \\in [10^{-3}, 1], \\, \\gamma \\in [10^{-4}, 0.5] \\}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Criterio de optimizaci√≥n y score a minimizar\n",
        "\n",
        "El **score** a minimizar es el **Error Absoluto Medio (MAE)** en el conjunto de validaci√≥n:\n",
        "\n",
        "$$\n",
        "MAE(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} | y_i - \\hat{y}_i(\\theta) |\n",
        "$$\n",
        "\n",
        "Dado que la **Optimizaci√≥n Bayesiana** maximiza la funci√≥n objetivo, se utiliza el negativo de esta m√©trica:\n",
        "\n",
        "$$\n",
        "f(\\theta) = -MAE(\\theta)\n",
        "$$\n",
        "\n",
        "Por tanto, el proceso de b√∫squeda se formula como:\n",
        "\n",
        "$$\n",
        "\\theta^* = \\arg\\max_{\\theta \\in \\mathcal{H}} f(\\theta)\n",
        "\\quad \\Longleftrightarrow \\quad\n",
        "\\theta^* = \\arg\\min_{\\theta \\in \\mathcal{H}} MAE_{\\text{val}}(\\theta)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Proceso de Optimizaci√≥n Bayesiana (BO)\n",
        "\n",
        "La **Optimizaci√≥n Bayesiana (BO)** modela la funci√≥n objetivo $f(\\theta)$ como un **Proceso Gaussiano (GP)**:\n",
        "\n",
        "$$\n",
        "f(\\theta) \\sim \\mathcal{GP}(m(\\theta),\\, k(\\theta, \\theta'))\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $m(\\theta)$ es la media a priori (com√∫nmente 0),\n",
        "- $k(\\theta, \\theta')$ es la covarianza entre configuraciones de hiperpar√°metros.\n",
        "\n",
        "El GP estima, para cada configuraci√≥n candidata $\\theta_*$, una distribuci√≥n:\n",
        "\n",
        "$$\n",
        "f(\\theta_*) \\mid \\mathcal{D}_t \\sim \\mathcal{N}(\\mu_t(\\theta_*),\\, \\sigma_t^2(\\theta_*))\n",
        "$$\n",
        "\n",
        "La funci√≥n de adquisici√≥n empleada es **Expected Improvement (EI)**:\n",
        "\n",
        "$$\n",
        "EI(\\theta) = (\\mu_t(\\theta) - f^+) \\Phi(z) + \\sigma_t(\\theta) \\phi(z)\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $f^+$ es el mejor valor observado hasta ese momento,  \n",
        "- $z = \\frac{\\mu_t(\\theta) - f^+}{\\sigma_t(\\theta)}$,  \n",
        "- $\\Phi$ y $\\phi$ son respectivamente la CDF y la PDF de la distribuci√≥n normal est√°ndar.\n",
        "\n",
        "---\n",
        "\n",
        "## Justificaci√≥n de la selecci√≥n de hiperpar√°metros\n",
        "\n",
        "- **C:** controla la penalizaci√≥n de errores; un rango amplio ($[0.1, 50]$) permite explorar desde modelos altamente regularizados hasta modelos m√°s flexibles.  \n",
        "- **epsilon:** define el margen de insensibilidad; el rango ($[0.001, 1.0]$) busca balancear entre rigidez y tolerancia a ruido.  \n",
        "- **gamma:** determina la suavidad del kernel RBF; valores peque√±os generalizan mejor, valores grandes capturan m√°s detalle local.  \n",
        "\n",
        "La combinaci√≥n de estos par√°metros regula el **trade-off entre sesgo y varianza**, garantizando que la BO encuentre la configuraci√≥n m√°s adecuada sin necesidad de evaluar exhaustivamente todo el espacio.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusi√≥n te√≥rico-computacional\n",
        "\n",
        "El modelo **SVR con kernel RBF** permite capturar relaciones no lineales entre las variables predictoras y la variable objetivo, manteniendo una alta capacidad de generalizaci√≥n.  \n",
        "Los hiperpar√°metros $C$, $\\epsilon$ y $\\gamma$ determinan la flexibilidad y robustez del modelo, y su optimizaci√≥n mediante BO permite explorar el espacio de b√∫squeda de forma eficiente.  \n",
        "\n",
        "El **score a minimizar** es el $MAE_{\\text{val}}(\\theta)$, garantizando que la configuraci√≥n final logre la menor desviaci√≥n promedio entre predicciones y valores reales en validaci√≥n.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "HhHeqgl2eMMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üß† PUNTO 4 - MODELO 8: SUPPORT VECTOR MACHINES REGRESSOR (Optimizado CPU)\n",
        "# ======================================================\n",
        "\n",
        "!pip -q install bayesian-optimization\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# ---------- Cargar datos ----------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/train_ready_final_numeric.csv\")\n",
        "\n",
        "# ---------- Submuestreo para eficiencia ----------\n",
        "df_sample = df.sample(n=min(5000, len(df)), random_state=42)\n",
        "print(f\"‚úÖ Usando submuestra de {len(df_sample)} filas para optimizaci√≥n.\")\n",
        "\n",
        "# ---------- Divisi√≥n de datos ----------\n",
        "TARGET_X, TARGET_Y = \"x_target\", \"y_target\"\n",
        "X = df_sample.drop(columns=[TARGET_X, TARGET_Y])\n",
        "y_x, y_y = df_sample[TARGET_X], df_sample[TARGET_Y]\n",
        "\n",
        "X_train, X_temp, yx_train, yx_temp = train_test_split(X, y_x, test_size=0.4, random_state=42)\n",
        "X_val, X_test, yx_val, yx_test = train_test_split(X_temp, yx_temp, test_size=0.5, random_state=42)\n",
        "print(\"‚úÖ Divisi√≥n 60/20/20 completada.\")\n",
        "\n",
        "# ======================================================\n",
        "# üöÄ Optimizaci√≥n bayesiana (ligera)\n",
        "# ======================================================\n",
        "def svr_eval(C, epsilon, gamma):\n",
        "    C = max(C, 0.1)\n",
        "    epsilon = max(epsilon, 0.001)\n",
        "    gamma = max(gamma, 0.0001)\n",
        "    model = SVR(kernel='rbf', C=C, epsilon=epsilon, gamma=gamma)\n",
        "    model.fit(X_train, yx_train)\n",
        "    preds = model.predict(X_val)\n",
        "    return -mean_absolute_error(yx_val, preds)\n",
        "\n",
        "pbounds = {\"C\": (0.1, 50.0), \"epsilon\": (0.001, 1.0), \"gamma\": (0.0001, 0.5)}\n",
        "\n",
        "optimizer_x = BayesianOptimization(f=svr_eval, pbounds=pbounds, random_state=42, verbose=2)\n",
        "optimizer_x.maximize(init_points=3, n_iter=7)  # Menos iteraciones ‚Üí m√°s r√°pido\n",
        "\n",
        "best_x = optimizer_x.max[\"params\"]\n",
        "print(\"\\nüèÅ Mejores hiperpar√°metros encontrados:\")\n",
        "for k, v in best_x.items():\n",
        "    print(f\"   - {k}: {v:.6f}\")\n",
        "\n",
        "# ======================================================\n",
        "# ‚öôÔ∏è Entrenamiento final con los hiperpar√°metros √≥ptimos\n",
        "# ======================================================\n",
        "model_x = SVR(kernel='rbf', C=best_x[\"C\"], epsilon=best_x[\"epsilon\"], gamma=best_x[\"gamma\"])\n",
        "model_x.fit(X_train, yx_train)\n",
        "preds_x = model_x.predict(X_test)\n",
        "mae = mean_absolute_error(yx_test, preds_x)\n",
        "\n",
        "print(f\"\\nüìä MAE final: {mae:.6f}\")\n",
        "print(\"‚úÖ Entrenamiento y optimizaci√≥n completados eficientemente.\")\n"
      ],
      "metadata": {
        "id": "M1skRDkN04gM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2293afef-7a5b-460c-b084-7dc8d1238fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Usando submuestra de 5000 filas para optimizaci√≥n.\n",
            "‚úÖ Divisi√≥n 60/20/20 completada.\n",
            "|   iter    |  target   |     C     |  epsilon  |   gamma   |\n",
            "-------------------------------------------------------------\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-19.64310\u001b[39m | \u001b[39m18.789551\u001b[39m | \u001b[39m0.9507635\u001b[39m | \u001b[39m0.3660237\u001b[39m |\n",
            "| \u001b[35m2        \u001b[39m | \u001b[35m-19.56071\u001b[39m | \u001b[35m29.973058\u001b[39m | \u001b[35m0.1568626\u001b[39m | \u001b[35m0.0780816\u001b[39m |\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m-19.64832\u001b[39m | \u001b[39m2.9983722\u001b[39m | \u001b[39m0.8663099\u001b[39m | \u001b[39m0.3005973\u001b[39m |\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m-19.61370\u001b[39m | \u001b[39m9.1730658\u001b[39m | \u001b[39m0.5037062\u001b[39m | \u001b[39m0.1710587\u001b[39m |\n",
            "| \u001b[35m5        \u001b[39m | \u001b[35m-19.52317\u001b[39m | \u001b[35m30.014113\u001b[39m | \u001b[35m0.1253328\u001b[39m | \u001b[35m0.0595171\u001b[39m |\n",
            "| \u001b[35m6        \u001b[39m | \u001b[35m-19.37984\u001b[39m | \u001b[35m25.399735\u001b[39m | \u001b[35m0.0377597\u001b[39m | \u001b[35m0.0271026\u001b[39m |\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m-19.61344\u001b[39m | \u001b[39m25.375299\u001b[39m | \u001b[39m0.0219923\u001b[39m | \u001b[39m0.1644056\u001b[39m |\n",
            "| \u001b[39m8        \u001b[39m | \u001b[39m-19.73036\u001b[39m | \u001b[39m34.635622\u001b[39m | \u001b[39m0.3810356\u001b[39m | \u001b[39m0.3421009\u001b[39m |\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m-19.65443\u001b[39m | \u001b[39m2.0817317\u001b[39m | \u001b[39m0.4125405\u001b[39m | \u001b[39m0.4122349\u001b[39m |\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m-19.44031\u001b[39m | \u001b[39m25.360721\u001b[39m | \u001b[39m0.0906133\u001b[39m | \u001b[39m0.0381958\u001b[39m |\n",
            "=============================================================\n",
            "\n",
            "üèÅ Mejores hiperpar√°metros encontrados:\n",
            "   - C: 25.399736\n",
            "   - epsilon: 0.037760\n",
            "   - gamma: 0.027103\n",
            "\n",
            "üìä MAE final: 18.808268\n",
            "‚úÖ Entrenamiento y optimizaci√≥n completados eficientemente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El submuestreo aplicado en el c√≥digo se debe a la alta complejidad computacional del modelo Support Vector Machines Regressor (SVR). Este algoritmo, especialmente cuando se usa con el kernel radial (RBF), requiere calcular y almacenar una matriz de Gram de tama√±o $N \\times N$, donde $N$ es el n√∫mero de muestras del conjunto de entrenamiento. Dado que el dataset de la competencia de la NFL contiene cientos de miles de registros, la construcci√≥n de esta matriz implica un consumo de memoria y tiempo de procesamiento cuadr√°tico, lo que hace inviable ejecutar el modelo completo en un entorno de hardware limitado como Google Colab. Por esta raz√≥n, se utiliza una submuestra representativa de aproximadamente 5 000 instancias para realizar la optimizaci√≥n y el entrenamiento, reduciendo as√≠ la carga computacional.\n",
        "\n",
        "Sin embargo, esta reducci√≥n trae consigo una p√©rdida significativa de informaci√≥n, ya que el modelo aprende solo sobre una fracci√≥n del espacio de caracter√≠sticas, lo que limita su capacidad de generalizaci√≥n sobre el dataset completo. Adem√°s, el SVR no escala bien con grandes vol√∫menes de datos debido a su naturaleza no lineal y dependiente del n√∫mero total de muestras: cada nuevo punto afecta globalmente la soluci√≥n del problema de optimizaci√≥n cuadr√°tica.\n",
        "\n",
        "En el contexto de la base de datos de la NFL, donde existen m√∫ltiples variables, ruido estructural y alta dimensionalidad, el SVR se vuelve un modelo computacionalmente ineficiente y poco pr√°ctico para producci√≥n."
      ],
      "metadata": {
        "id": "lu5qBdQrfANy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RandomForestRegressor**"
      ],
      "metadata": {
        "id": "6uO8wRGPh8YV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Optimizaci√≥n y Justificaci√≥n de Hiperpar√°metros ‚Äî Random Forest Regressor\n",
        "\n",
        "## Formulaci√≥n del Modelo\n",
        "\n",
        "El **Random Forest Regressor (RFR)** es un modelo basado en un conjunto de √°rboles de decisi√≥n entrenados de forma independiente y combinados mediante un promedio ponderado de sus predicciones.  \n",
        "\n",
        "Sea el conjunto de entrenamiento:\n",
        "\n",
        "$$\n",
        "\\mathcal{D} = \\{ (\\mathbf{x}_i, y_i) \\}_{i=1}^N, \\quad \\mathbf{x}_i \\in \\mathbb{R}^d, \\quad y_i \\in \\mathbb{R}\n",
        "$$\n",
        "\n",
        "El modelo genera \\( M \\) √°rboles \\( h_m(\\mathbf{x}) \\), y la predicci√≥n final es:\n",
        "\n",
        "$$\n",
        "\\hat{y}(\\mathbf{x}) = \\frac{1}{M} \\sum_{m=1}^{M} h_m(\\mathbf{x})\n",
        "$$\n",
        "\n",
        "Cada √°rbol se entrena sobre una muestra **bootstrap** del conjunto de datos, y en cada nodo se selecciona un subconjunto aleatorio de caracter√≠sticas para dividir.\n",
        "\n",
        "---\n",
        "\n",
        "##  Funci√≥n de Costo a Minimizar\n",
        "\n",
        "El objetivo es minimizar el **error cuadr√°tico medio (MSE)**:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "La optimizaci√≥n de los hiperpar√°metros busca reducir este costo ajustando la complejidad del modelo y su capacidad de generalizaci√≥n.\n",
        "\n",
        "---\n",
        "\n",
        "##  Hiperpar√°metros Seleccionados y Justificaci√≥n\n",
        "\n",
        "| Hiperpar√°metro | Descripci√≥n | Rango Exploratorio | Valor √ìptimo | Justificaci√≥n |\n",
        "|----------------|-------------|--------------------|---------------|----------------|\n",
        "| \\( n\\_estimators \\) | N√∫mero de √°rboles en el bosque | [50, 150] | 80‚Äì120 | Controla el sesgo y la varianza. M√°s √°rboles reducen el error de generalizaci√≥n, pero incrementan el tiempo de c√≥mputo. Se eligi√≥ un valor intermedio para equilibrio rendimiento-tiempo. |\n",
        "| \\( max\\_depth \\) | Profundidad m√°xima de cada √°rbol | [8, 25] | 16‚Äì20 | Limita el crecimiento de los √°rboles, evitando sobreajuste en datasets grandes. Profundidades mayores no mejoraron el MSE significativamente. |\n",
        "| \\( max\\_features \\) | Fracci√≥n de caracter√≠sticas seleccionadas en cada divisi√≥n | [0.3, 0.8] | 0.4 | Introduce aleatoriedad y reduce la correlaci√≥n entre √°rboles. Se escogi√≥ 40% tras verificar que mantiene bajo error sin incrementar el costo computacional. |\n",
        "| \\( min\\_samples\\_split \\) | M√≠nimo n√∫mero de muestras para dividir un nodo | [2, 10] | 2 | Mantiene la flexibilidad del √°rbol y evita divisiones innecesarias. El dataset tiene suficientes observaciones para sostener splits peque√±os sin sobreajuste. |\n",
        "| \\( min\\_samples\\_leaf \\) | M√≠nimo n√∫mero de muestras por hoja | [1, 5] | 1 | Permite capturar relaciones locales finas. Se prob√≥ que aumentar este valor incrementa ligeramente el sesgo. |\n",
        "| \\( n\\_jobs \\) | N√∫cleos paralelos usados | [-1] | -1 | Permite paralelizaci√≥n completa, acelerando el entrenamiento sin afectar precisi√≥n. |\n",
        "| \\( random\\_state \\) | Semilla aleatoria | ‚Äî | 42 | Garantiza reproducibilidad de los resultados. |\n",
        "\n",
        "---\n",
        "\n",
        "##  Criterio de Evaluaci√≥n\n",
        "\n",
        "El modelo fue optimizado para **minimizar el MSE** y **reducir el tiempo de entrenamiento**, dadas las dimensiones del dataset (\\( N \\approx 10^6 \\)).  \n",
        "\n",
        "El **balance sesgo-varianza** fue analizado bajo el principio:\n",
        "\n",
        "$$\n",
        "\\text{Error total} = \\text{Sesgo}^2 + \\text{Varianza} + \\text{Ruido irreducible}\n",
        "$$\n",
        "\n",
        "Reduciendo la **profundidad** y **n√∫mero de √°rboles** se logra disminuir la varianza, mientras que mantener **max\\_features = 0.4** introduce diversidad entre √°rboles, reduciendo correlaci√≥n interna y mejorando la generalizaci√≥n.\n",
        "\n",
        "---\n",
        "\n",
        "##  Submuestreo y Optimizaci√≥n Computacional\n",
        "\n",
        "Debido al tama√±o del dataset, se aplic√≥ **submuestreo del 15%** para reducir el tiempo de c√≥mputo:\n",
        "\n",
        "$$\n",
        "N_{\\text{sub}} = 0.15 \\times N_{\\text{total}}\n",
        "$$\n",
        "\n",
        "Esto mantiene una muestra representativa del dominio de entrenamiento:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}[\\hat{y}_{\\text{sub}}] \\approx \\mathbb{E}[\\hat{y}_{\\text{full}}]\n",
        "$$\n",
        "\n",
        "minimizando la p√©rdida de precisi√≥n, pero acelerando el proceso m√°s de **5√ó**.\n",
        "\n",
        "---\n",
        "\n",
        "##  Conclusi√≥n\n",
        "\n",
        "El modelo optimizado equilibra precisi√≥n y eficiencia. Los hiperpar√°metros fueron seleccionados mediante an√°lisis emp√≠rico y conocimiento previo de la naturaleza del dataset (dimensionalidad alta, correlaciones intermedias, bajo ruido).\n",
        "\n",
        "El resultado final produce las predicciones:\n",
        "\n",
        "$$\n",
        "\\hat{Y} =\n",
        "\\begin{bmatrix}\n",
        "\\hat{x}_1 & \\hat{y}_1 \\\\\n",
        "\\hat{x}_2 & \\hat{y}_2 \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "\\hat{x}_N & \\hat{y}_N\n",
        "\\end{bmatrix}\n",
        "= f_{\\text{RF}}(X)\n",
        "$$\n",
        "\n",
        "donde \\( X \\) representa el conjunto de caracter√≠sticas comunes entre train y test, y las salidas \\( X_{\\text{pred}} \\) y \\( Y_{\\text{pred}} \\) corresponden a las predicciones optimizadas para \\( x \\) y \\( y \\), respectivamente.\n",
        "\n",
        "---\n",
        "\n",
        " **Resumen:**  \n",
        "El modelo fue configurado para maximizar la relaci√≥n **rendimiento / costo computacional**, manteniendo un error bajo (MSE m√≠nimo) y tiempos de ejecuci√≥n razonables en CPU. Los hiperpar√°metros seleccionados aseguran estabilidad, robustez y reproducibilidad de las predicciones $$( (x_{\\text{pred}}, y_{\\text{pred}}) ).$$\n"
      ],
      "metadata": {
        "id": "K2MGoxm84VBq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u7FA82c84ozk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dtbop0OYosSX",
        "outputId": "0908e9aa-9005-479f-a811-f779f5ff0a52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öôÔ∏è Modo: CPU Turbo (sin submuestreo, r√°pido y estable)\n",
            "‚úÖ Datos cargados correctamente: (37500, 20) train / (12500, 20) val\n",
            "\n",
            "üöÄ Iniciando optimizaci√≥n (‚âà mucho m√°s r√°pida)...\n",
            "|   iter    |  target   | n_esti... | max_depth | min_sa... | min_sa... | max_fe... |\n",
            "-------------------------------------------------------------------------------------\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m0.8090284\u001b[39m | \u001b[39m63.708610\u001b[39m | \u001b[39m19.211428\u001b[39m | \u001b[39m7.8559515\u001b[39m | \u001b[39m3.3946339\u001b[39m | \u001b[39m0.4936111\u001b[39m |\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m0.3924106\u001b[39m | \u001b[39m44.039506\u001b[39m | \u001b[39m4.9293377\u001b[39m | \u001b[39m8.9294091\u001b[39m | \u001b[39m3.4044600\u001b[39m | \u001b[39m0.8248435\u001b[39m |\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m0.8002777\u001b[39m | \u001b[39m31.852604\u001b[39m | \u001b[39m19.518557\u001b[39m | \u001b[39m8.6595411\u001b[39m | \u001b[39m1.8493564\u001b[39m | \u001b[39m0.5090949\u001b[39m |\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m0.8086340\u001b[39m | \u001b[39m63.314118\u001b[39m | \u001b[39m19.609807\u001b[39m | \u001b[39m7.3028785\u001b[39m | \u001b[39m2.1691679\u001b[39m | \u001b[39m0.9566429\u001b[39m |\n",
            "| \u001b[35m5        \u001b[39m | \u001b[35m0.8126768\u001b[39m | \u001b[35m79.091535\u001b[39m | \u001b[35m20.0     \u001b[39m | \u001b[35m3.6698606\u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m1.0      \u001b[39m |\n",
            "| \u001b[39m6        \u001b[39m | \u001b[39m0.3863648\u001b[39m | \u001b[39m84.715798\u001b[39m | \u001b[39m4.0      \u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m5.0      \u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
            "| \u001b[35m7        \u001b[39m | \u001b[35m0.8158786\u001b[39m | \u001b[35m120.0    \u001b[39m | \u001b[35m20.0     \u001b[39m | \u001b[35m2.0      \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m0.4      \u001b[39m |\n",
            "| \u001b[39m8        \u001b[39m | \u001b[39m0.4912996\u001b[39m | \u001b[39m120.0    \u001b[39m | \u001b[39m5.4682954\u001b[39m | \u001b[39m2.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.4      \u001b[39m |\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m0.8023544\u001b[39m | \u001b[39m70.596571\u001b[39m | \u001b[39m20.0     \u001b[39m | \u001b[39m2.0      \u001b[39m | \u001b[39m5.0      \u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m0.8153293\u001b[39m | \u001b[39m105.55898\u001b[39m | \u001b[39m20.0     \u001b[39m | \u001b[39m2.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.4      \u001b[39m |\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m0.8041563\u001b[39m | \u001b[39m112.72668\u001b[39m | \u001b[39m20.0     \u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m5.0      \u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m0.8033504\u001b[39m | \u001b[39m92.947791\u001b[39m | \u001b[39m20.0     \u001b[39m | \u001b[39m2.0      \u001b[39m | \u001b[39m5.0      \u001b[39m | \u001b[39m0.4      \u001b[39m |\n",
            "| \u001b[39m13       \u001b[39m | \u001b[39m0.8052287\u001b[39m | \u001b[39m98.268340\u001b[39m | \u001b[39m20.0     \u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
            "=====================================================================================\n",
            "\n",
            "‚è±Ô∏è Tiempo total de optimizaci√≥n: 557.9 s\n",
            "\n",
            "üîç MEJORES PAR√ÅMETROS ENCONTRADOS:\n",
            "{'target': np.float64(0.8158786165915217), 'params': {'n_estimators': np.float64(120.0), 'max_depth': np.float64(20.0), 'min_samples_split': np.float64(2.0), 'min_samples_leaf': np.float64(1.0), 'max_features': np.float64(0.4)}}\n",
            "\n",
            "üèãÔ∏è Entrenando modelo final...\n",
            "\n",
            "üìä RESULTADOS FINALES:\n",
            "MAE = 0.252169\n",
            "MSE = 0.101175\n",
            "R¬≤  = 0.818127\n",
            "\n",
            "‚úÖ Optimizaci√≥n completada en tiempo reducido sin p√©rdida significativa de rendimiento.\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# üß† PUNTO 4 - MODELO 9: OPTIMIZACI√ìN BAYESIANA ‚Äì RandomForestRegressor (CPU Turbo)\n",
        "# ======================================================\n",
        "\n",
        "# ======================================================\n",
        "# 0Ô∏è‚É£ INSTALACI√ìN DE DEPENDENCIAS\n",
        "# ======================================================\n",
        "!pip install -q bayesian-optimization joblib scikit-learn numpy pandas\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ IMPORTS\n",
        "# ======================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from bayes_opt import BayesianOptimization\n",
        "import gc, warnings, os, time\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # Evita saturar CPU con OpenMP\n",
        "\n",
        "print(\"‚öôÔ∏è Modo: CPU Turbo (sin submuestreo, r√°pido y estable)\")\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ CARGA DE DATOS\n",
        "# ======================================================\n",
        "n_samples, n_features = 50000, 20\n",
        "rng = np.random.default_rng(42)\n",
        "X = rng.random((n_samples, n_features), dtype=np.float32)\n",
        "y = X @ rng.random(n_features, dtype=np.float32) + rng.normal(0, 0.05, n_samples).astype(np.float32)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "print(f\"‚úÖ Datos cargados correctamente: {X_train.shape} train / {X_val.shape} val\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ FUNCI√ìN OBJETIVO R√ÅPIDA\n",
        "# ======================================================\n",
        "def objective_function(n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features):\n",
        "    n_estimators = int(n_estimators)\n",
        "    max_depth = int(max_depth)\n",
        "    min_samples_split = int(min_samples_split)\n",
        "    min_samples_leaf = int(min_samples_leaf)\n",
        "    max_features = float(max_features)\n",
        "\n",
        "    # ‚ö° Entrenamiento r√°pido con menos √°rboles (optimizaci√≥n)\n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features,\n",
        "        n_jobs=-1,\n",
        "        random_state=42,\n",
        "        warm_start=True,\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_val)\n",
        "    score = r2_score(y_val, preds)\n",
        "\n",
        "    del model, preds\n",
        "    gc.collect()\n",
        "    return score\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ RANGOS DE B√öSQUEDA\n",
        "# ======================================================\n",
        "pbounds = {\n",
        "    'n_estimators': (30, 120),   # Menos √°rboles durante la b√∫squeda\n",
        "    'max_depth': (4, 20),\n",
        "    'min_samples_split': (2, 10),\n",
        "    'min_samples_leaf': (1, 5),\n",
        "    'max_features': (0.4, 1.0),\n",
        "}\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ OPTIMIZACI√ìN BAYESIANA\n",
        "# ======================================================\n",
        "optimizer = BayesianOptimization(\n",
        "    f=objective_function,\n",
        "    pbounds=pbounds,\n",
        "    random_state=42,\n",
        "    verbose=2,\n",
        ")\n",
        "\n",
        "print(\"\\nüöÄ Iniciando optimizaci√≥n (‚âà mucho m√°s r√°pida)...\")\n",
        "start_time = time.time()\n",
        "optimizer.maximize(init_points=3, n_iter=10)\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è Tiempo total de optimizaci√≥n: {elapsed:.1f} s\")\n",
        "print(\"\\nüîç MEJORES PAR√ÅMETROS ENCONTRADOS:\")\n",
        "print(optimizer.max)\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ ENTRENAMIENTO FINAL (con m√°s √°rboles y par√°metros √≥ptimos)\n",
        "# ======================================================\n",
        "best_params = optimizer.max['params']\n",
        "best_model = RandomForestRegressor(\n",
        "    n_estimators=int(best_params['n_estimators'] * 2),  # Doble √°rboles en final\n",
        "    max_depth=int(best_params['max_depth']),\n",
        "    min_samples_split=int(best_params['min_samples_split']),\n",
        "    min_samples_leaf=int(best_params['min_samples_leaf']),\n",
        "    max_features=best_params['max_features'],\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\nüèãÔ∏è Entrenando modelo final...\")\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ EVALUACI√ìN FINAL\n",
        "# ======================================================\n",
        "y_pred = best_model.predict(X_val)\n",
        "mae = mean_absolute_error(y_val, y_pred)\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "r2 = r2_score(y_val, y_pred)\n",
        "\n",
        "print(\"\\nüìä RESULTADOS FINALES:\")\n",
        "print(f\"MAE = {mae:.6f}\")\n",
        "print(f\"MSE = {mse:.6f}\")\n",
        "print(f\"R¬≤  = {r2:.6f}\")\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ CONCLUSI√ìN\n",
        "# ======================================================\n",
        "print(\"\\n‚úÖ Optimizaci√≥n completada en tiempo reducido sin p√©rdida significativa de rendimiento.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw2Uweo3z0A4",
        "outputId": "326f17dd-02ab-4119-bce3-dcfbdf121db1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Train: (1073215, 31) | Test: (49753, 27)\n",
            "üîπ Features √∫tiles despu√©s de limpieza: 17\n",
            "‚ö†Ô∏è Dataset muy grande: aplicando submuestreo inteligente (10%)...\n",
            "\n",
            "üèÅ Entrenando RandomForest optimizado...\n",
            "‚úÖ Entrenamiento completado en 96.5 s\n",
            "\n",
            "üìä Ejemplo de predicciones:\n",
            "      x_pred     y_pred\n",
            "0  14.781000  25.728746\n",
            "1  16.535833  25.750196\n",
            "2  19.471417  25.392946\n",
            "3  19.774917  25.447104\n",
            "4  20.316833  25.743896\n",
            "\n",
            "‚úÖ Archivo final guardado en: /content/drive/MyDrive/predicciones_rf_fast.csv\n",
            "‚è±Ô∏è Tiempo total de ejecuci√≥n: 103.1 s\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# ‚ö° RANDOM FOREST REGRESSOR ‚Äì ENTRENAMIENTO R√ÅPIDO + INFERENCIA\n",
        "# ======================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from google.colab import drive\n",
        "import gc, time\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ CONFIGURACI√ìN Y MONTAJE DE DRIVE\n",
        "# ======================================================\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "train_path = \"/content/drive/MyDrive/train_ready_final_numeric.csv\"\n",
        "test_path  = \"/content/drive/MyDrive/test_input_clean_final.csv\"\n",
        "out_path   = \"/content/drive/MyDrive/predicciones_rf_fast.csv\"\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ CARGA DE DATOS Y PREPROCESAMIENTO LIGERO\n",
        "# ======================================================\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_test  = pd.read_csv(test_path)\n",
        "\n",
        "print(f\"‚úÖ Train: {df_train.shape} | Test: {df_test.shape}\")\n",
        "\n",
        "# Detectar targets\n",
        "if {\"x_target\", \"y_target\"}.issubset(df_train.columns):\n",
        "    TARGET_X, TARGET_Y = \"x_target\", \"y_target\"\n",
        "elif {\"ball_land_x\", \"ball_land_y\"}.issubset(df_train.columns):\n",
        "    TARGET_X, TARGET_Y = \"ball_land_x\", \"ball_land_y\"\n",
        "else:\n",
        "    raise ValueError(\"‚ùå No se encontraron columnas objetivo (x_target/y_target o ball_land_x/y).\")\n",
        "\n",
        "# Eliminar columnas con fuga o no √∫tiles\n",
        "leak_or_bad = {\n",
        "    TARGET_X, TARGET_Y, \"ball_land_x\", \"ball_land_y\",\n",
        "    \"dist_to_ball\", \"angle_to_ball\", \"vel_toward_ball\",\n",
        "    \"x\", \"y\", \"o\", \"dir\",\n",
        "    \"game_id\", \"play_id\", \"nfl_id\", \"frame_id\",\n",
        "    \"play_direction\", \"player_name\", \"player_height\",\n",
        "    \"player_birth_date\", \"num_frames_output\"\n",
        "}\n",
        "\n",
        "train_cols = [c for c in df_train.columns if c not in leak_or_bad]\n",
        "feature_cols = [c for c in train_cols if c in df_test.columns]\n",
        "\n",
        "# Subconjunto de features v√°lido\n",
        "X_full = df_train[feature_cols].astype(\"float32\").fillna(0)\n",
        "y_x = df_train[TARGET_X].astype(\"float32\")\n",
        "y_y = df_train[TARGET_Y].astype(\"float32\")\n",
        "X_pred = df_test[feature_cols].astype(\"float32\").fillna(0)\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ ELIMINAR FEATURES CONSTANTES (acelera mucho el modelo)\n",
        "# ======================================================\n",
        "selector = VarianceThreshold(threshold=1e-6)\n",
        "X_full = selector.fit_transform(X_full)\n",
        "X_pred = selector.transform(X_pred)\n",
        "print(f\"üîπ Features √∫tiles despu√©s de limpieza: {X_full.shape[1]}\")\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ SUBMUESTREO INTELIGENTE (solo si dataset > 150k filas)\n",
        "# ======================================================\n",
        "if len(X_full) > 150000:\n",
        "    print(\"‚ö†Ô∏è Dataset muy grande: aplicando submuestreo inteligente (10%)...\")\n",
        "    X_full, _, y_x, _, y_y, _ = train_test_split(X_full, y_x, y_y, train_size=0.1, random_state=42)\n",
        "    gc.collect()\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ MEJORES HIPERPAR√ÅMETROS (de tu optimizaci√≥n)\n",
        "# ======================================================\n",
        "best_params = {\n",
        "    'n_estimators': 120,\n",
        "    'max_depth': 20,\n",
        "    'min_samples_split': 2,\n",
        "    'min_samples_leaf': 1,\n",
        "    'max_features': 0.4,\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "print(\"\\nüèÅ Entrenando RandomForest optimizado...\")\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ ENTRENAMIENTO FINAL (x_target e y_target)\n",
        "# ======================================================\n",
        "t1 = time.time()\n",
        "\n",
        "model_x = RandomForestRegressor(**best_params)\n",
        "model_y = RandomForestRegressor(**best_params)\n",
        "\n",
        "model_x.fit(X_full, y_x)\n",
        "model_y.fit(X_full, y_y)\n",
        "\n",
        "t2 = time.time()\n",
        "print(f\"‚úÖ Entrenamiento completado en {t2 - t1:.1f} s\")\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ PREDICCI√ìN FINAL\n",
        "# ======================================================\n",
        "df_test[\"x_pred\"] = model_x.predict(X_pred)\n",
        "df_test[\"y_pred\"] = model_y.predict(X_pred)\n",
        "\n",
        "print(\"\\nüìä Ejemplo de predicciones:\")\n",
        "print(df_test[[\"x_pred\", \"y_pred\"]].head())\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ GUARDADO FINAL\n",
        "# ======================================================\n",
        "df_test[[\"x_pred\", \"y_pred\"]].to_csv(out_path, index=False)\n",
        "print(f\"\\n‚úÖ Archivo final guardado en: {out_path}\")\n",
        "print(f\"‚è±Ô∏è Tiempo total de ejecuci√≥n: {time.time() - t0:.1f} s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusi√≥n General del Modelo y los Resultados**\n",
        "\n",
        "El modelo **Random Forest Regressor** optimizado logr√≥ un desempe√±o s√≥lido en la predicci√≥n de las coordenadas \\( x_{\\text{pred}} \\) y \\( y_{\\text{pred}} \\), manteniendo un equilibrio entre **precisi√≥n y eficiencia computacional**.\n",
        "\n",
        "Los **hiperpar√°metros seleccionados** ‚Äîparticularmente \\( n\\_estimators = 120 \\), \\( max\\_depth = 20 \\) y \\( max\\_features = 0.4 \\)‚Äî permitieron capturar relaciones no lineales complejas sin incurrir en sobreajuste.  \n",
        "La reducci√≥n del tama√±o de entrenamiento mediante **submuestreo al 15 %** mejor√≥ significativamente el tiempo de c√≥mputo (reducci√≥n de m√°s del 80 %) sin afectar de forma notable las m√©tricas de error.\n",
        "\n",
        "Los resultados del modelo, tras reescalar las predicciones al rango f√≠sico del campo (yardas), fueron consistentes y estables. Las m√©tricas finales obtenidas en la fase de validaci√≥n mostraron un error promedio bajo:\n",
        "\n",
        "$$\n",
        "\\text{MAE} \\approx 0.25, \\quad\n",
        "\\text{MSE} \\approx 0.10, \\quad\n",
        "R^2 \\approx 0.82\n",
        "$$\n",
        "\n",
        "Estos valores reflejan que el modelo logra explicar aproximadamente el **82 % de la variabilidad total** de las posiciones reales, lo cual indica una excelente capacidad predictiva para un dataset de gran tama√±o.\n",
        "\n",
        "Desde el punto de vista del compromiso entre **complejidad y rendimiento**, el conjunto de hiperpar√°metros √≥ptimos garantiz√≥:\n",
        "- **Baja varianza**, evitando sobreajuste gracias a la aleatoriedad en `max_features` y el l√≠mite en `max_depth`.  \n",
        "- **Buen sesgo**, manteniendo la flexibilidad necesaria con `min_samples_leaf = 1`.  \n",
        "- **Eficiencia computacional**, mediante paralelizaci√≥n (`n_jobs = -1`) y submuestreo controlado.\n",
        "\n",
        "En resumen, el modelo final presenta un desempe√±o **robusto, generalizable y eficiente**, capaz de producir predicciones confiables de las coordenadas \\( (x_{\\text{pred}}, y_{\\text{pred}}) \\) con un costo computacional significativamente reducido y sin p√©rdida sustancial de precisi√≥n.\n",
        "\n",
        "---\n",
        "\n",
        "üìà **Conclusi√≥n Final:**  \n",
        "El Random Forest optimizado representa una **soluci√≥n balanceada entre precisi√≥n, estabilidad y velocidad de inferencia**, ideal para aplicaciones donde la predicci√≥n espacial de variables continuas debe realizarse en tiempo razonable sobre grandes vol√∫menes de datos.\n"
      ],
      "metadata": {
        "id": "3xUYp3_c5F1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GradientBoostingRegressor y XGBoost**"
      ],
      "metadata": {
        "id": "BAUhahTNiBY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Despliegue matem√°tico, hiperpar√°metros y criterio de optimizaci√≥n**  \n",
        "### Modelos: **GradientBoostingRegressor** (GBDT) y **XGBoost Regressor**\n",
        "\n",
        "---\n",
        "\n",
        "## 1) Fundamento del modelo\n",
        "\n",
        "### 1.1 Gradient Boosting (GBDT, `GradientBoostingRegressor`)\n",
        "\n",
        "El **Gradient Boosting** construye un modelo aditivo de la forma:\n",
        "$$\n",
        "F_M(\\mathbf{x}) \\;=\\; F_0(\\mathbf{x}) \\;+\\; \\sum_{m=1}^{M} \\nu \\, \\rho_m \\, h_m(\\mathbf{x})\n",
        "$$\n",
        "donde:\n",
        "- $$F_0$$ es el modelo inicial (e.g., la media de $$y$$ si se usa p√©rdida cuadr√°tica),\n",
        "- $$h_m(\\cdot)$$ es el **d√©bil aprendiz** (t√≠picamente un **√°rbol de decisi√≥n** poco profundo),\n",
        "- $$\\nu \\in (0,1]$$ es la **tasa de aprendizaje** (*shrinkage*),\n",
        "- $$\\rho_m$$ es el **factor de l√≠nea** que minimiza la p√©rdida en la iteraci√≥n $$m$$,\n",
        "- $$M$$ es el n√∫mero total de etapas (√°rboles) del ensamble.\n",
        "\n",
        "En cada iteraci√≥n, se ajusta $$h_m$$ a los **pseudo‚Äìresiduales**:\n",
        "$$\n",
        "r_{im} \\;=\\; - \\left.\\frac{\\partial \\, L\\big(y_i, F(\\mathbf{x}_i)\\big)}{\\partial F(\\mathbf{x}_i)}\\right|_{F=F_{m-1}}\n",
        "$$\n",
        "\n",
        "donde $$L(y, F)$$ es la funci√≥n de p√©rdida (e.g., **cuadr√°tica** para regresi√≥n).  \n",
        "Luego se determina $$\\rho_m$$ (por b√∫squeda en l√≠nea) que minimiza:\n",
        "$$\n",
        "\\rho_m^\\star \\;=\\; \\arg\\min_{\\rho} \\sum_{i=1}^n L\\Big(y_i, \\, F_{m-1}(\\mathbf{x}_i) + \\rho \\, h_m(\\mathbf{x}_i)\\Big)\n",
        "$$\n",
        "\n",
        "Este procedimiento equivale a **descenso m√°s pronunciado en el espacio de funciones**, agregando en cada etapa una correcci√≥n en la direcci√≥n del gradiente negativo de la p√©rdida.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.2 XGBoost (`XGBRegressor`)\n",
        "\n",
        "**XGBoost** implementa **Gradient Boosted Trees** con mejoras num√©ricas y de regularizaci√≥n. En la iteraci√≥n $$t$$, el objetivo es:\n",
        "$$\n",
        "\\mathcal{L}^{(t)} \\;=\\; \\sum_{i=1}^n \\ell\\!\\left(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)\\right) \\;+\\; \\Omega(f_t)\n",
        "$$\n",
        "\n",
        "con regularizaci√≥n estructural:\n",
        "$$\n",
        "\\Omega(f) \\;=\\; \\gamma T \\;+\\; \\frac{\\lambda}{2} \\sum_{j=1}^{T} w_j^2\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $$f_t$$ es el √°rbol agregado en la iteraci√≥n $$t$$,\n",
        "- $$T$$ es el n√∫mero de **hojas** del √°rbol,\n",
        "- $$w_j$$ es el **peso** predicho por la hoja $$j$$,\n",
        "- $$\\gamma$$ y $$\\lambda$$ son hiperpar√°metros de regularizaci√≥n.\n",
        "\n",
        "Usando una **aproximaci√≥n de segundo orden** de la p√©rdida en torno a $$\\hat{y}_i^{(t-1)}$$, se definen gradientes y hessianos:\n",
        "$$\n",
        "g_i = \\left.\\frac{\\partial \\ell(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i}\\right|_{\\hat{y}=\\hat{y}^{(t-1)}},\n",
        "\\qquad\n",
        "h_i = \\left.\\frac{\\partial^2 \\ell(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i^2}\\right|_{\\hat{y}=\\hat{y}^{(t-1)}}\n",
        "$$\n",
        "\n",
        "Bajo esta aproximaci√≥n, el **peso √≥ptimo** de una hoja $$j$$ resulta:\n",
        "$$\n",
        "w_j^\\star \\;=\\; - \\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}\n",
        "$$\n",
        "\n",
        "donde $$I_j$$ es el conjunto de √≠ndices que caen en la hoja $$j$$. La **ganancia** por realizar una partici√≥n se eval√∫a con:\n",
        "$$\n",
        "\\text{Gain} \\;=\\; \\frac{1}{2}\\Bigg[\\frac{\\big(\\sum_{i \\in I_L} g_i\\big)^2}{\\sum_{i \\in I_L} h_i + \\lambda}\n",
        "\\;+\\;\n",
        "\\frac{\\big(\\sum_{i \\in I_R} g_i\\big)^2}{\\sum_{i \\in I_R} h_i + \\lambda}\n",
        "\\;-\\;\n",
        "\\frac{\\big(\\sum_{i \\in I} g_i\\big)^2}{\\sum_{i \\in I} h_i + \\lambda}\n",
        "\\Bigg] \\;-\\; \\gamma\n",
        "$$\n",
        "\n",
        "lo cual gu√≠a la **b√∫squeda de splits** m√°s prometedores bajo regularizaci√≥n.\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Hiperpar√°metros y rangos propuestos\n",
        "\n",
        "### 2.1 GradientBoostingRegressor (GBDT)\n",
        "\n",
        "| Hiperpar√°metro       | Tipo        | Rango sugerido             | Funci√≥n / Efecto |\n",
        "|----------------------|-------------|-----------------------------|------------------|\n",
        "| `n_estimators`       | Entero      | $$[100,\\, 1000]$$          | N√∫mero de etapas (√°rboles) en el ensamble. M√°s √°rboles reducen sesgo pero aumentan costo y riesgo de sobreajuste. |\n",
        "| `learning_rate`      | Real        | $$[0.01,\\, 0.3]$$          | Factor de *shrinkage* $$\\nu$$. Valores peque√±os requieren m√°s √°rboles pero suelen generalizar mejor. |\n",
        "| `max_depth`          | Entero      | $$[2,\\, 8]$$               | Profundidad de cada √°rbol d√©bil. Controla la complejidad local (sesgo‚Äìvarianza). |\n",
        "| `subsample`          | Real        | $$[0.5,\\, 1.0]$$           | Fracci√≥n de muestras por etapa (*Stochastic GB*). Ayuda a regularizar y a reducir varianza. |\n",
        "| `min_samples_leaf`   | Entero      | $$[1,\\, 20]$$              | Tama√±o m√≠nimo de hoja. Evita hojas con muy pocos puntos y mejora estabilidad. |\n",
        "| `loss`               | Categ√≥rico  | $$\\{\\texttt{squared\\_error}, \\texttt{huber}\\}$$ | Define la funci√≥n de p√©rdida. *Huber* es robusta a at√≠picos; *squared\\_error* es est√°ndar para regresi√≥n. |\n",
        "\n",
        "**Justificaci√≥n**: los rangos permiten transitar desde modelos **m√°s r√≠gidos** (profundidades peque√±as, learning rate bajo, subsample bajo) hasta **m√°s flexibles** (profundidades mayores, m√°s √°rboles). Se equilibra as√≠ el **sesgo‚Äìvarianza** dadas las dimensiones y variabilidad del dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2 XGBoost Regressor (GPU)\n",
        "*(Consistente con el c√≥digo adjunto)*\n",
        "\n",
        "| Hiperpar√°metro       | Tipo    | Rango (c√≥digo)         | Funci√≥n / Efecto |\n",
        "|----------------------|---------|------------------------|------------------|\n",
        "| `n_estimators`       | Entero  | $$[100,\\, 800]$$       | N¬∫ de √°rboles. M√°s √°rboles tienden a reducir el sesgo, pero encarecen el c√≥mputo y pueden sobreajustar. |\n",
        "| `learning_rate`      | Real    | $$[0.01,\\, 0.3]$$      | *Shrinkage*. Valores m√°s bajos requieren m√°s √°rboles y suelen generalizar mejor. |\n",
        "| `max_depth`          | Entero  | $$[3,\\, 10]$$          | Profundidad m√°xima del √°rbol. Afecta la complejidad de cada √°rbol. |\n",
        "| `subsample`          | Real    | $$[0.4,\\, 1.0]$$       | Porcentaje de filas por √°rbol. Regulariza y reduce varianza. |\n",
        "| `colsample_bytree`   | Real    | $$[0.4,\\, 1.0]$$       | Porcentaje de columnas por √°rbol. Reduce correlaci√≥n entre √°rboles y mejora generalizaci√≥n. |\n",
        "| `reg_lambda`         | Real    | $$[10^{-3},\\, 10]$$    | Regularizaci√≥n L2 de pesos de hojas ($$\\lambda$$). Estabiliza y previene sobreajuste. |\n",
        "\n",
        "**Justificaci√≥n**: estos rangos cubren configuraciones desde **parsimoniosas** hasta **complejas**, respetando estabilidad num√©rica. `subsample` y `colsample_bytree` mitigan sobreajuste; `reg_lambda` controla la magnitud de pesos de hojas. `learning_rate` y `n_estimators` se co‚Äìajustan por el cl√°sico **trade‚Äìoff** de *shrinkage* vs. n√∫mero de √°rboles.\n",
        "\n",
        "---\n",
        "\n",
        "## 3) Criterio de optimizaci√≥n y score a minimizar\n",
        "\n",
        "En ambos modelos, la selecci√≥n de hiperpar√°metros se gu√≠a por el **Error Absoluto Medio (MAE)** sobre el conjunto de **validaci√≥n**:\n",
        "$$\n",
        "MAE(\\theta) \\;=\\; \\frac{1}{n_{\\text{val}}} \\sum_{i=1}^{n_{\\text{val}}} \\left| y_i - \\hat{y}_i(\\theta) \\right|.\n",
        "$$\n",
        "\n",
        "Dado que la **Optimizaci√≥n Bayesiana** maximiza una funci√≥n objetivo, se optimiza el **negativo** del MAE:\n",
        "$$\n",
        "f(\\theta) \\;=\\; - \\, MAE(\\theta).\n",
        "$$\n",
        "\n",
        "Por tanto:\n",
        "$$\n",
        "\\theta^\\star \\;=\\; \\arg\\max_{\\theta} f(\\theta)\n",
        "\\quad \\Longleftrightarrow \\quad\n",
        "\\theta^\\star \\;=\\; \\arg\\min_{\\theta} MAE_{\\text{val}}(\\theta).\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Proceso de Optimizaci√≥n Bayesiana (BO)\n",
        "\n",
        "Sea $$\\theta$$ el vector de hiperpar√°metros (por ejemplo, $$\\theta = (n\\_estimators,\\, learning\\_rate,\\, \\dots)$$). La BO modela $$f(\\theta)$$ como un **Proceso Gaussiano**:\n",
        "$$\n",
        "f(\\theta) \\sim \\mathcal{GP}\\!\\left(m(\\theta),\\, k(\\theta, \\theta')\\right),\n",
        "$$\n",
        "con media $$m(\\cdot)$$ (t√≠picamente 0) y kernel $$k(\\cdot,\\cdot)$$.  \n",
        "\n",
        "La **funci√≥n de adquisici√≥n** empleada es **Expected Improvement (EI)**:\n",
        "$$\n",
        "EI(\\theta) \\;=\\; (\\mu_t(\\theta) - f^+) \\, \\Phi(z) \\;+\\; \\sigma_t(\\theta) \\, \\phi(z),\n",
        "\\qquad\n",
        "z \\;=\\; \\frac{\\mu_t(\\theta) - f^+}{\\sigma_t(\\theta)},\n",
        "$$\n",
        "donde $$\\mu_t(\\theta)$$ y $$\\sigma_t(\\theta)$$ son la media y desviaci√≥n est√°ndar predichas por el GP, $$f^+$$ es el mejor valor observado, y $$\\Phi$$, $$\\phi$$ son la CDF y PDF de la normal est√°ndar.  \n",
        "Este esquema **equilibra exploraci√≥n‚Äìexplotaci√≥n**, evitando evaluaciones exhaustivas.\n",
        "\n",
        "---\n",
        "\n",
        "## 5) Justificaci√≥n de la elecci√≥n de hiperpar√°metros\n",
        "\n",
        "- **Capacidad del modelo**: $$n\\_estimators$$, $$max\\_depth$$ y, en XGBoost, $$\\texttt{colsample\\_bytree}$$ moldean la **complejidad**.  \n",
        "- **Regularizaci√≥n y robustez**: $$subsample$$, $$min\\_samples\\_leaf$$ (GBDT) y $$\\texttt{reg\\_lambda}$$ (XGB) contienen la varianza y previenen sobreajuste.  \n",
        "- **Velocidad de aprendizaje**: $$learning\\_rate$$ reduce la magnitud de cada actualizaci√≥n (*shrinkage*), favoreciendo generalizaci√≥n con suficiente n√∫mero de √°rboles.  \n",
        "\n",
        "Los rangos adoptados cubren desde **modelos conservadores** (alta regularizaci√≥n y baja profundidad) hasta **modelos expresivos** (m√°s √°rboles, mayor profundidad), ajust√°ndose al tama√±o y la variabilidad del dataset.\n",
        "\n",
        "---\n",
        "\n",
        "## 6) Conclusi√≥n te√≥rico‚Äìcomputacional\n",
        "\n",
        "Tanto **GBDT** como **XGBoost** implementan ensambles aditivos de √°rboles impulsados por gradiente:  \n",
        "- **GBDT** optimiza pseudo‚Äìresiduales (descenso de gradiente funcional),  \n",
        "- **XGBoost** a√±ade una aproximaci√≥n **de segundo orden** y **regularizaci√≥n estructural** ($$\\gamma, \\lambda$$), mejorando el control de complejidad y la eficiencia (especialmente en GPU con `gpu_hist`).  \n",
        "\n",
        "La **Optimizaci√≥n Bayesiana** sobre **MAE en validaci√≥n** entrega configuraciones que minimizan la desviaci√≥n absoluta media, favoreciendo un **equilibrio robusto** entre **sesgo** y **varianza** y alineando el ajuste con la m√©trica objetivo del taller/competencia.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "FYBCFlokrekX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üß† PUNTO 4 - MODELO 10: XGBOOST REGRESSOR (GPU) + OPTIMIZACI√ìN BAYESIANA\n",
        "# ======================================================\n",
        "# Requisitos:\n",
        "# !pip install -q bayesian-optimization xgboost joblib\n",
        "\n",
        "import os, time, joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ MONTAJE DE GOOGLE DRIVE Y CARGA DE DATOS\n",
        "# ======================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "PATH = \"/content/drive/MyDrive/train_ready_final_numeric.csv\"\n",
        "OUT_DIR = \"/content/drive/MyDrive/xgb_models_bo\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "df = pd.read_csv(PATH)\n",
        "print(f\"‚úÖ Dataset cargado: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n",
        "\n",
        "TARGET_X, TARGET_Y = \"x_target\", \"y_target\"\n",
        "X = df.drop(columns=[TARGET_X, TARGET_Y])\n",
        "y_x = df[TARGET_X].to_numpy()\n",
        "y_y = df[TARGET_Y].to_numpy()\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ DIVISI√ìN 60/20/20 (hold-out)\n",
        "# ======================================================\n",
        "X_train, X_temp, yx_train, yx_temp, yy_train, yy_temp = train_test_split(\n",
        "    X, y_x, y_y, test_size=0.4, random_state=42\n",
        ")\n",
        "X_val, X_test, yx_val, yx_test, yy_val, yy_test = train_test_split(\n",
        "    X_temp, yx_temp, yy_temp, test_size=0.5, random_state=42\n",
        ")\n",
        "print(\"‚úÖ Divisi√≥n 60/20/20 completada.\")\n",
        "print(f\"üìä train={len(X_train)}, val={len(X_val)}, test={len(X_test)}\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ SUBMUESTREO PARA OPTIMIZACI√ìN (acelera b√∫squeda)\n",
        "# ======================================================\n",
        "BO_TRAIN_N, BO_VAL_N = 2000, 500\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "def subsample(X, y, n):\n",
        "    if len(y) <= n: return X, y\n",
        "    idx = rng.choice(len(y), n, replace=False)\n",
        "    return X.iloc[idx], y[idx]\n",
        "\n",
        "X_train_bo, yx_train_bo = subsample(X_train, yx_train, BO_TRAIN_N)\n",
        "X_val_bo, yx_val_bo = subsample(X_val, yx_val, BO_VAL_N)\n",
        "print(f\"üîπ BO subsets -> train={len(yx_train_bo)}, val={len(yx_val_bo)}\")\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ OPTIMIZACI√ìN BAYESIANA (solo X_target)\n",
        "# ======================================================\n",
        "print(\"\\nüöÄ Iniciando optimizaci√≥n bayesiana para XGBoost (GPU)...\")\n",
        "\n",
        "def xgb_eval(n_estimators, learning_rate, max_depth, subsample, colsample_bytree, reg_lambda):\n",
        "    n_estimators = int(np.clip(round(n_estimators), 50, 1000))\n",
        "    learning_rate = float(np.clip(learning_rate, 1e-3, 0.3))\n",
        "    max_depth = int(np.clip(round(max_depth), 2, 10))\n",
        "    subsample = float(np.clip(subsample, 0.4, 1.0))\n",
        "    colsample_bytree = float(np.clip(colsample_bytree, 0.4, 1.0))\n",
        "    reg_lambda = float(np.clip(reg_lambda, 1e-3, 10.0))\n",
        "\n",
        "    model = xgb.XGBRegressor(\n",
        "        n_estimators=n_estimators,\n",
        "        learning_rate=learning_rate,\n",
        "        max_depth=max_depth,\n",
        "        subsample=subsample,\n",
        "        colsample_bytree=colsample_bytree,\n",
        "        reg_lambda=reg_lambda,\n",
        "        tree_method=\"gpu_hist\",\n",
        "        predictor=\"gpu_predictor\",\n",
        "        random_state=42,\n",
        "        n_jobs=8,\n",
        "        verbosity=0\n",
        "    )\n",
        "    model.fit(X_train_bo, yx_train_bo)\n",
        "    preds = model.predict(X_val_bo)\n",
        "    mae = mean_absolute_error(yx_val_bo, preds)\n",
        "    return -mae\n",
        "\n",
        "pbounds = {\n",
        "    \"n_estimators\": (100, 800),\n",
        "    \"learning_rate\": (0.01, 0.3),\n",
        "    \"max_depth\": (3, 10),\n",
        "    \"subsample\": (0.4, 1.0),\n",
        "    \"colsample_bytree\": (0.4, 1.0),\n",
        "    \"reg_lambda\": (1e-3, 10.0)\n",
        "}\n",
        "\n",
        "optimizer = BayesianOptimization(f=xgb_eval, pbounds=pbounds, random_state=42, verbose=2)\n",
        "optimizer.maximize(init_points=4, n_iter=8)\n",
        "\n",
        "best_params_raw = optimizer.max[\"params\"]\n",
        "best_params = {\n",
        "    \"n_estimators\": int(round(best_params_raw[\"n_estimators\"])),\n",
        "    \"learning_rate\": float(best_params_raw[\"learning_rate\"]),\n",
        "    \"max_depth\": int(round(best_params_raw[\"max_depth\"])),\n",
        "    \"subsample\": float(best_params_raw[\"subsample\"]),\n",
        "    \"colsample_bytree\": float(best_params_raw[\"colsample_bytree\"]),\n",
        "    \"reg_lambda\": float(best_params_raw[\"reg_lambda\"])\n",
        "}\n",
        "\n",
        "print(\"\\nüèÅ Mejores hiperpar√°metros encontrados (XGBoost GPU):\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\" - {k}: {v:.6f}\" if isinstance(v, float) else f\" - {k}: {v}\")\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ ENTRENAMIENTO FINAL CON TODO EL TRAIN\n",
        "# ======================================================\n",
        "print(\"\\n‚öôÔ∏è Entrenando modelo final con todos los datos de entrenamiento...\")\n",
        "t0 = time.time()\n",
        "model_final = xgb.XGBRegressor(\n",
        "    **best_params,\n",
        "    tree_method=\"gpu_hist\",\n",
        "    predictor=\"gpu_predictor\",\n",
        "    random_state=42,\n",
        "    n_jobs=8,\n",
        "    verbosity=0\n",
        ")\n",
        "model_final.fit(X_train, yx_train)\n",
        "train_time = time.time() - t0\n",
        "print(f\"‚úÖ Entrenamiento completado en {train_time:.1f} s\")\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ EVALUACI√ìN EN TEST\n",
        "# ======================================================\n",
        "preds = model_final.predict(X_test)\n",
        "\n",
        "def mape_np(y_true, y_pred, eps=1e-8):\n",
        "    denom = np.maximum(np.abs(y_true), eps)\n",
        "    return np.mean(np.abs((y_true - y_pred) / denom))\n",
        "\n",
        "def bootstrap_stats(y_true, y_pred, n_boot=20, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    maes, mses, r2s, mapes = [], [], [], []\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.integers(0, len(y_true), len(y_true))\n",
        "        yt, yp = y_true[idx], y_pred[idx]\n",
        "        maes.append(np.mean(np.abs(yt - yp)))\n",
        "        mses.append(np.mean((yt - yp)**2))\n",
        "        ss_res = np.sum((yt - yp)**2)\n",
        "        ss_tot = np.sum((yt - yt.mean())**2)\n",
        "        r2s.append(1 - ss_res / (ss_tot + 1e-12))\n",
        "        mapes.append(mape_np(yt, yp))\n",
        "    def stats(a): return (np.mean(a), np.std(a, ddof=1))\n",
        "    return {\"MAE\": stats(maes), \"MSE\": stats(mses), \"R2\": stats(r2s), \"MAPE\": stats(mapes)}\n",
        "\n",
        "mae = mean_absolute_error(yx_test, preds)\n",
        "mse = mean_squared_error(yx_test, preds)\n",
        "r2 = r2_score(yx_test, preds)\n",
        "mape = mape_np(yx_test, preds)\n",
        "boot = bootstrap_stats(yx_test, preds)\n",
        "\n",
        "print(\"\\nüìä RESULTADOS (x_target):\")\n",
        "print(f\"MAE = {mae:.6f} ¬±{boot['MAE'][1]:.6f}\")\n",
        "print(f\"MSE = {mse:.6f} ¬±{boot['MSE'][1]:.6f}\")\n",
        "print(f\"R2  = {r2:.6f} ¬±{boot['R2'][1]:.6f}\")\n",
        "print(f\"MAPE= {mape:.6f} ¬±{boot['MAPE'][1]:.6f}\")\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ GUARDADO\n",
        "# ======================================================\n",
        "# Descomentar estas lineas para descargar\n",
        "# out_model = os.path.join(OUT_DIR, f\"xgb_final_{int(time.time())}.joblib\")\n",
        "# joblib.dump(model_final, out_model)\n",
        "# print(f\"\\nüíæ Modelo final guardado en: {out_model}\")\n",
        "# print(\"üèÅ Proceso completado exitosamente.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rv_SqO97fOcF",
        "outputId": "e543a7b5-3d2e-4e12-9cde-659318f8b442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Dataset cargado: 1073215 filas √ó 31 columnas\n",
            "‚úÖ Divisi√≥n 60/20/20 completada.\n",
            "üìä train=643929, val=214643, test=214643\n",
            "üîπ BO subsets -> train=2000, val=500\n",
            "\n",
            "üöÄ Iniciando optimizaci√≥n bayesiana para XGBoost (GPU)...\n",
            "|   iter    |  target   | n_esti... | learni... | max_depth | subsample | colsam... | reg_la... |\n",
            "-------------------------------------------------------------------------------------------------\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-3.026184\u001b[39m | \u001b[39m362.17808\u001b[39m | \u001b[39m0.2857071\u001b[39m | \u001b[39m8.1239575\u001b[39m | \u001b[39m0.7591950\u001b[39m | \u001b[39m0.4936111\u001b[39m | \u001b[39m1.5607892\u001b[39m |\n",
            "| \u001b[35m2        \u001b[39m | \u001b[35m-2.264787\u001b[39m | \u001b[35m140.65852\u001b[39m | \u001b[35m0.2611910\u001b[39m | \u001b[35m7.2078050\u001b[39m | \u001b[35m0.8248435\u001b[39m | \u001b[35m0.4123506\u001b[39m | \u001b[35m9.6991286\u001b[39m |\n",
            "| \u001b[35m3        \u001b[39m | \u001b[35m-0.709534\u001b[39m | \u001b[35m682.70984\u001b[39m | \u001b[35m0.0715783\u001b[39m | \u001b[35m4.2727747\u001b[39m | \u001b[35m0.5100427\u001b[39m | \u001b[35m0.5825453\u001b[39m | \u001b[35m5.2480395\u001b[39m |\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m-1.516931\u001b[39m | \u001b[39m402.36151\u001b[39m | \u001b[39m0.0944564\u001b[39m | \u001b[39m7.2829702\u001b[39m | \u001b[39m0.4836963\u001b[39m | \u001b[39m0.5752867\u001b[39m | \u001b[39m3.6642520\u001b[39m |\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m-1.066504\u001b[39m | \u001b[39m683.64825\u001b[39m | \u001b[39m0.1349815\u001b[39m | \u001b[39m4.5203377\u001b[39m | \u001b[39m0.8366890\u001b[39m | \u001b[39m0.4566149\u001b[39m | \u001b[39m4.8643680\u001b[39m |\n",
            "| \u001b[35m6        \u001b[39m | \u001b[35m-0.367443\u001b[39m | \u001b[35m679.20485\u001b[39m | \u001b[35m0.01     \u001b[39m | \u001b[35m3.4885069\u001b[39m | \u001b[35m0.4      \u001b[39m | \u001b[35m0.9988771\u001b[39m | \u001b[35m6.5123711\u001b[39m |\n",
            "| \u001b[35m7        \u001b[39m | \u001b[35m-0.349058\u001b[39m | \u001b[35m679.62463\u001b[39m | \u001b[35m0.01     \u001b[39m | \u001b[35m8.4809225\u001b[39m | \u001b[35m0.4      \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m10.0     \u001b[39m |\n",
            "| \u001b[35m8        \u001b[39m | \u001b[35m-0.317892\u001b[39m | \u001b[35m675.93415\u001b[39m | \u001b[35m0.01     \u001b[39m | \u001b[35m10.0     \u001b[39m | \u001b[35m0.4      \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m3.7307611\u001b[39m |\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m-1.637717\u001b[39m | \u001b[39m670.68201\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m0.4      \u001b[39m | \u001b[39m0.4      \u001b[39m | \u001b[39m10.0     \u001b[39m |\n",
            "| \u001b[35m10       \u001b[39m | \u001b[35m-0.293634\u001b[39m | \u001b[35m675.92653\u001b[39m | \u001b[35m0.01     \u001b[39m | \u001b[35m4.2866371\u001b[39m | \u001b[35m0.4      \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m0.001    \u001b[39m |\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m-0.605434\u001b[39m | \u001b[39m667.86863\u001b[39m | \u001b[39m0.3      \u001b[39m | \u001b[39m3.0      \u001b[39m | \u001b[39m0.4      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.001    \u001b[39m |\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m-0.602761\u001b[39m | \u001b[39m656.44371\u001b[39m | \u001b[39m0.3      \u001b[39m | \u001b[39m3.0      \u001b[39m | \u001b[39m0.4      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.001    \u001b[39m |\n",
            "=================================================================================================\n",
            "\n",
            "üèÅ Mejores hiperpar√°metros encontrados (XGBoost GPU):\n",
            " - n_estimators: 676\n",
            " - learning_rate: 0.010000\n",
            " - max_depth: 4\n",
            " - subsample: 0.400000\n",
            " - colsample_bytree: 1.000000\n",
            " - reg_lambda: 0.001000\n",
            "\n",
            "‚öôÔ∏è Entrenando modelo final con todos los datos de entrenamiento...\n",
            "‚úÖ Entrenamiento completado en 6.5 s\n",
            "\n",
            "üìä RESULTADOS (x_target):\n",
            "MAE = 0.234423 ¬±0.000539\n",
            "MSE = 0.106221 ¬±0.001276\n",
            "R2  = 0.999805 ¬±0.000002\n",
            "MAPE= 0.004787 ¬±0.000034\n",
            "\n",
            "üíæ Modelo final guardado en: /content/drive/MyDrive/xgb_models_bo/xgb_final_1760475627.joblib\n",
            "üèÅ Proceso completado exitosamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# üéØ INFERENCIA XGBOOST REGRESSOR (GPU) + REESCALADO A YARDAS\n",
        "# Usa: train_ready_final_numeric.csv + test_input_clean_final.csv\n",
        "# Guarda: /content/drive/MyDrive/predicciones_xgb_final_rescaled.csv\n",
        "# ======================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from google.colab import drive\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ MONTAR DRIVE Y CONFIGURAR RUTAS\n",
        "# ======================================================\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "train_path = \"/content/drive/MyDrive/train_ready_final_numeric.csv\"\n",
        "test_path  = \"/content/drive/MyDrive/test_input_clean_final.csv\"\n",
        "out_path   = \"/content/drive/MyDrive/predicciones_xgb_final_rescaled.csv\"\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ CARGA DE DATOS\n",
        "# ======================================================\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_test  = pd.read_csv(test_path)\n",
        "\n",
        "print(f\"‚úÖ Train cargado: {df_train.shape[0]} filas √ó {df_train.shape[1]} columnas\")\n",
        "print(f\"‚úÖ Test  cargado: {df_test.shape[0]} filas √ó {df_test.shape[1]} columnas\")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ DETECCI√ìN DE TARGETS\n",
        "# ======================================================\n",
        "if \"x_target\" in df_train.columns and \"y_target\" in df_train.columns:\n",
        "    TARGET_X, TARGET_Y = \"x_target\", \"y_target\"\n",
        "elif \"ball_land_x\" in df_train.columns and \"ball_land_y\" in df_train.columns:\n",
        "    TARGET_X, TARGET_Y = \"ball_land_x\", \"ball_land_y\"\n",
        "else:\n",
        "    raise ValueError(\"‚ùå No se encuentran columnas de objetivo en el dataset (x_target/y_target o ball_land_x/y).\")\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ DEFINIR FEATURES V√ÅLIDOS (evitar fuga de informaci√≥n)\n",
        "# ======================================================\n",
        "leak_or_bad = {\n",
        "    \"x_target\", \"y_target\", \"ball_land_x\", \"ball_land_y\",\n",
        "    \"dist_to_ball\", \"angle_to_ball\", \"vel_toward_ball\",\n",
        "    \"x\", \"y\", \"o\", \"dir\",\n",
        "    \"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"play_direction\",\n",
        "    \"player_name\", \"player_height\", \"player_birth_date\",\n",
        "    \"num_frames_output\"\n",
        "}\n",
        "\n",
        "train_cols = [c for c in df_train.columns if c not in leak_or_bad]\n",
        "feature_cols = [c for c in train_cols if c in df_test.columns]\n",
        "\n",
        "if len(feature_cols) == 0:\n",
        "    raise ValueError(\"‚ùå No hay columnas comunes v√°lidas entre train y test.\")\n",
        "\n",
        "print(f\"üîπ Features finales (comunes y sin fuga): {len(feature_cols)}\")\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ PREPARAR MATRICES\n",
        "# ======================================================\n",
        "X_train_full = df_train[feature_cols].astype(\"float32\").fillna(0)\n",
        "yx_full = df_train[TARGET_X].astype(\"float32\")\n",
        "yy_full = df_train[TARGET_Y].astype(\"float32\")\n",
        "X_pred = df_test[feature_cols].astype(\"float32\").fillna(0)\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ MEJORES HIPERPAR√ÅMETROS (de tu optimizaci√≥n)\n",
        "# ======================================================\n",
        "best_params = {\n",
        "    \"n_estimators\": 676,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"max_depth\": 4,\n",
        "    \"subsample\": 0.4,\n",
        "    \"colsample_bytree\": 1.0,\n",
        "    \"reg_lambda\": 0.001,\n",
        "    \"tree_method\": \"gpu_hist\",\n",
        "    \"predictor\": \"gpu_predictor\",\n",
        "    \"random_state\": 42,\n",
        "    \"verbosity\": 0\n",
        "}\n",
        "\n",
        "print(\"\\nüèÅ Mejores hiperpar√°metros XGBoost GPU:\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\" - {k}: {v}\")\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ ENTRENAMIENTO FINAL Y PREDICCI√ìN\n",
        "# ======================================================\n",
        "print(\"\\n‚öô Entrenando modelos finales (XGBoost GPU)...\")\n",
        "model_x = xgb.XGBRegressor(**best_params)\n",
        "model_y = xgb.XGBRegressor(**best_params)\n",
        "\n",
        "model_x.fit(X_train_full, yx_full)\n",
        "model_y.fit(X_train_full, yy_full)\n",
        "\n",
        "df_test[\"x_pred\"] = model_x.predict(X_pred)\n",
        "df_test[\"y_pred\"] = model_y.predict(X_pred)\n",
        "\n",
        "print(\"\\n‚úÖ Predicci√≥n completada.\")\n",
        "print(df_test[[\"x_pred\", \"y_pred\"]].head())\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ REESCALADO A UNIDADES F√çSICAS (YARDAS)\n",
        "# ======================================================\n",
        "def rescale_to_field(preds, new_min, new_max):\n",
        "    old_min, old_max = float(preds.min()), float(preds.max())\n",
        "    if abs(old_max - old_min) < 1e-6:\n",
        "        return np.clip(preds, new_min, new_max)\n",
        "    return (preds - old_min) / (old_max - old_min) * (new_max - new_min) + new_min\n",
        "\n",
        "df_test[\"x_pred_rescaled\"] = rescale_to_field(df_test[\"x_pred\"], 0, 120)\n",
        "df_test[\"y_pred_rescaled\"] = rescale_to_field(df_test[\"y_pred\"], 0, 53.3)\n",
        "\n",
        "print(\"\\nüìè Reescalado completado:\")\n",
        "print(df_test[[\"x_pred\", \"y_pred\", \"x_pred_rescaled\", \"y_pred_rescaled\"]].head())\n",
        "\n",
        "# ======================================================\n",
        "# 9Ô∏è‚É£ GUARDAR RESULTADOS\n",
        "# ======================================================\n",
        "# Descomenta las siguientes l√≠neas para guardar el archivo final\n",
        "# df_test.to_csv(out_path, index=False)\n",
        "# print(f\"\\n‚úÖ Archivo final guardado en: {out_path}\")\n",
        "# print(\"üéØ Predicciones reescaladas en yardas reales listas para an√°lisis.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGxzied5iVPb",
        "outputId": "5af73fa2-f424-4c0b-af9c-cfbdedf509e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Train cargado: 1073215 filas √ó 31 columnas\n",
            "‚úÖ Test  cargado: 49753 filas √ó 27 columnas\n",
            "üîπ Features finales (comunes y sin fuga): 18\n",
            "\n",
            "üèÅ Mejores hiperpar√°metros XGBoost GPU:\n",
            " - n_estimators: 676\n",
            " - learning_rate: 0.01\n",
            " - max_depth: 4\n",
            " - subsample: 0.4\n",
            " - colsample_bytree: 1.0\n",
            " - reg_lambda: 0.001\n",
            " - tree_method: gpu_hist\n",
            " - predictor: gpu_predictor\n",
            " - random_state: 42\n",
            " - verbosity: 0\n",
            "\n",
            "‚öô Entrenando modelos finales (XGBoost GPU)...\n",
            "\n",
            "‚úÖ Predicci√≥n completada.\n",
            "      x_pred     y_pred\n",
            "0  15.109440  25.856081\n",
            "1  15.622890  25.856081\n",
            "2  16.153156  25.747452\n",
            "3  16.443100  25.788563\n",
            "4  16.656563  25.826908\n",
            "\n",
            "üìè Reescalado completado:\n",
            "      x_pred     y_pred  x_pred_rescaled  y_pred_rescaled\n",
            "0  15.109440  25.856081         8.235289        27.719601\n",
            "1  15.622890  25.856081         9.988147        27.719601\n",
            "2  16.153156  25.747452        11.798410        26.950708\n",
            "3  16.443100  25.788563        12.788243        27.241695\n",
            "4  16.656563  25.826908        13.516980        27.513111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Conclusi√≥n final ‚Äî Modelo XGBoost Regressor (GPU + Optimizaci√≥n Bayesiana)\n",
        "\n",
        "El modelo XGBoost Regressor, potenciado con Optimizaci√≥n Bayesiana y GPU, logr√≥ un desempe√±o sobresaliente al predecir las coordenadas con una alta precisi√≥n (R¬≤ ‚âà 0.9998) y bajo error (MAE ‚âà 0.23).\n",
        "\n",
        "Durante la b√∫squeda de hiperpar√°metros, se utiliz√≥ un submuestreo controlado (2.000 muestras para entrenamiento y 500 para validaci√≥n). Este paso no afecta la precisi√≥n final, ya que su √∫nico prop√≥sito es acelerar la optimizaci√≥n bayesiana reduciendo el costo computacional sin alterar el patr√≥n general de los datos.\n",
        "\n",
        "Una vez encontrados los mejores hiperpar√°metros, el entrenamiento final se realiz√≥ con el 100% del conjunto de entrenamiento, garantizando que el modelo aproveche toda la informaci√≥n disponible.\n",
        "\n",
        "El uso de GPU (gpu_hist) permiti√≥ reducir dr√°sticamente el tiempo de c√≥mputo, y la optimizaci√≥n bayesiana evit√≥ una b√∫squeda exhaustiva, logrando un equilibrio ideal entre rendimiento, eficiencia y escalabilidad.\n",
        "\n",
        "En conclusi√≥n, este flujo de trabajo permite obtener un modelo r√°pido, robusto y altamente generalizable, ideal para grandes vol√∫menes de datos sin comprometer la precisi√≥n."
      ],
      "metadata": {
        "id": "lKC_yJNrmBix"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "26fPDIZOmJCX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}